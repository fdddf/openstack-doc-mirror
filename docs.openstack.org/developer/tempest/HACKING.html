<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Tempest Coding Guide &mdash; Tempest 14.0.1.dev225 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '14.0.1.dev225',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Tempest 14.0.1.dev225 documentation" href="index.html" />
    <link rel="next" title="Reviewing Tempest Code" href="REVIEWING.html" />
    <link rel="prev" title="Tempest Run" href="run.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="tempest-coding-guide">
<h1>Tempest Coding Guide<a class="headerlink" href="#tempest-coding-guide" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Step 1: Read the OpenStack Style Commandments
<a class="reference external" href="http://docs.openstack.org/developer/hacking/">http://docs.openstack.org/developer/hacking/</a></li>
<li>Step 2: Read on</li>
</ul>
<div class="section" id="tempest-specific-commandments">
<h2>Tempest Specific Commandments<a class="headerlink" href="#tempest-specific-commandments" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><dl class="first docutils">
<dt>[T102] Cannot import OpenStack python clients in tempest/api &amp;</dt>
<dd>tempest/scenario tests</dd>
</dl>
</li>
<li>[T104] Scenario tests require a services decorator</li>
<li>[T105] Tests cannot use setUpClass/tearDownClass</li>
<li>[T106] vim configuration should not be kept in source files.</li>
<li>[T107] Check that a service tag isn't in the module path</li>
<li>[T108] Check no hyphen at the end of rand_name() argument</li>
<li><dl class="first docutils">
<dt>[T109] Cannot use testtools.skip decorator; instead use</dt>
<dd>decorators.skip_because from tempest.lib</dd>
</dl>
</li>
<li>[T110] Check that service client names of GET should be consistent</li>
<li>[T111] Check that service client names of DELETE should be consistent</li>
<li>[T112] Check that tempest.lib should not import local tempest code</li>
<li>[T113] Check that tests use data_utils.rand_uuid() instead of uuid.uuid4()</li>
<li>[T114] Check that tempest.lib does not use tempest config</li>
<li>[N322] Method's default argument shouldn't be mutable</li>
</ul>
</div>
<div class="section" id="test-data-configuration">
<h2>Test Data/Configuration<a class="headerlink" href="#test-data-configuration" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Assume nothing about existing test data</li>
<li>Tests should be self contained (provide their own data)</li>
<li>Clean up test data at the completion of each test</li>
<li>Use configuration files for values that will vary by environment</li>
</ul>
</div>
<div class="section" id="exception-handling">
<h2>Exception Handling<a class="headerlink" href="#exception-handling" title="Permalink to this headline">¶</a></h2>
<p>According to the <code class="docutils literal"><span class="pre">The</span> <span class="pre">Zen</span> <span class="pre">of</span> <span class="pre">Python</span></code> the
<code class="docutils literal"><span class="pre">Errors</span> <span class="pre">should</span> <span class="pre">never</span> <span class="pre">pass</span> <span class="pre">silently.</span></code>
Tempest usually runs in special environment (jenkins gate jobs), in every
error or failure situation we should provide as much error related
information as possible, because we usually do not have the chance to
investigate the situation after the issue happened.</p>
<p>In every test case the abnormal situations must be very verbosely explained,
by the exception and the log.</p>
<p>In most cases the very first issue is the most important information.</p>
<p>Try to avoid using <code class="docutils literal"><span class="pre">try</span></code> blocks in the test cases, as both the <code class="docutils literal"><span class="pre">except</span></code>
and <code class="docutils literal"><span class="pre">finally</span></code> blocks could replace the original exception,
when the additional operations leads to another exception.</p>
<p>Just letting an exception to propagate, is not a bad idea in a test case,
at all.</p>
<p>Try to avoid using any exception handling construct which can hide the errors
origin.</p>
<p>If you really need to use a <code class="docutils literal"><span class="pre">try</span></code> block, please ensure the original
exception at least logged.  When the exception is logged you usually need
to <code class="docutils literal"><span class="pre">raise</span></code> the same or a different exception anyway.</p>
<p>Use of <code class="docutils literal"><span class="pre">self.addCleanup</span></code> is often a good way to avoid having to catch
exceptions and still ensure resources are correctly cleaned up if the
test fails part way through.</p>
<p>Use the <code class="docutils literal"><span class="pre">self.assert*</span></code> methods provided by the unit test framework.
This signals the failures early on.</p>
<p>Avoid using the <code class="docutils literal"><span class="pre">self.fail</span></code> alone, its stack trace will signal
the <code class="docutils literal"><span class="pre">self.fail</span></code> line as the origin of the error.</p>
<p>Avoid constructing complex boolean expressions for assertion.
The <code class="docutils literal"><span class="pre">self.assertTrue</span></code> or <code class="docutils literal"><span class="pre">self.assertFalse</span></code> without a <code class="docutils literal"><span class="pre">msg</span></code> argument,
will just tell you the single boolean value, and you will not know anything
about the values used in the formula, the <code class="docutils literal"><span class="pre">msg</span></code> argument might be good enough
for providing more information.</p>
<p>Most other assert method can include more information by default.
For example <code class="docutils literal"><span class="pre">self.assertIn</span></code> can include the whole set.</p>
<p>It is recommended to use testtools <a class="reference external" href="http://testtools.readthedocs.org/en/latest/for-test-authors.html#matchers">matcher</a> for the more tricky assertions.
You can implement your own specific <a class="reference external" href="http://testtools.readthedocs.org/en/latest/for-test-authors.html#matchers">matcher</a> as well.</p>
<p>If the test case fails you can see the related logs and the information
carried by the exception (exception class, backtrack and exception info).
This and the service logs are your only guide to finding the root cause of flaky
issues.</p>
</div>
<div class="section" id="test-cases-are-independent">
<h2>Test cases are independent<a class="headerlink" href="#test-cases-are-independent" title="Permalink to this headline">¶</a></h2>
<p>Every <code class="docutils literal"><span class="pre">test_method</span></code> must be callable individually and MUST NOT depends on,
any other <code class="docutils literal"><span class="pre">test_method</span></code> or <code class="docutils literal"><span class="pre">test_method</span></code> ordering.</p>
<p>Test cases MAY depend on commonly initialized resources/facilities, like
credentials management, testresources and so on. These facilities, MUST be able
to work even if just one <code class="docutils literal"><span class="pre">test_method</span></code> is selected for execution.</p>
</div>
<div class="section" id="service-tagging">
<h2>Service Tagging<a class="headerlink" href="#service-tagging" title="Permalink to this headline">¶</a></h2>
<p>Service tagging is used to specify which services are exercised by a particular
test method. You specify the services with the tempest.test.services decorator.
For example:</p>
<p>&#64;services('compute', 'image')</p>
<p>Valid service tag names are the same as the list of directories in tempest.api
that have tests.</p>
<p>For scenario tests having a service tag is required. For the api tests service
tags are only needed if the test method makes an api call (either directly or
indirectly through another service) that differs from the parent directory
name. For example, any test that make an api call to a service other than nova
in tempest.api.compute would require a service tag for those services, however
they do not need to be tagged as compute.</p>
</div>
<div class="section" id="test-fixtures-and-resources">
<h2>Test fixtures and resources<a class="headerlink" href="#test-fixtures-and-resources" title="Permalink to this headline">¶</a></h2>
<p>Test level resources should be cleaned-up after the test execution. Clean-up
is best scheduled using <cite>addCleanup</cite> which ensures that the resource cleanup
code is always invoked, and in reverse order with respect to the creation
order.</p>
<p>Test class level resources should be defined in the <cite>resource_setup</cite> method of
the test class, except for any credential obtained from the credentials
provider, which should be set-up in the <cite>setup_credentials</cite> method.</p>
<p>The test base class <cite>BaseTestCase</cite> defines Tempest framework for class level
fixtures. <cite>setUpClass</cite> and <cite>tearDownClass</cite> are defined here and cannot be
overwritten by subclasses (enforced via hacking rule T105).</p>
<p>Set-up is split in a series of steps (setup stages), which can be overwritten
by test classes. Set-up stages are:</p>
<ul class="simple">
<li><cite>skip_checks</cite></li>
<li><cite>setup_credentials</cite></li>
<li><cite>setup_clients</cite></li>
<li><cite>resource_setup</cite></li>
</ul>
<p>Tear-down is also split in a series of steps (teardown stages), which are
stacked for execution only if the corresponding setup stage had been
reached during the setup phase. Tear-down stages are:</p>
<ul class="simple">
<li><cite>clear_credentials</cite> (defined in the base test class)</li>
<li><cite>resource_cleanup</cite></li>
</ul>
</div>
<div class="section" id="skipping-tests">
<h2>Skipping Tests<a class="headerlink" href="#skipping-tests" title="Permalink to this headline">¶</a></h2>
<p>Skipping tests should be based on configuration only. If that is not possible,
it is likely that either a configuration flag is missing, or the test should
fail rather than be skipped.
Using discovery for skipping tests is generally discouraged.</p>
<p>When running a test that requires a certain &quot;feature&quot; in the target
cloud, if that feature is missing we should fail, because either the test
configuration is invalid, or the cloud is broken and the expected &quot;feature&quot; is
not there even if the cloud was configured with it.</p>
</div>
<div class="section" id="negative-tests">
<h2>Negative Tests<a class="headerlink" href="#negative-tests" title="Permalink to this headline">¶</a></h2>
<p>Error handling is an important aspect of API design and usage. Negative
tests are a way to ensure that an application can gracefully handle
invalid or unexpected input. However, as a black box integration test
suite, Tempest is not suitable for handling all negative test cases, as
the wide variety and complexity of negative tests can lead to long test
runs and knowledge of internal implementation details. The bulk of
negative testing should be handled with project function tests.
All negative tests should be based on <a class="reference external" href="https://github.com/openstack/api-wg/blob/master/guidelines/http.rst#failure-code-clarifications">API-WG guideline</a> . Such negative
tests can block any changes from accurate failure code to invalid one.</p>
<p>If facing some gray area which is not clarified on the above guideline, propose
a new guideline to the API-WG. With a proposal to the API-WG we will be able to
build a consensus across all OpenStack projects and improve the quality and
consistency of all the APIs.</p>
<p>In addition, we have some guidelines for additional negative tests.</p>
<ul class="simple">
<li>About BadRequest(HTTP400) case: We can add a single negative tests of
BadRequest for each resource and method(POST, PUT).
Please don't implement more negative tests on the same combination of
resource and method even if API request parameters are different from
the existing test.</li>
<li>About NotFound(HTTP404) case: We can add a single negative tests of
NotFound for each resource and method(GET, PUT, DELETE, HEAD).
Please don't implement more negative tests on the same combination
of resource and method.</li>
</ul>
<p>The above guidelines don't cover all cases and we will grow these guidelines
organically over time. Patches outside of the above guidelines are left up to
the reviewers' discretion and if we face some conflicts between reviewers, we
will expand the guideline based on our discussion and experience.</p>
</div>
<div class="section" id="test-skips-because-of-known-bugs">
<h2>Test skips because of Known Bugs<a class="headerlink" href="#test-skips-because-of-known-bugs" title="Permalink to this headline">¶</a></h2>
<p>If a test is broken because of a bug it is appropriate to skip the test until
bug has been fixed. You should use the skip_because decorator so that
Tempest's skip tracking tool can watch the bug status.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="nd">@skip_because</span><span class="p">(</span><span class="n">bug</span><span class="o">=</span><span class="s2">&quot;980688&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_this_and_that</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
  <span class="o">...</span>
</pre></div>
</div>
</div>
<div class="section" id="guidelines">
<h2>Guidelines<a class="headerlink" href="#guidelines" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Do not submit changesets with only testcases which are skipped as
they will not be merged.</li>
<li>Consistently check the status code of responses in testcases. The
earlier a problem is detected the easier it is to debug, especially
where there is complicated setup required.</li>
</ul>
</div>
<div class="section" id="parallel-test-execution">
<h2>Parallel Test Execution<a class="headerlink" href="#parallel-test-execution" title="Permalink to this headline">¶</a></h2>
<p>Tempest by default runs its tests in parallel this creates the possibility for
interesting interactions between tests which can cause unexpected failures.
Dynamic credentials provides protection from most of the potential race
conditions between tests outside the same class. But there are still a few of
things to watch out for to try to avoid issues when running your tests in
parallel.</p>
<ul class="simple">
<li>Resources outside of a project scope still have the potential to conflict. This
is a larger concern for the admin tests since most resources and actions that
require admin privileges are outside of projects.</li>
<li>Races between methods in the same class are not a problem because
parallelization in tempest is at the test class level, but if there is a json
and xml version of the same test class there could still be a race between
methods.</li>
<li>The rand_name() function from tempest.common.utils.data_utils should be used
anywhere a resource is created with a name. Static naming should be avoided
to prevent resource conflicts.</li>
<li>If the execution of a set of tests is required to be serialized then locking
can be used to perform this. See AggregatesAdminTest in
tempest.api.compute.admin for an example of using locking.</li>
</ul>
</div>
<div class="section" id="sample-configuration-file">
<h2>Sample Configuration File<a class="headerlink" href="#sample-configuration-file" title="Permalink to this headline">¶</a></h2>
<p>The sample config file is autogenerated using a script. If any changes are made
to the config variables in tempest/config.py then the sample config file must be
regenerated. This can be done running:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">egenconfig</span>
</pre></div>
</div>
</div>
<div class="section" id="unit-tests">
<h2>Unit Tests<a class="headerlink" href="#unit-tests" title="Permalink to this headline">¶</a></h2>
<p>Unit tests are a separate class of tests in tempest. They verify tempest
itself, and thus have a different set of guidelines around them:</p>
<ol class="arabic simple">
<li>They can not require anything running externally. All you should need to
run the unit tests is the git tree, python and the dependencies installed.
This includes running services, a config file, etc.</li>
<li>The unit tests cannot use setUpClass, instead fixtures and testresources
should be used for shared state between tests.</li>
</ol>
</div>
<div class="section" id="test-documentation">
<span id="testdocumentation"></span><h2>Test Documentation<a class="headerlink" href="#test-documentation" title="Permalink to this headline">¶</a></h2>
<p>For tests being added we need to require inline documentation in the form of
docstrings to explain what is being tested. In API tests for a new API a class
level docstring should be added to an API reference doc. If one doesn't exist
a TODO comment should be put indicating that the reference needs to be added.
For individual API test cases a method level docstring should be used to
explain the functionality being tested if the test name isn't descriptive
enough. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_get_role_by_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a role by its id.&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>the docstring there is superfluous and shouldn't be added. but for a method
like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_volume_backup_create_get_detailed_list_restore_delete</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>a docstring would be useful because while the test title is fairly descriptive
the operations being performed are complex enough that a bit more explanation
will help people figure out the intent of the test.</p>
<p>For scenario tests a class level docstring describing the steps in the scenario
is required. If there is more than one test case in the class individual
docstrings for the workflow in each test methods can be used instead. A good
example of this would be:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TestVolumeBootPattern</span><span class="p">(</span><span class="n">manager</span><span class="o">.</span><span class="n">ScenarioTest</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This test case attempts to reproduce the following steps:</span>

<span class="sd">     * Create in Cinder some bootable volume importing a Glance image</span>
<span class="sd">     * Boot an instance from the bootable volume</span>
<span class="sd">     * Write content to the volume</span>
<span class="sd">     * Delete an instance and Boot a new instance from the volume</span>
<span class="sd">     * Check written content in the instance</span>
<span class="sd">     * Create a volume snapshot while the instance is running</span>
<span class="sd">     * Boot an additional instance from the new snapshot based volume</span>
<span class="sd">     * Check written content in the instance booted from snapshot</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="test-identification-with-idempotent-id">
<h2>Test Identification with Idempotent ID<a class="headerlink" href="#test-identification-with-idempotent-id" title="Permalink to this headline">¶</a></h2>
<p>Every function that provides a test must have an <code class="docutils literal"><span class="pre">idempotent_id</span></code> decorator
that is a unique <code class="docutils literal"><span class="pre">uuid-4</span></code> instance. This ID is used to complement the fully
qualified test name and track test functionality through refactoring. The
format of the metadata looks like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="nd">@test.idempotent_id</span><span class="p">(</span><span class="s1">&#39;585e934c-448e-43c4-acbf-d06a9b899997&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_list_servers_with_detail</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># The created server should be in the detailed list of all servers</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Tempest.lib includes a <code class="docutils literal"><span class="pre">check-uuid</span></code> tool that will test for the existence
and uniqueness of idempotent_id metadata for every test. If you have
tempest installed you run the tool against Tempest by calling from the
tempest repo:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">check</span><span class="o">-</span><span class="n">uuid</span>
</pre></div>
</div>
<p>It can be invoked against any test suite by passing a package name:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>check-uuid --package &lt;package_name&gt;
</pre></div>
</div>
<p>Tests without an <code class="docutils literal"><span class="pre">idempotent_id</span></code> can be automatically fixed by running
the command with the <code class="docutils literal"><span class="pre">--fix</span></code> flag, which will modify the source package
by inserting randomly generated uuids for every test that does not have
one:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">check</span><span class="o">-</span><span class="n">uuid</span> <span class="o">--</span><span class="n">fix</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">check-uuid</span></code> tool is used as part of the tempest gate job
to ensure that all tests have an <code class="docutils literal"><span class="pre">idempotent_id</span></code> decorator.</p>
</div>
<div class="section" id="branchless-tempest-considerations">
<h2>Branchless Tempest Considerations<a class="headerlink" href="#branchless-tempest-considerations" title="Permalink to this headline">¶</a></h2>
<p>Starting with the OpenStack Icehouse release Tempest no longer has any stable
branches. This is to better ensure API consistency between releases because
the API behavior should not change between releases. This means that the stable
branches are also gated by the Tempest master branch, which also means that
proposed commits to Tempest must work against both the master and all the
currently supported stable branches of the projects. As such there are a few
special considerations that have to be accounted for when pushing new changes
to tempest.</p>
<div class="section" id="new-tests-for-new-features">
<h3>1. New Tests for new features<a class="headerlink" href="#new-tests-for-new-features" title="Permalink to this headline">¶</a></h3>
<p>When adding tests for new features that were not in previous releases of the
projects the new test has to be properly skipped with a feature flag. Whether
this is just as simple as using the &#64;test.requires_ext() decorator to check
if the required extension (or discoverable optional API) is enabled or adding
a new config option to the appropriate section. If there isn't a method of
selecting the new <strong>feature</strong> from the config file then there won't be a
mechanism to disable the test with older stable releases and the new test won't
be able to merge.</p>
</div>
<div class="section" id="bug-fix-on-core-project-needing-tempest-changes">
<h3>2. Bug fix on core project needing Tempest changes<a class="headerlink" href="#bug-fix-on-core-project-needing-tempest-changes" title="Permalink to this headline">¶</a></h3>
<p>When trying to land a bug fix which changes a tested API you'll have to use the
following procedure:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>- Propose change to the project, get a +2 on the change even with failing
- Propose skip on Tempest which will only be approved after the
  corresponding change in the project has a +2 on change
- Land project change in master and all open stable branches (if required)
- Land changed test in Tempest
</pre></div>
</div>
<p>Otherwise the bug fix won't be able to land in the project.</p>
</div>
<div class="section" id="new-tests-for-existing-features">
<h3>3. New Tests for existing features<a class="headerlink" href="#new-tests-for-existing-features" title="Permalink to this headline">¶</a></h3>
<p>If a test is being added for a feature that exists in all the current releases
of the projects then the only concern is that the API behavior is the same
across all the versions of the project being tested. If the behavior is not
consistent the test will not be able to merge.</p>
</div>
</div>
<div class="section" id="api-stability">
<h2>API Stability<a class="headerlink" href="#api-stability" title="Permalink to this headline">¶</a></h2>
<p>For new tests being added to Tempest the assumption is that the API being
tested is considered stable and adheres to the OpenStack API stability
guidelines. If an API is still considered experimental or in development then
it should not be tested by Tempest until it is considered stable.</p>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Tempest Coding Guide</a><ul>
<li><a class="reference internal" href="#tempest-specific-commandments">Tempest Specific Commandments</a></li>
<li><a class="reference internal" href="#test-data-configuration">Test Data/Configuration</a></li>
<li><a class="reference internal" href="#exception-handling">Exception Handling</a></li>
<li><a class="reference internal" href="#test-cases-are-independent">Test cases are independent</a></li>
<li><a class="reference internal" href="#service-tagging">Service Tagging</a></li>
<li><a class="reference internal" href="#test-fixtures-and-resources">Test fixtures and resources</a></li>
<li><a class="reference internal" href="#skipping-tests">Skipping Tests</a></li>
<li><a class="reference internal" href="#negative-tests">Negative Tests</a></li>
<li><a class="reference internal" href="#test-skips-because-of-known-bugs">Test skips because of Known Bugs</a></li>
<li><a class="reference internal" href="#guidelines">Guidelines</a></li>
<li><a class="reference internal" href="#parallel-test-execution">Parallel Test Execution</a></li>
<li><a class="reference internal" href="#sample-configuration-file">Sample Configuration File</a></li>
<li><a class="reference internal" href="#unit-tests">Unit Tests</a></li>
<li><a class="reference internal" href="#test-documentation">Test Documentation</a></li>
<li><a class="reference internal" href="#test-identification-with-idempotent-id">Test Identification with Idempotent ID</a></li>
<li><a class="reference internal" href="#branchless-tempest-considerations">Branchless Tempest Considerations</a><ul>
<li><a class="reference internal" href="#new-tests-for-new-features">1. New Tests for new features</a></li>
<li><a class="reference internal" href="#bug-fix-on-core-project-needing-tempest-changes">2. Bug fix on core project needing Tempest changes</a></li>
<li><a class="reference internal" href="#new-tests-for-existing-features">3. New Tests for existing features</a></li>
</ul>
</li>
<li><a class="reference internal" href="#api-stability">API Stability</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="run.html"
                                  title="previous chapter">Tempest Run</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="REVIEWING.html"
                                  title="next chapter">Reviewing Tempest Code</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/tempest
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/HACKING.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="REVIEWING.html" title="Reviewing Tempest Code"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="run.html" title="Tempest Run"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Tempest 14.0.1.dev225 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2013, OpenStack QA Team.
      Last updated on &#39;Tue Feb 14 13:17:43 2017, commit 6827cd3&#39;.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/Tempest");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>