<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Identifying issues and resolutions &mdash; swift 2.12.1.dev102 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2.12.1.dev102',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="swift 2.12.1.dev102 documentation" href="../index.html" />
    <link rel="up" title="Swift Ops Runbook" href="index.html" />
    <link rel="next" title="Software configuration procedures" href="procedures.html" />
    <link rel="prev" title="Swift Ops Runbook" href="index.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="identifying-issues-and-resolutions">
<h1>Identifying issues and resolutions<a class="headerlink" href="#identifying-issues-and-resolutions" title="Permalink to this headline">¶</a></h1>
<div class="section" id="is-the-system-up">
<h2>Is the system up?<a class="headerlink" href="#is-the-system-up" title="Permalink to this headline">¶</a></h2>
<p>If you have a report that Swift is down, perform the following basic checks:</p>
<ol class="arabic simple">
<li>Run swift functional tests.</li>
<li>From a server in your data center, use <code class="docutils literal"><span class="pre">curl</span></code> to check <code class="docutils literal"><span class="pre">/healthcheck</span></code>
(see below).</li>
<li>If you have a monitoring system, check your monitoring system.</li>
<li>Check your hardware load balancers infrastructure.</li>
<li>Run swift-recon on a proxy node.</li>
</ol>
</div>
<div class="section" id="functional-tests-usage">
<h2>Functional tests usage<a class="headerlink" href="#functional-tests-usage" title="Permalink to this headline">¶</a></h2>
<p>We would recommend that you set up the functional tests to run against your
production system. Run regularly this can be a useful tool to validate
that the system is configured correctly. In addition, it can provide
early warning about failures in your system (if the functional tests stop
working, user applications will also probably stop working).</p>
<p>A script for running the function tests is located in <code class="docutils literal"><span class="pre">swift/.functests</span></code>.</p>
</div>
<div class="section" id="external-monitoring">
<h2>External monitoring<a class="headerlink" href="#external-monitoring" title="Permalink to this headline">¶</a></h2>
<p>We use pingdom.com to monitor the external Swift API. We suggest the
following:</p>
<blockquote>
<div><ul class="simple">
<li>Do a GET on <code class="docutils literal"><span class="pre">/healthcheck</span></code></li>
<li>Create a container, make it public (x-container-read:
.r*,.rlistings), create a small file in the container; do a GET
on the object</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="diagnose-general-approach">
<h2>Diagnose: General approach<a class="headerlink" href="#diagnose-general-approach" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Look at service status in your monitoring system.</li>
<li>In addition to system monitoring tools and issue logging by users,
swift errors will often result in log entries (see <a class="reference internal" href="#swift-logs"><span>Diagnose: Interpreting messages in /var/log/swift/ files</span></a>).</li>
<li>Look at any logs your deployment tool produces.</li>
<li>Log files should be reviewed for error signatures (see below) that
may point to a known issue, or root cause issues reported by the
diagnostics tools, prior to escalation.</li>
</ul>
<div class="section" id="dependencies">
<h3>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h3>
<p>The Swift software is dependent on overall system health. Operating
system level issues with network connectivity, domain name resolution,
user management, hardware and system configuration and capacity in terms
of memory and free disk space, may result is secondary Swift issues.
System level issues should be resolved prior to diagnosis of swift
issues.</p>
</div>
</div>
<div class="section" id="diagnose-swift-dispersion-report">
<h2>Diagnose: Swift-dispersion-report<a class="headerlink" href="#diagnose-swift-dispersion-report" title="Permalink to this headline">¶</a></h2>
<p>The swift-dispersion-report is a useful tool to gauge the general
health of the system. Configure the <code class="docutils literal"><span class="pre">swift-dispersion</span></code> report to cover at
a minimum every disk drive in your system (usually 1% coverage).
See <a class="reference internal" href="../admin_guide.html#dispersion-report"><span>Dispersion Report</span></a> for details of how to configure and
use the dispersion reporting tool.</p>
<p>The <code class="docutils literal"><span class="pre">swift-dispersion-report</span></code> tool can take a long time to run, especially
if any servers are down. We suggest you run it regularly
(e.g., in a cron job) and save the results. This makes it easy to refer
to the last report without having to wait for a long-running command
to complete.</p>
</div>
<div class="section" id="diagnose-is-system-responding-to-healthcheck">
<h2>Diagnose: Is system responding to /healthcheck?<a class="headerlink" href="#diagnose-is-system-responding-to-healthcheck" title="Permalink to this headline">¶</a></h2>
<p>When you want to establish if a swift endpoint is running, run <code class="docutils literal"><span class="pre">curl</span> <span class="pre">-k</span></code>
against <a class="reference external" href="https://">https://</a><em>[ENDPOINT]</em>/healthcheck.</p>
</div>
<div class="section" id="diagnose-interpreting-messages-in-var-log-swift-files">
<span id="swift-logs"></span><h2>Diagnose: Interpreting messages in <code class="docutils literal"><span class="pre">/var/log/swift/</span></code> files<a class="headerlink" href="#diagnose-interpreting-messages-in-var-log-swift-files" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In the Hewlett Packard Enterprise Helion Public Cloud we send logs to
<code class="docutils literal"><span class="pre">proxy.log</span></code> (proxy-server logs), <code class="docutils literal"><span class="pre">server.log</span></code> (object-server,
account-server, container-server logs), <code class="docutils literal"><span class="pre">background.log</span></code> (all
other servers [object-replicator, etc]).</p>
</div>
<p>The following table lists known issues:</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Logfile</strong></th>
<th class="head"><strong>Signature</strong></th>
<th class="head"><strong>Issue</strong></th>
<th class="head"><strong>Steps to take</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>/var/log/syslog</td>
<td>kernel: [] sd .... [csbu:sd...] Sense Key: Medium Error</td>
<td>Suggests disk surface issues</td>
<td>Run <code class="docutils literal"><span class="pre">swift-drive-audit</span></code> on the target node to check for disk errors,
repair disk errors</td>
</tr>
<tr class="row-odd"><td>/var/log/syslog</td>
<td>kernel: [] sd .... [csbu:sd...] Sense Key: Hardware Error</td>
<td>Suggests storage hardware issues</td>
<td>Run diagnostics on the target node to check for disk failures,
replace failed disks</td>
</tr>
<tr class="row-even"><td>/var/log/syslog</td>
<td>kernel: [] .... I/O error, dev sd.... ,sector ....</td>
<td>&nbsp;</td>
<td>Run diagnostics on the target node to check for disk errors</td>
</tr>
<tr class="row-odd"><td>/var/log/syslog</td>
<td>pound: NULL get_thr_arg</td>
<td>Multiple threads woke up</td>
<td>Noise, safe to ignore</td>
</tr>
<tr class="row-even"><td>/var/log/swift/proxy.log</td>
<td>.... ERROR .... ConnectionTimeout ....</td>
<td>A storage node is not responding in a timely fashion</td>
<td>Check if node is down, not running Swift,
unconfigured, storage off-line or for network issues between the
proxy and non responding node</td>
</tr>
<tr class="row-odd"><td>/var/log/swift/proxy.log</td>
<td>proxy-server .... HTTP/1.0 500 ....</td>
<td>A proxy server has reported an internal server error</td>
<td>Examine the logs for any errors at the time the error was reported to
attempt to understand the cause of the error.</td>
</tr>
<tr class="row-even"><td>/var/log/swift/server.log</td>
<td>.... ERROR .... ConnectionTimeout ....</td>
<td>A storage server is not responding in a timely fashion</td>
<td>Check if node is down, not running Swift,
unconfigured, storage off-line or for network issues between the
server and non responding node</td>
</tr>
<tr class="row-odd"><td>/var/log/swift/server.log</td>
<td>.... ERROR .... Remote I/O error: &#8216;/srv/node/disk....</td>
<td>A storage device is not responding as expected</td>
<td>Run <code class="docutils literal"><span class="pre">swift-drive-audit</span></code> and check the filesystem named in the error
for corruption (unmount &amp; xfs_repair). Check if the filesystem
is mounted and working.</td>
</tr>
<tr class="row-even"><td>/var/log/swift/background.log</td>
<td>object-server ERROR container update failed .... Connection refused</td>
<td>A container server node could not be contacted</td>
<td>Check if node is down, not running Swift,
unconfigured, storage off-line or for network issues between the
server and non responding node</td>
</tr>
<tr class="row-odd"><td>/var/log/swift/background.log</td>
<td>object-updater ERROR with remote .... ConnectionTimeout</td>
<td>The remote container server is busy</td>
<td>If the container is very large, some errors updating it can be
expected. However, this error can also occur if there is a networking
issue.</td>
</tr>
<tr class="row-even"><td>/var/log/swift/background.log</td>
<td>account-reaper STDOUT: .... error: ECONNREFUSED</td>
<td>Network connectivity issue or the target server is down.</td>
<td>Resolve network issue or reboot the target server</td>
</tr>
<tr class="row-odd"><td>/var/log/swift/background.log</td>
<td>.... ERROR .... ConnectionTimeout</td>
<td>A storage server is not responding in a timely fashion</td>
<td>The target server may be busy. However, this error can also occur if
there is a networking issue.</td>
</tr>
<tr class="row-even"><td>/var/log/swift/background.log</td>
<td>.... ERROR syncing .... Timeout</td>
<td>A timeout occurred syncing data to another node.</td>
<td>The target server may be busy. However, this error can also occur if
there is a networking issue.</td>
</tr>
<tr class="row-odd"><td>/var/log/swift/background.log</td>
<td>.... ERROR Remote drive not mounted ....</td>
<td>A storage server disk is unavailable</td>
<td>Repair and remount the file system (on the remote node)</td>
</tr>
<tr class="row-even"><td>/var/log/swift/background.log</td>
<td>object-replicator .... responded as unmounted</td>
<td>A storage server disk is unavailable</td>
<td>Repair and remount the file system (on the remote node)</td>
</tr>
<tr class="row-odd"><td>/var/log/swift/*.log</td>
<td>STDOUT: EXCEPTION IN</td>
<td>A unexpected error occurred</td>
<td>Read the Traceback details, if it matches known issues
(e.g. active network/disk issues), check for re-ocurrences
after the primary issues have been resolved</td>
</tr>
<tr class="row-even"><td>/var/log/rsyncd.log</td>
<td>rsync: mkdir &#8220;/disk....failed: No such file or directory....</td>
<td>A local storage server disk is unavailable</td>
<td>Run diagnostics on the node to check for a failed or
unmounted disk</td>
</tr>
<tr class="row-odd"><td>/var/log/swift*</td>
<td>Exception: Could not bind to 0.0.0.0:6xxx</td>
<td>Possible Swift process restart issue. This indicates an old swift
process is still running.</td>
<td>Restart Swift services. If some swift services are reported down,
check if they left residual process behind.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="diagnose-parted-reports-the-backup-gpt-table-is-corrupt">
<h2>Diagnose: Parted reports the backup GPT table is corrupt<a class="headerlink" href="#diagnose-parted-reports-the-backup-gpt-table-is-corrupt" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p class="first">If a GPT table is broken, a message like the following should be
observed when the following command is run:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ sudo parted -l
</pre></div>
</div>
<div class="code highlight-python"><div class="highlight"><pre><span></span>Error: The backup GPT table is corrupt, but the primary appears OK,
so that will be used.

OK/Cancel?
</pre></div>
</div>
</li>
</ul>
<p>To fix, go to <a class="reference internal" href="procedures.html#fix-broken-gpt-table"><span>Fix broken GPT table (broken disk partition)</span></a></p>
</div>
<div class="section" id="diagnose-drives-diagnostic-reports-a-fs-label-is-not-acceptable">
<h2>Diagnose: Drives diagnostic reports a FS label is not acceptable<a class="headerlink" href="#diagnose-drives-diagnostic-reports-a-fs-label-is-not-acceptable" title="Permalink to this headline">¶</a></h2>
<p>If diagnostics reports something like  &#8220;FS label: obj001dsk011 is not
acceptable&#8221;, it indicates that a partition has a valid disk label, but an
invalid filesystem label. In such cases proceed as follows:</p>
<ol class="arabic">
<li><p class="first">Verify that the disk labels are correct:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>FS=/dev/sd#1

sudo parted -l | grep object
</pre></div>
</div>
</li>
<li><p class="first">If partition labels are inconsistent then, resolve the disk label issues
before proceeding:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo parted -s ${FS} name ${PART_NO} ${PART_NAME} #Partition Label
#PART_NO is 1 for object disks and 3 for OS disks
#PART_NAME follows the convention seen in &quot;sudo parted -l | grep object&quot;
</pre></div>
</div>
</li>
<li><p class="first">If the Filesystem label is missing then create it with care:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo xfs_admin -l ${FS} #Filesystem label (12 Char limit)

#Check for the existence of a FS label

OBJNO=&lt;3 Length Object No.&gt;

#I.E OBJNO for sw-stbaz3-object0007 would be 007

DISKNO=&lt;3 Length Disk No.&gt;

#I.E DISKNO for /dev/sdb would be 001, /dev/sdc would be 002 etc.

sudo xfs_admin -L &quot;obj${OBJNO}dsk${DISKNO}&quot; ${FS}

#Create a FS Label
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="diagnose-failed-luns">
<h2>Diagnose: Failed LUNs<a class="headerlink" href="#diagnose-failed-luns" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The HPE Helion Public Cloud uses direct attach SmartArray
controllers/drives. The information here is specific to that
environment. The hpacucli utility mentioned here may be called
hpssacli in your environment.</p>
</div>
<p>The <code class="docutils literal"><span class="pre">swift_diagnostics</span></code> mount checks may return a warning that a LUN has
failed, typically accompanied by DriveAudit check failures and device
errors.</p>
<p>Such cases are typically caused by a drive failure, and if drive check
also reports a failed status for the underlying drive, then follow
the procedure to replace the disk.</p>
<p>Otherwise the lun can be re-enabled as follows:</p>
<ol class="arabic">
<li><p class="first">Generate a hpssacli diagnostic report. This report allows the DC
team to troubleshoot potential cabling or hardware issues so it is
imperative that you run it immediately when troubleshooting a failed
LUN. You will come back later and grep this file for more details, but
just generate it for now.</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo hpssacli controller all diag file=/tmp/hpacu.diag ris=on xml=off zip=off
</pre></div>
</div>
</li>
</ol>
<p>Export the following variables using the below instructions before
proceeding further.</p>
<ol class="arabic">
<li><p class="first">Print a list of logical drives and their numbers and take note of the
failed drive&#8217;s number and array value (example output: &#8220;array A
logicaldrive 1...&#8221; would be exported as LDRIVE=1):</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo hpssacli controller slot=1 ld all show
</pre></div>
</div>
</li>
<li><p class="first">Export the number of the logical drive that was retrieved from the
previous command into the LDRIVE variable:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>export LDRIVE=&lt;LogicalDriveNumber&gt;
</pre></div>
</div>
</li>
<li><p class="first">Print the array value and Port:Box:Bay for all drives and take note of
the Port:Box:Bay for the failed drive (example output: &#8221; array A
physicaldrive 2C:1:1...&#8221; would be exported as PBOX=2C:1:1). Match the
array value of this output with the array value obtained from the
previous command to be sure you are working on the same drive. Also,
the array value usually matches the device name (For example, /dev/sdc
in the case of &#8220;array c&#8221;), but we will run a different command to be sure
we are operating on the correct device.</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo hpssacli controller slot=1 pd all show
</pre></div>
</div>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Sometimes a LUN may appear to be failed as it is not and cannot
be mounted but the hpssacli/parted commands may show no problems with
the LUNS/drives. In this case, the filesystem may be corrupt and may be
necessary to run <code class="docutils literal"><span class="pre">sudo</span> <span class="pre">xfs_check</span> <span class="pre">/dev/sd[a-l][1-2]</span></code> to see if there is
an xfs issue. The results of running this command may require that
<code class="docutils literal"><span class="pre">xfs_repair</span></code> is run.</p>
</div>
<ol class="arabic">
<li><p class="first">Export the Port:Box:Bay for the failed drive into the PBOX variable:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>export PBOX=&lt;Port:Box:Bay&gt;
</pre></div>
</div>
</li>
<li><p class="first">Print the physical device information and take note of the Disk Name
(example output: &#8220;Disk Name: /dev/sdk&#8221; would be exported as
DEV=/dev/sdk):</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo hpssacli controller slot=1 ld ${LDRIVE} show detail | grep -i &quot;Disk Name&quot;
</pre></div>
</div>
</li>
<li><p class="first">Export the device name variable from the preceding command (example:
/dev/sdk):</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>export DEV=&lt;Device&gt;
</pre></div>
</div>
</li>
<li><p class="first">Export the filesystem variable. Disks that are split between the
operating system and data storage, typically sda and sdb, should  only
have repairs done on their data filesystem, usually /dev/sda2 and
/dev/sdb2, Other data only disks have just one partition on the device,
so the filesystem will be 1. In any case you should verify the data
filesystem by running <code class="docutils literal"><span class="pre">df</span> <span class="pre">-h</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">/srv/node</span></code> and using the listed
data filesystem for the device in question as the export. For example:
/dev/sdk1.</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>export FS=&lt;Filesystem&gt;
</pre></div>
</div>
</li>
<li><p class="first">Verify the LUN is failed, and the device is not:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo hpssacli controller slot=1 ld all show
sudo hpssacli controller slot=1 pd all show
sudo hpssacli controller slot=1 ld ${LDRIVE} show detail
sudo hpssacli controller slot=1 pd ${PBOX} show detail
</pre></div>
</div>
</li>
<li><p class="first">Stop the swift and rsync service:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo service rsync stop
sudo swift-init shutdown all
</pre></div>
</div>
</li>
<li><p class="first">Unmount the problem drive, fix the LUN and the filesystem:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo umount ${FS}
</pre></div>
</div>
</li>
<li><p class="first">If umount fails, you should run lsof search for the mountpoint and
kill any lingering processes before repeating the unpount:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo hpacucli controller slot=1 ld ${LDRIVE} modify reenable
sudo xfs_repair ${FS}
</pre></div>
</div>
</li>
<li><p class="first">If the <code class="docutils literal"><span class="pre">xfs_repair</span></code> complains about possible journal data, use the
<code class="docutils literal"><span class="pre">xfs_repair</span> <span class="pre">-L</span></code> option to zeroise the journal log.</p>
</li>
<li><p class="first">Once complete test-mount the filesystem, and tidy up its lost and
found area.</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo mount ${FS} /mnt
sudo rm -rf /mnt/lost+found/
sudo umount /mnt
</pre></div>
</div>
</li>
<li><p class="first">Mount the filesystem and restart swift and rsync.</p>
</li>
<li><p class="first">Run the following to determine if a DC ticket is needed to check the
cables on the node:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>grep -y media.exchanged /tmp/hpacu.diag
grep -y hot.plug.count /tmp/hpacu.diag
</pre></div>
</div>
</li>
<li><p class="first">If the output reports any non 0x00 values, it suggests that the cables
should be checked. For example, log a DC ticket to check the sas cables
between the drive and the expander.</p>
</li>
</ol>
</div>
<div class="section" id="diagnose-slow-disk-devices">
<span id="diagnose-slow-disk-drives"></span><h2>Diagnose: Slow disk devices<a class="headerlink" href="#diagnose-slow-disk-devices" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">collectl is an open-source performance gathering/analysis tool.</p>
</div>
<p>If the diagnostics report a message such as <code class="docutils literal"><span class="pre">sda:</span> <span class="pre">drive</span> <span class="pre">is</span> <span class="pre">slow</span></code>, you
should log onto the node and run the following command (remove <code class="docutils literal"><span class="pre">-c</span> <span class="pre">1</span></code> option to continuously monitor
the data):</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ /usr/bin/collectl -s D -c 1
waiting for 1 second sample...
# DISK STATISTICS (/sec)
#          &lt;---------reads---------&gt;&lt;---------writes---------&gt;&lt;--------averages--------&gt; Pct
#Name       KBytes Merged  IOs Size  KBytes Merged  IOs Size  RWSize  QLen  Wait SvcTim Util
sdb            204      0   33    6      43      0    4   11       6     1     7      6   23
sda             84      0   13    6     108     21    6   18      10     1     7      7   13
sdc            100      0   16    6       0      0    0    0       6     1     7      6    9
sdd            140      0   22    6      22      0    2   11       6     1     9      9   22
sde             76      0   12    6     255      0   52    5       5     1     2      1   10
sdf            276      0   44    6       0      0    0    0       6     1    11      8   38
sdg            112      0   17    7      18      0    2    9       6     1     7      7   13
sdh           3552      0   73   49       0      0    0    0      48     1     9      8   62
sdi             72      0   12    6       0      0    0    0       6     1     8      8   10
sdj            112      0   17    7      22      0    2   11       7     1    10      9   18
sdk            120      0   19    6      21      0    2   11       6     1     8      8   16
sdl            144      0   22    7      18      0    2    9       6     1     9      7   18
dm-0             0      0    0    0       0      0    0    0       0     0     0      0    0
dm-1             0      0    0    0      60      0   15    4       4     0     0      0    0
dm-2             0      0    0    0      48      0   12    4       4     0     0      0    0
dm-3             0      0    0    0       0      0    0    0       0     0     0      0    0
dm-4             0      0    0    0       0      0    0    0       0     0     0      0    0
dm-5             0      0    0    0       0      0    0    0       0     0     0      0    0
</pre></div>
</div>
<p>Look at the <code class="docutils literal"><span class="pre">Wait</span></code> and <code class="docutils literal"><span class="pre">SvcTime</span></code> values. It is not normal for
these values to exceed 50msec. This is known to impact customer
performance (upload/download). For a controller problem, many/all drives
will show long wait and service times. A reboot may correct the problem;
otherwise hardware replacement is needed.</p>
<p>Another way to look at the data is as follows:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ /opt/hp/syseng/disk-anal.pl -d
Disk: sda  Wait: 54580 371  65  25  12   6   6   0   1   2   0  46
Disk: sdb  Wait: 54532 374  96  36  16   7   4   1   0   2   0  46
Disk: sdc  Wait: 54345 554 105  29  15   4   7   1   4   4   0  46
Disk: sdd  Wait: 54175 553 254  31  20  11   6   6   2   2   1  53
Disk: sde  Wait: 54923  66  56  15   8   7   7   0   1   0   2  29
Disk: sdf  Wait: 50952 941 565 403 426 366 442 447 338  99  38  97
Disk: sdg  Wait: 50711 689 808 562 642 675 696 185  43  14   7  82
Disk: sdh  Wait: 51018 668 688 483 575 542 692 275  55  22   9  87
Disk: sdi  Wait: 51012 1011 849 672 568 240 344 280  38  13   6  81
Disk: sdj  Wait: 50724 743 770 586 662 509 684 283  46  17  11  79
Disk: sdk  Wait: 50886 700 585 517 633 511 729 352  89  23   8  81
Disk: sdl  Wait: 50106 617 794 553 604 504 532 501 288 234 165 216
Disk: sda  Time: 55040  22  16   6   1   1  13   0   0   0   3  12

Disk: sdb  Time: 55014  41  19   8   3   1   8   0   0   0   3  17
Disk: sdc  Time: 55032  23  14   8   9   2   6   1   0   0   0  19
Disk: sdd  Time: 55022  29  17  12   6   2  11   0   0   0   1  14
Disk: sde  Time: 55018  34  15  11  12   1   9   0   0   0   2  12
Disk: sdf  Time: 54809 250  45   7   1   0   0   0   0   0   1   1
Disk: sdg  Time: 55070  36   6   2   0   0   0   0   0   0   0   0
Disk: sdh  Time: 55079  33   2   0   0   0   0   0   0   0   0   0
Disk: sdi  Time: 55074  28   7   2   0   0   2   0   0   0   0   1
Disk: sdj  Time: 55067  35  10   0   1   0   0   0   0   0   0   1
Disk: sdk  Time: 55068  31  10   3   0   0   1   0   0   0   0   1
Disk: sdl  Time: 54905 130  61   7   3   4   1   0   0   0   0   3
</pre></div>
</div>
<p>This shows the historical distribution of the wait and service times
over a day. This is how you read it:</p>
<ul class="simple">
<li>sda did 54580 operations with a short wait time, 371 operations with
a longer wait time and 65 with an even longer wait time.</li>
<li>sdl did 50106 operations with a short wait time, but as you can see
many took longer.</li>
</ul>
<p>There is a clear pattern that sdf to sdl have a problem. Actually, sda
to sde would more normally have lots of zeros in their data. But maybe
this is a busy system. In this example it is worth changing the
controller as the individual drives may be ok.</p>
<p>After the controller is changed, use collectl -s D as described above to
see if the problem has cleared. disk-anal.pl will continue to show
historical data. You can look at recent data as follows. It only looks
at data from 13:15 to 14:15. As you can see, this is a relatively clean
system (few if any long wait or service times):</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ /opt/hp/syseng/disk-anal.pl -d -t 13:15-14:15
Disk: sda  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdb  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdc  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdd  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sde  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdf  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdg  Wait:  3594   6   0   0   0   0   0   0   0   0   0   0
Disk: sdh  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdi  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdj  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdk  Wait:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdl  Wait:  3599   1   0   0   0   0   0   0   0   0   0   0
Disk: sda  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdb  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdc  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdd  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sde  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdf  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdg  Time:  3594   6   0   0   0   0   0   0   0   0   0   0
Disk: sdh  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdi  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdj  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdk  Time:  3600   0   0   0   0   0   0   0   0   0   0   0
Disk: sdl  Time:  3599   1   0   0   0   0   0   0   0   0   0   0
</pre></div>
</div>
<p>For long wait times, where the service time appears normal is to check
the logical drive cache status. While the cache may be enabled, it can
be disabled on a per-drive basis.</p>
</div>
<div class="section" id="diagnose-slow-network-link-measuring-network-performance">
<h2>Diagnose: Slow network link - Measuring network performance<a class="headerlink" href="#diagnose-slow-network-link-measuring-network-performance" title="Permalink to this headline">¶</a></h2>
<p>Network faults can cause performance between Swift nodes to degrade. Testing
with <code class="docutils literal"><span class="pre">netperf</span></code> is recommended. Other methods (such as copying large
files) may also work, but can produce inconclusive results.</p>
<p>Install <code class="docutils literal"><span class="pre">netperf</span></code> on all systems if not
already installed. Check that the UFW rules for its control port are in place.
However, there are no pre-opened ports for netperf&#8217;s data connection. Pick a
port number. In this example, 12866 is used because it is one higher
than netperf&#8217;s default control port number, 12865. If you get very
strange results including zero values, you may not have gotten the data
port opened in UFW at the target or may have gotten the netperf
command-line wrong.</p>
<p>Pick a <code class="docutils literal"><span class="pre">source</span></code> and <code class="docutils literal"><span class="pre">target</span></code> node. The source is often a proxy node
and the target is often an object node. Using the same source proxy you
can test communication to different object nodes in different AZs to
identity possible bottlenecks.</p>
<div class="section" id="running-tests">
<h3>Running tests<a class="headerlink" href="#running-tests" title="Permalink to this headline">¶</a></h3>
<ol class="arabic">
<li><p class="first">Prepare the <code class="docutils literal"><span class="pre">target</span></code> node as follows:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo iptables -I INPUT -p tcp -j ACCEPT
</pre></div>
</div>
<p>Or, do:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo ufw allow 12866/tcp
</pre></div>
</div>
</li>
<li><p class="first">On the <code class="docutils literal"><span class="pre">source</span></code> node, run the following command to check
throughput. Note the double-dash before the -P option.
The command takes 10 seconds to complete. The <code class="docutils literal"><span class="pre">target</span></code> node is 192.168.245.5.</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ netperf -H 192.168.245.5 -- -P 12866
MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 12866 AF_INET to
&lt;redacted&gt;.72.4 (&lt;redacted&gt;.72.4) port 12866 AF_INET : demo
Recv   Send    Send
Socket Socket  Message  Elapsed
Size   Size    Size     Time     Throughput
bytes  bytes   bytes    secs.    10^6bits/sec
87380  16384  16384    10.02     923.69
</pre></div>
</div>
</li>
<li><p class="first">On the <code class="docutils literal"><span class="pre">source</span></code> node, run the following command to check latency:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ netperf -H 192.168.245.5 -t TCP_RR -- -P 12866
MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 12866
AF_INET to &lt;redacted&gt;.72.4 (&lt;redacted&gt;.72.4) port 12866 AF_INET : demo
: first burst 0
Local  Remote Socket   Size    Request  Resp.   Elapsed  Trans.
Send   Recv   Size     Size    Time     Rate
bytes  Bytes  bytes    bytes   secs.    per sec
16384  87380  1        1       10.00    11753.37
16384  87380
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="expected-results">
<h3>Expected results<a class="headerlink" href="#expected-results" title="Permalink to this headline">¶</a></h3>
<p>Faults will show up as differences between different pairs of nodes.
However, for reference, here are some expected numbers:</p>
<ul class="simple">
<li>For throughput, proxy to proxy, expect ~9300 Mbit/sec  (proxies have
a 10Ge link).</li>
<li>For throughout, proxy to object, expect ~920 Mbit/sec  (at time of
writing this, object nodes have a 1Ge link).</li>
<li>For throughput, object to object, expect ~920 Mbit/sec.</li>
<li>For latency (all types), expect ~11000 transactions/sec.</li>
</ul>
</div>
</div>
<div class="section" id="diagnose-remapping-sectors-experiencing-ures">
<h2>Diagnose: Remapping sectors experiencing UREs<a class="headerlink" href="#diagnose-remapping-sectors-experiencing-ures" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p class="first">Find the bad sector, device, and filesystem in <code class="docutils literal"><span class="pre">kern.log</span></code>.</p>
</li>
<li><p class="first">Set the environment variables SEC, DEV &amp; FS, for example:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>SEC=2930954256
DEV=/dev/sdi
FS=/dev/sdi1
</pre></div>
</div>
</li>
<li><p class="first">Verify that the sector is bad:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo dd if=${DEV} of=/dev/null bs=512 count=1 skip=${SEC}
</pre></div>
</div>
</li>
<li><p class="first">If the sector is bad this command will output an input/output error:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>dd: reading `/dev/sdi`: Input/output error
0+0 records in
0+0 records out
</pre></div>
</div>
</li>
<li><p class="first">Prevent chef from attempting to re-mount the filesystem while the
repair is in progress:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo mv /etc/chef/client.pem /etc/chef/xx-client.xx-pem
</pre></div>
</div>
</li>
<li><p class="first">Stop the swift and rsync service:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo service rsync stop
sudo swift-init shutdown all
</pre></div>
</div>
</li>
<li><p class="first">Unmount the problem drive:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo umount ${FS}
</pre></div>
</div>
</li>
<li><p class="first">Overwrite/remap the bad sector:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo dd_rescue -d -A -m8b -s ${SEC}b ${DEV} ${DEV}
</pre></div>
</div>
</li>
<li><p class="first">This command should report an input/output error the first time
it is run. Run the command a second time, if it successfully remapped
the bad sector it should not report an input/output error.</p>
</li>
<li><p class="first">Verify the sector is now readable:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo dd if=${DEV} of=/dev/null bs=512 count=1 skip=${SEC}
</pre></div>
</div>
</li>
<li><p class="first">If the sector is now readable this command should not report an
input/output error.</p>
</li>
<li><p class="first">If more than one problem sector is listed, set the SEC environment
variable to the next sector in the list:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="n">SEC</span><span class="o">=</span><span class="mi">123456789</span>
</pre></div>
</div>
</li>
<li><p class="first">Repeat from step 8.</p>
</li>
<li><p class="first">Repair the filesystem:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo xfs_repair ${FS}
</pre></div>
</div>
</li>
<li><p class="first">If <code class="docutils literal"><span class="pre">xfs_repair</span></code> reports that the filesystem has valuable filesystem
changes:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo xfs_repair ${FS}
Phase 1 - find and verify superblock...
Phase 2 - using internal log
        - zero log...
ERROR: The filesystem has valuable metadata changes in a log which
needs to be replayed.
Mount the filesystem to replay the log, and unmount it before
re-running xfs_repair.
If you are unable to mount the filesystem, then use the -L option to
destroy the log and attempt a repair. Note that destroying the log may
cause corruption -- please attempt a mount of the filesystem before
doing this.
</pre></div>
</div>
</li>
<li><p class="first">You should attempt to mount the filesystem, and clear the lost+found
area:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo mount $FS /mnt
sudo rm -rf /mnt/lost+found/*
sudo umount /mnt
</pre></div>
</div>
</li>
<li><p class="first">If the filesystem fails to mount then you will need to use the
<code class="docutils literal"><span class="pre">xfs_repair</span> <span class="pre">-L</span></code> option to force log zeroing.
Repeat step 11.</p>
</li>
<li><p class="first">If <code class="docutils literal"><span class="pre">xfs_repair</span></code> reports that an additional input/output error has been
encountered, get the sector details as follows:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo grep &quot;I/O error&quot; /var/log/kern.log | grep sector | tail -1
</pre></div>
</div>
</li>
<li><p class="first">If new input/output error is reported then set the SEC environment
variable to the problem sector number:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="n">SEC</span><span class="o">=</span><span class="mi">234567890</span>
</pre></div>
</div>
</li>
<li><p class="first">Repeat from step 8</p>
</li>
<li><p class="first">Remount the filesystem and restart swift and rsync.</p>
<ul class="simple">
<li>If all UREs in the kern.log have been fixed and you are still unable
to have xfs_repair disk, it is possible that the URE&#8217;s have
corrupted the filesystem or possibly destroyed the drive altogether.
In this case, the first step is to re-format the filesystem and if
this fails, get the disk replaced.</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="diagnose-high-system-latency">
<h2>Diagnose: High system latency<a class="headerlink" href="#diagnose-high-system-latency" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The latency measurements described here are specific to the HPE
Helion Public Cloud.</p>
</div>
<ul class="simple">
<li>A bad NIC on a proxy server. However, as explained above, this
usually causes the peak to rise, but average should remain near
normal parameters. A quick fix is to shutdown the proxy.</li>
<li>A stuck memcache server. Accepts connections, but then will not respond.
Expect to see timeout messages in <code class="docutils literal"><span class="pre">/var/log/proxy.log</span></code> (port 11211).
Swift Diags will also report this as a failed node/port. A quick fix
is to shutdown the proxy server.</li>
<li>A bad/broken object server can also cause problems if the accounts
used by the monitor program happen to live on the bad object server.</li>
<li>A general network problem within the data canter. Compare the results
with the Pingdom monitors to see if they also have a problem.</li>
</ul>
</div>
<div class="section" id="diagnose-interface-reports-errors">
<h2>Diagnose: Interface reports errors<a class="headerlink" href="#diagnose-interface-reports-errors" title="Permalink to this headline">¶</a></h2>
<p>Should a network interface on a Swift node begin reporting network
errors, it may well indicate a cable, switch, or network issue.</p>
<p>Get an overview of the interface with:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo ifconfig eth{n}
sudo ethtool eth{n}
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">Link</span> <span class="pre">Detected:</span></code> indicator will read <code class="docutils literal"><span class="pre">yes</span></code> if the nic is
cabled.</p>
<p>Establish the adapter type with:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo ethtool  -i eth{n}
</pre></div>
</div>
<p>Gather the interface statistics with:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo ethtool  -S eth{n}
</pre></div>
</div>
<p>If the nick supports self test, this can be performed with:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo ethtool  -t eth{n}
</pre></div>
</div>
<p>Self tests should read <code class="docutils literal"><span class="pre">PASS</span></code> if the nic is operating correctly.</p>
<p>Nic module drivers can be re-initialised by carefully removing and
re-installing the modules (this avoids rebooting the server).
For example, mellanox drivers use a two part driver mlx4_en and
mlx4_core. To reload these you must carefully remove the mlx4_en
(ethernet) then the mlx4_core modules, and reinstall them in the
reverse order.</p>
<p>As the interface will be disabled while the modules are unloaded, you
must be very careful not to lock yourself out so it may be better
to script this.</p>
</div>
<div class="section" id="diagnose-hung-swift-object-replicator">
<h2>Diagnose: Hung swift object replicator<a class="headerlink" href="#diagnose-hung-swift-object-replicator" title="Permalink to this headline">¶</a></h2>
<p>A replicator reports in its log that remaining time exceeds
100 hours. This may indicate that the swift <code class="docutils literal"><span class="pre">object-replicator</span></code> is stuck and not
making progress. Another useful way to check this is with the
&#8216;swift-recon -r&#8217; command on a swift proxy server:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo swift-recon -r
===============================================================================

--&gt; Starting reconnaissance on 384 hosts
===============================================================================
[2013-07-17 12:56:19] Checking on replication
[replication_time] low: 2, high: 80, avg: 28.8, total: 11037, Failed: 0.0%, no_result: 0, reported: 383
Oldest completion was 2013-06-12 22:46:50 (12 days ago) by 192.168.245.3:6200.
Most recent completion was 2013-07-17 12:56:19 (5 seconds ago) by 192.168.245.5:6200.
===============================================================================
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">Oldest</span> <span class="pre">completion</span></code> line in this example indicates that the
object-replicator on swift object server 192.168.245.3 has not completed
the replication cycle in 12 days. This replicator is stuck. The object
replicator cycle is generally less than 1 hour. Though an replicator
cycle of 15-20 hours can occur if nodes are added to the system and a
new ring has been deployed.</p>
<p>You can further check if the object replicator is stuck by logging on
the object server and checking the object replicator progress with
the following command:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>#  sudo grep object-rep /var/log/swift/background.log | grep -e &quot;Starting object replication&quot; -e &quot;Object replication complete&quot; -e &quot;partitions rep&quot;
Jul 16 06:25:46 192.168.245.4 object-replicator 15344/16450 (93.28%) partitions replicated in 69018.48s (0.22/sec, 22h remaining)
Jul 16 06:30:46 192.168.245.4object-replicator 15344/16450 (93.28%) partitions replicated in 69318.58s (0.22/sec, 22h remaining)
Jul 16 06:35:46 192.168.245.4 object-replicator 15344/16450 (93.28%) partitions replicated in 69618.63s (0.22/sec, 23h remaining)
Jul 16 06:40:46 192.168.245.4 object-replicator 15344/16450 (93.28%) partitions replicated in 69918.73s (0.22/sec, 23h remaining)
Jul 16 06:45:46 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 70218.75s (0.22/sec, 24h remaining)
Jul 16 06:50:47 192.168.245.4object-replicator 15348/16450 (93.30%) partitions replicated in 70518.85s (0.22/sec, 24h remaining)
Jul 16 06:55:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 70818.95s (0.22/sec, 25h remaining)
Jul 16 07:00:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 71119.05s (0.22/sec, 25h remaining)
Jul 16 07:05:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 71419.15s (0.21/sec, 26h remaining)
Jul 16 07:10:47 192.168.245.4object-replicator 15348/16450 (93.30%) partitions replicated in 71719.25s (0.21/sec, 26h remaining)
Jul 16 07:15:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 72019.27s (0.21/sec, 27h remaining)
Jul 16 07:20:47 192.168.245.4object-replicator 15348/16450 (93.30%) partitions replicated in 72319.37s (0.21/sec, 27h remaining)
Jul 16 07:25:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 72619.47s (0.21/sec, 28h remaining)
Jul 16 07:30:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 72919.56s (0.21/sec, 28h remaining)
Jul 16 07:35:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 73219.67s (0.21/sec, 29h remaining)
Jul 16 07:40:47 192.168.245.4 object-replicator 15348/16450 (93.30%) partitions replicated in 73519.76s (0.21/sec, 29h remaining)
</pre></div>
</div>
<p>The above status is output every 5 minutes to <code class="docutils literal"><span class="pre">/var/log/swift/background.log</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The &#8216;remaining&#8217; time is increasing as time goes on, normally the
time remaining should be decreasing. Also note the partition number. For example,
15344 remains the same for several status lines. Eventually the object
replicator detects the hang and attempts to make progress by killing the
problem thread. The replicator then progresses to the next partition but
quite often it again gets stuck on the same partition.</p>
</div>
<p>One of the reasons for the object replicator hanging like this is
filesystem corruption on the drive. The following is a typical log entry
of a corrupted filesystem detected by the object replicator:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span># sudo bzgrep &quot;Remote I/O error&quot; /var/log/swift/background.log* |grep srv | - tail -1
Jul 12 03:33:30 192.168.245.4 object-replicator STDOUT: ERROR:root:Error hashing suffix#012Traceback (most recent call last):#012 File
&quot;/usr/lib/python2.7/dist-packages/swift/obj/replicator.py&quot;, line 199, in get_hashes#012 hashes[suffix] = hash_suffix(suffix_dir,
reclaim_age)#012 File &quot;/usr/lib/python2.7/dist-packages/swift/obj/replicator.py&quot;, line 84, in hash_suffix#012 path_contents =
sorted(os.listdir(path))#012OSError: [Errno 121] Remote I/O error: &#39;/srv/node/disk4/objects/1643763/b51&#39;
</pre></div>
</div>
<p>An <code class="docutils literal"><span class="pre">ls</span></code> of the problem file or directory usually shows something like the following:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span># ls -l /srv/node/disk4/objects/1643763/b51
ls: cannot access /srv/node/disk4/objects/1643763/b51: Remote I/O error
</pre></div>
</div>
<p>If no entry with <code class="docutils literal"><span class="pre">Remote</span> <span class="pre">I/O</span> <span class="pre">error</span></code> occurs in the <code class="docutils literal"><span class="pre">background.log</span></code> it is
not possible to determine why the object-replicator is hung. It may be
that the <code class="docutils literal"><span class="pre">Remote</span> <span class="pre">I/O</span> <span class="pre">error</span></code> entry is older than 7 days and so has been
rotated out of the logs. In this scenario it may be best to simply
restart the object-replicator.</p>
<ol class="arabic">
<li><p class="first">Stop the object-replicator:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># sudo swift-init object-replicator stop</span>
</pre></div>
</div>
</li>
<li><p class="first">Make sure the object replicator has stopped, if it has hung, the stop
command will not stop the hung process:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># ps auxww | - grep swift-object-replicator</span>
</pre></div>
</div>
</li>
<li><p class="first">If the previous ps shows the object-replicator is still running, kill
the process:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># kill -9 &lt;pid-of-swift-object-replicator&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">Start the object-replicator:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># sudo swift-init object-replicator start</span>
</pre></div>
</div>
</li>
</ol>
<p>If the above grep did find an <code class="docutils literal"><span class="pre">Remote</span> <span class="pre">I/O</span> <span class="pre">error</span></code> then it may be possible
to repair the problem filesystem.</p>
<ol class="arabic">
<li><p class="first">Stop swift and rsync:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># sudo swift-init all shutdown</span>
<span class="c1"># sudo service rsync stop</span>
</pre></div>
</div>
</li>
<li><p class="first">Make sure all swift process have stopped:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># ps auxww | grep swift | grep python</span>
</pre></div>
</div>
</li>
<li><p class="first">Kill any swift processes still running.</p>
</li>
<li><p class="first">Unmount the problem filesystem:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># sudo umount /srv/node/disk4</span>
</pre></div>
</div>
</li>
<li><p class="first">Repair the filesystem:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span><span class="c1"># sudo xfs_repair -P /dev/sde1</span>
</pre></div>
</div>
</li>
<li><p class="first">If the <code class="docutils literal"><span class="pre">xfs_repair</span></code> fails then it may be necessary to re-format the
filesystem. See <a class="reference internal" href="procedures.html#fix-broken-xfs-filesystem"><span>Procedure: Fix broken XFS filesystem</span></a>. If the
<code class="docutils literal"><span class="pre">xfs_repair</span></code> is successful, re-enable chef using the following command
and replication should commence again.</p>
</li>
</ol>
</div>
<div class="section" id="diagnose-high-cpu-load">
<h2>Diagnose: High CPU load<a class="headerlink" href="#diagnose-high-cpu-load" title="Permalink to this headline">¶</a></h2>
<p>The CPU load average on an object server, as shown with the
&#8216;uptime&#8217; command, is typically under 10 when the server is
lightly-moderately loaded:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ uptime
07:59:26 up 99 days,  5:57,  1 user,  load average: 8.59, 8.39, 8.32
</pre></div>
</div>
<p>During times of increased activity, due to user transactions or object
replication, the CPU load average can increase to  to around 30.</p>
<p>However, sometimes the CPU load average can increase significantly. The
following is an example of an object server that has extremely high CPU
load:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>$ uptime
07:44:02 up 18:22,  1 user,  load average: 407.12, 406.36, 404.59
</pre></div>
</div>
</div>
<div class="section" id="further-issues-and-resolutions">
<h2>Further issues and resolutions<a class="headerlink" href="#further-issues-and-resolutions" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The urgency levels in each <strong>Action</strong> column indicates whether or
not it is required to take immediate action, or if the problem can be worked
on during business hours.</p>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Scenario</strong></th>
<th class="head"><strong>Description</strong></th>
<th class="head"><strong>Action</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">/healthcheck</span></code> latency is high.</td>
<td>The <code class="docutils literal"><span class="pre">/healthcheck</span></code> test does not tax the proxy very much so any drop in value is probably related to
network issues, rather than the proxies being very busy. A very slow proxy might impact the average
number, but it would need to be very slow to shift the number that much.</td>
<td><p class="first">Check networks. Do a <code class="docutils literal"><span class="pre">curl</span> <span class="pre">https://&lt;ip-address&gt;:&lt;port&gt;/healthcheck</span></code> where
<code class="docutils literal"><span class="pre">ip-address</span></code> is individual proxy IP address.
Repeat this for every proxy server to see if you can pin point the problem.</p>
<p class="last">Urgency: If there are other indications that your system is slow, you should treat
this as an urgent problem.</p>
</td>
</tr>
<tr class="row-odd"><td>Swift process is not running.</td>
<td>You can use <code class="docutils literal"><span class="pre">swift-init</span></code> status to check if swift processes are running on any
given server.</td>
<td><p class="first">Run this command:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo swift-init all start
</pre></div>
</div>
<p>Examine messages in the swift log files to see if there are any
error messages related to any of the swift processes since the time you
ran the <code class="docutils literal"><span class="pre">swift-init</span></code> command.</p>
<p>Take any corrective actions that seem necessary.</p>
<p class="last">Urgency: If this only affects one server, and you have more than one,
identifying and fixing the problem can wait until business hours.
If this same problem affects many servers, then you need to take corrective
action immediately.</p>
</td>
</tr>
<tr class="row-even"><td>ntpd is not running.</td>
<td>NTP is not running.</td>
<td><p class="first">Configure and start NTP.</p>
<p class="last">Urgency: For proxy servers, this is vital.</p>
</td>
</tr>
<tr class="row-odd"><td>Host clock is not syncd to an NTP server.</td>
<td>Node time settings does not match NTP server time.
This may take some time to sync after a reboot.</td>
<td>Assuming NTP is configured and running, you have to wait until the times sync.</td>
</tr>
<tr class="row-even"><td>A swift process has hundreds, to thousands of open file descriptors.</td>
<td>May happen to any of the swift processes.
Known to have happened with a <code class="docutils literal"><span class="pre">rsyslod</span></code> restart and where <code class="docutils literal"><span class="pre">/tmp</span></code> was hanging.</td>
<td><p class="first">Restart the swift processes on the affected node:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>% sudo swift-init all reload
</pre></div>
</div>
<dl class="last docutils">
<dt>Urgency:</dt>
<dd><p class="first">If known performance problem: Immediate</p>
<p class="last">If system seems fine: Medium</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td>A swift process is not owned by the swift user.</td>
<td>If the UID of the swift user has changed, then the processes might not be
owned by that UID.</td>
<td>Urgency: If this only affects one server, and you have more than one,
identifying and fixing the problem can wait until business hours.
If this same problem affects many servers, then you need to take corrective
action immediately.</td>
</tr>
<tr class="row-even"><td>Object account or container files not owned by swift.</td>
<td>This typically happens if during a reinstall or a re-image of a server that the UID
of the swift user was changed. The data files in the object account and container
directories are owned by the original swift UID. As a result, the current swift
user does not own these files.</td>
<td><p class="first">Correct the UID of the swift user to reflect that of the original UID. An alternate
action is to change the ownership of every file on all file systems. This alternate
action is often impractical and will take considerable time.</p>
<p class="last">Urgency: If this only affects one server, and you have more than one,
identifying and fixing the problem can wait until business hours.
If this same problem affects many servers, then you need to take corrective
action immediately.</p>
</td>
</tr>
<tr class="row-odd"><td>A disk drive has a high IO wait or service time.</td>
<td>If high wait IO times are seen for a single disk, then the disk drive is the problem.
If most/all devices are slow, the controller is probably the source of the problem.
The controller cache may also be miss configured – which will cause similar long
wait or service times.</td>
<td><p class="first">As a first step, if your controllers have a cache, check that it is enabled and their battery/capacitor
is working.</p>
<p>Second, reboot the server.
If problem persists, file a DC ticket to have the drive or controller replaced.
See <a class="reference internal" href="#diagnose-slow-disk-drives"><span>Diagnose: Slow disk devices</span></a> on how to check the drive wait or service times.</p>
<p class="last">Urgency: Medium</p>
</td>
</tr>
<tr class="row-even"><td>The network interface is not up.</td>
<td>Use the <code class="docutils literal"><span class="pre">ifconfig</span></code> and <code class="docutils literal"><span class="pre">ethtool</span></code> commands to determine the network state.</td>
<td><p class="first">You can try restarting the interface. However, generally the interface
(or cable) is probably broken, especially if the interface is flapping.</p>
<p class="last">Urgency: If this only affects one server, and you have more than one,
identifying and fixing the problem can wait until business hours.
If this same problem affects many servers, then you need to take corrective
action immediately.</p>
</td>
</tr>
<tr class="row-odd"><td>Network interface card (NIC) is not operating at the expected speed.</td>
<td>The NIC is running at a slower speed than its nominal rated speed.
For example, it is running at 100 Mb/s and the NIC is a 1Ge NIC.</td>
<td><ol class="first arabic simple">
<li>Try resetting the interface with:</li>
</ol>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo ethtool -s eth0 speed 1000
</pre></div>
</div>
<p>... and then run:</p>
<div class="code highlight-python"><div class="highlight"><pre><span></span>sudo lshw -class
</pre></div>
</div>
<p>See if size goes to the expected speed. Failing
that, check hardware (NIC cable/switch port).</p>
<ol class="arabic simple" start="2">
<li>If persistent, consider shutting down the server (especially if a proxy)
until the problem is identified and resolved. If you leave this server
running it can have a large impact on overall performance.</li>
</ol>
<p class="last">Urgency: High</p>
</td>
</tr>
<tr class="row-even"><td>The interface RX/TX error count is non-zero.</td>
<td>A value of 0 is typical, but counts of 1 or 2 do not indicate a problem.</td>
<td><ol class="first arabic">
<li><p class="first">For low numbers (For example, 1 or 2), you can simply ignore. Numbers in the range
3-30 probably indicate that the error count has crept up slowly over a long time.
Consider rebooting the server to remove the report from the noise.</p>
<p>Typically, when a cable or interface is bad, the error count goes to 400+. For example,
it stands out. There may be other symptoms such as the interface going up and down or
not running at correct speed. A server with a high error count should be watched.</p>
</li>
<li><p class="first">If the error count continues to climb, consider taking the server down until
it can be properly investigated. In any case, a reboot should be done to clear
the error count.</p>
</li>
</ol>
<p class="last">Urgency: High, if the error count increasing.</p>
</td>
</tr>
<tr class="row-odd"><td>In a swift log you see a message that a process has not replicated in over 24 hours.</td>
<td>The replicator has not successfully completed a run in the last 24 hours.
This indicates that the replicator has probably hung.</td>
<td><p class="first">Use <code class="docutils literal"><span class="pre">swift-init</span></code> to stop and then restart the replicator process.</p>
<p class="last">Urgency: Low. However if you
recently added or replaced disk drives then you should treat this urgently.</p>
</td>
</tr>
<tr class="row-even"><td>Container Updater has not run in 4 hour(s).</td>
<td>The service may appear to be running however, it may be hung. Examine their swift
logs to see if there are any error messages relating to the container updater. This
may potentially explain why the container is not running.</td>
<td><p class="first">Urgency: Medium
This may have been triggered by a recent restart of the  rsyslog daemon.
Restart the service with:
.. code:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span>sudo swift-init &lt;service&gt; reload
</pre></div>
</div>
</td>
</tr>
<tr class="row-odd"><td>Object replicator: Reports the remaining time and that time is more than 100 hours.</td>
<td>Each replication cycle the object replicator writes a log message to its log
reporting statistics about the current cycle. This includes an estimate for the
remaining time needed to replicate all objects. If this time is longer than
100 hours, there is a problem with the replication process.</td>
<td><p class="first">Urgency: Medium
Restart the service with:
.. code:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo swift-init object-replicator reload
</pre></div>
</div>
<p class="last">Check that the remaining replication time is going down.</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="../index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Identifying issues and resolutions</a><ul>
<li><a class="reference internal" href="#is-the-system-up">Is the system up?</a></li>
<li><a class="reference internal" href="#functional-tests-usage">Functional tests usage</a></li>
<li><a class="reference internal" href="#external-monitoring">External monitoring</a></li>
<li><a class="reference internal" href="#diagnose-general-approach">Diagnose: General approach</a><ul>
<li><a class="reference internal" href="#dependencies">Dependencies</a></li>
</ul>
</li>
<li><a class="reference internal" href="#diagnose-swift-dispersion-report">Diagnose: Swift-dispersion-report</a></li>
<li><a class="reference internal" href="#diagnose-is-system-responding-to-healthcheck">Diagnose: Is system responding to /healthcheck?</a></li>
<li><a class="reference internal" href="#diagnose-interpreting-messages-in-var-log-swift-files">Diagnose: Interpreting messages in <code class="docutils literal"><span class="pre">/var/log/swift/</span></code> files</a></li>
<li><a class="reference internal" href="#diagnose-parted-reports-the-backup-gpt-table-is-corrupt">Diagnose: Parted reports the backup GPT table is corrupt</a></li>
<li><a class="reference internal" href="#diagnose-drives-diagnostic-reports-a-fs-label-is-not-acceptable">Diagnose: Drives diagnostic reports a FS label is not acceptable</a></li>
<li><a class="reference internal" href="#diagnose-failed-luns">Diagnose: Failed LUNs</a></li>
<li><a class="reference internal" href="#diagnose-slow-disk-devices">Diagnose: Slow disk devices</a></li>
<li><a class="reference internal" href="#diagnose-slow-network-link-measuring-network-performance">Diagnose: Slow network link - Measuring network performance</a><ul>
<li><a class="reference internal" href="#running-tests">Running tests</a></li>
<li><a class="reference internal" href="#expected-results">Expected results</a></li>
</ul>
</li>
<li><a class="reference internal" href="#diagnose-remapping-sectors-experiencing-ures">Diagnose: Remapping sectors experiencing UREs</a></li>
<li><a class="reference internal" href="#diagnose-high-system-latency">Diagnose: High system latency</a></li>
<li><a class="reference internal" href="#diagnose-interface-reports-errors">Diagnose: Interface reports errors</a></li>
<li><a class="reference internal" href="#diagnose-hung-swift-object-replicator">Diagnose: Hung swift object replicator</a></li>
<li><a class="reference internal" href="#diagnose-high-cpu-load">Diagnose: High CPU load</a></li>
<li><a class="reference internal" href="#further-issues-and-resolutions">Further issues and resolutions</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="index.html"
                                  title="previous chapter">Swift Ops Runbook</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="procedures.html"
                                  title="next chapter">Software configuration procedures</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/swift
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../_sources/ops_runbook/diagnose.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="procedures.html" title="Software configuration procedures"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="Swift Ops Runbook"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">swift 2.12.1.dev102 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Swift Ops Runbook</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2017, OpenStack Foundation.
      Last updated on &#39;Tue Feb 14 07:57:22 2017, commit 7cb6882&#39;.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/swift");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>