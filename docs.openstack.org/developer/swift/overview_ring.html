<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>The Rings &mdash; swift 2.12.1.dev102 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '2.12.1.dev102',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="swift 2.12.1.dev102 documentation" href="index.html" />
    <link rel="next" title="Storage Policies" href="overview_policies.html" />
    <link rel="prev" title="Swift Architectural Overview" href="overview_architecture.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="the-rings">
<h1>The Rings<a class="headerlink" href="#the-rings" title="Permalink to this headline">¶</a></h1>
<p>The rings determine where data should reside in the cluster. There is a
separate ring for account databases, container databases, and individual
object storage policies but each ring works in the same way. These rings are
externally managed, in that the server processes themselves do not modify the
rings, they are instead given new rings modified by other tools.</p>
<p>The ring uses a configurable number of bits from a path&#8217;s MD5 hash as a
partition index that designates a device. The number of bits kept from the hash
is known as the partition power, and 2 to the partition power indicates the
partition count. Partitioning the full MD5 hash ring allows other parts of the
cluster to work in batches of items at once which ends up either more efficient
or at least less complex than working with each item separately or the entire
cluster all at once.</p>
<p>Another configurable value is the replica count, which indicates how many of
the partition-&gt;device assignments comprise a single ring. For a given partition
number, each replica will be assigned to a different device in the ring.</p>
<p>Devices are added to the ring to describe the capacity available for
part-replica assignment.  Devices are placed into failure domains consisting
of region, zone, and server.  Regions can be used to describe geographical
systems characterized by lower-bandwidth or higher latency between machines in
different regions.  Many rings will consist of only a single region.  Zones
can be used to group devices based on physical locations, power separations,
network separations, or any other attribute that would lessen multiple
replicas being unavailable at the same time.</p>
<p>Devices are given a weight which describes relative weight of the device in
comparison to other devices.</p>
<p>When building a ring all of each part&#8217;s replicas will be assigned to devices
according to their weight.  Additionally, each replica of a part will attempt
to be assigned to a device who&#8217;s failure domain does not already have a
replica for the part.  Only a single replica of a part may be assigned to each
device - you must have as many devices as replicas.</p>
<div class="section" id="ring-builder">
<h2>Ring Builder<a class="headerlink" href="#ring-builder" title="Permalink to this headline">¶</a></h2>
<p>The rings are built and managed manually by a utility called the ring-builder.
The ring-builder assigns partitions to devices and writes an optimized Python
structure to a gzipped, serialized file on disk for shipping out to the servers.
The server processes just check the modification time of the file occasionally
and reload their in-memory copies of the ring structure as needed. Because of
how the ring-builder manages changes to the ring, using a slightly older ring
usually just means one of the three replicas for a subset of the partitions
will be incorrect, which can be easily worked around.</p>
<p>The ring-builder also keeps its own builder file with the ring information and
additional data required to build future rings. It is very important to keep
multiple backup copies of these builder files. One option is to copy the
builder files out to every server while copying the ring files themselves.
Another is to upload the builder files into the cluster itself. Complete loss
of a builder file will mean creating a new ring from scratch, nearly all
partitions will end up assigned to different devices, and therefore nearly all
data stored will have to be replicated to new locations. So, recovery from a
builder file loss is possible, but data will definitely be unreachable for an
extended time.</p>
</div>
<div class="section" id="ring-data-structure">
<h2>Ring Data Structure<a class="headerlink" href="#ring-data-structure" title="Permalink to this headline">¶</a></h2>
<p>The ring data structure consists of three top level fields: a list of devices
in the cluster, a list of lists of device ids indicating partition to device
assignments, and an integer indicating the number of bits to shift an MD5 hash
to calculate the partition for the hash.</p>
<div class="section" id="list-of-devices">
<h3>List of Devices<a class="headerlink" href="#list-of-devices" title="Permalink to this headline">¶</a></h3>
<p>The list of devices is known internally to the Ring class as devs. Each item in
the list of devices is a dictionary with the following keys:</p>
<table border="1" class="docutils">
<colgroup>
<col width="8%" />
<col width="9%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>id</td>
<td>integer</td>
<td>The index into the list devices.</td>
</tr>
<tr class="row-even"><td>zone</td>
<td>integer</td>
<td>The zone the device resides in.</td>
</tr>
<tr class="row-odd"><td>region</td>
<td>integer</td>
<td>The region the zone resides in.</td>
</tr>
<tr class="row-even"><td>weight</td>
<td>float</td>
<td>The relative weight of the device in comparison to other
devices. This usually corresponds directly to the amount of
disk space the device has compared to other devices. For
instance a device with 1 terabyte of space might have a weight
of 100.0 and another device with 2 terabytes of space might
have a weight of 200.0. This weight can also be used to bring
back into balance a device that has ended up with more or less
data than desired over time. A good average weight of 100.0
allows flexibility in lowering the weight later if necessary.</td>
</tr>
<tr class="row-odd"><td>ip</td>
<td>string</td>
<td>The IP address or hostname of the server containing the device.</td>
</tr>
<tr class="row-even"><td>port</td>
<td>int</td>
<td>The TCP port the listening server process uses that serves
requests for the device.</td>
</tr>
<tr class="row-odd"><td>device</td>
<td>string</td>
<td>The on disk name of the device on the server.
For example: sdb1</td>
</tr>
<tr class="row-even"><td>meta</td>
<td>string</td>
<td>A general-use field for storing additional information for the
device. This information isn&#8217;t used directly by the server
processes, but can be useful in debugging. For example, the
date and time of installation and hardware manufacturer could
be stored here.</td>
</tr>
</tbody>
</table>
<p>Note: The list of devices may contain holes, or indexes set to None, for
devices that have been removed from the cluster. However, device ids are
reused. Device ids are reused to avoid potentially running out of device id
slots when there are available slots (from prior removal of devices). A
consequence of this device id reuse is that the device id (integer value) does
not necessarily correspond with the chronology of when the device was added to
the ring. Also, some devices may be temporarily disabled by setting their
weight to 0.0. To obtain a list of active devices (for uptime polling, for
example) the Python code would look like: <code class="docutils literal"><span class="pre">devices</span> <span class="pre">=</span> <span class="pre">list(self._iter_devs())</span></code></p>
</div>
<div class="section" id="partition-assignment-list">
<h3>Partition Assignment List<a class="headerlink" href="#partition-assignment-list" title="Permalink to this headline">¶</a></h3>
<p>This is a list of array(&#8216;H&#8217;) of devices ids. The outermost list contains an
array(&#8216;H&#8217;) for each replica. Each array(&#8216;H&#8217;) has a length equal to the
partition count for the ring. Each integer in the array(&#8216;H&#8217;) is an index into
the above list of devices. The partition list is known internally to the Ring
class as _replica2part2dev_id.</p>
<p>So, to create a list of device dictionaries assigned to a partition, the Python
code would look like: <code class="docutils literal"><span class="pre">devices</span> <span class="pre">=</span> <span class="pre">[self.devs[part2dev_id[partition]]</span> <span class="pre">for</span>
<span class="pre">part2dev_id</span> <span class="pre">in</span> <span class="pre">self._replica2part2dev_id]</span></code></p>
<p>array(&#8216;H&#8217;) is used for memory conservation as there may be millions of
partitions.</p>
</div>
<div class="section" id="partition-shift-value">
<h3>Partition Shift Value<a class="headerlink" href="#partition-shift-value" title="Permalink to this headline">¶</a></h3>
<p>The partition shift value is known internally to the Ring class as _part_shift.
This value used to shift an MD5 hash to calculate the partition on which the
data for that hash should reside. Only the top four bytes of the hash is used
in this process. For example, to compute the partition for the path
/account/container/object the Python code might look like: <code class="docutils literal"><span class="pre">partition</span> <span class="pre">=</span>
<span class="pre">unpack_from('&gt;I',</span> <span class="pre">md5('/account/container/object').digest())[0]</span> <span class="pre">&gt;&gt;</span>
<span class="pre">self._part_shift</span></code></p>
<p>For a ring generated with part_power P, the partition shift value is
32 - P.</p>
</div>
<div class="section" id="fractional-replicas">
<h3>Fractional Replicas<a class="headerlink" href="#fractional-replicas" title="Permalink to this headline">¶</a></h3>
<p>A ring is not restricted to having an integer number of replicas. In order to
support the gradual changing of replica counts, the ring is able to have a real
number of replicas.</p>
<p>When the number of replicas is not an integer, then the last element of
_replica2part2dev_id will have a length that is less than the partition count
for the ring. This means that some partitions will have more replicas than
others. For example, if a ring has 3.25 replicas, then 25% of its partitions
will have four replicas, while the remaining 75% will have just three.</p>
</div>
<div class="section" id="dispersion">
<span id="ring-dispersion"></span><h3>Dispersion<a class="headerlink" href="#dispersion" title="Permalink to this headline">¶</a></h3>
<p>With each rebalance, the ring builder calculates a dispersion metric. This is
the percentage of partitions in the ring that have too many replicas within a
particular failure domain.</p>
<p>For example, if you have three servers in a cluster but two replicas for a
partition get placed onto the same server, that partition will count towards
the dispersion metric.</p>
<p>A lower dispersion value is better, and the value can be used to find the
proper value for &#8220;overload&#8221;.</p>
</div>
<div class="section" id="overload">
<span id="ring-overload"></span><h3>Overload<a class="headerlink" href="#overload" title="Permalink to this headline">¶</a></h3>
<p>The ring builder tries to keep replicas as far apart as possible while
still respecting device weights. When it can&#8217;t do both, the overload
factor determines what happens. Each device will take some extra
fraction of its desired partitions to allow for replica dispersion;
once that extra fraction is exhausted, replicas will be placed closer
together than optimal.</p>
<p>Essentially, the overload factor lets the operator trade off replica
dispersion (durability) against data dispersion (uniform disk usage).</p>
<p>The default overload factor is 0, so device weights will be strictly
followed.</p>
<p>With an overload factor of 0.1, each device will accept 10% more
partitions than it otherwise would, but only if needed to maintain
partition dispersion.</p>
<p>Example: Consider a 3-node cluster of machines with equal-size disks;
let node A have 12 disks, node B have 12 disks, and node C have only
11 disks. Let the ring have an overload factor of 0.1 (10%).</p>
<p>Without the overload, some partitions would end up with replicas only
on nodes A and B. However, with the overload, every device is willing
to accept up to 10% more partitions for the sake of dispersion. The
missing disk in C means there is one disk&#8217;s worth of partitions that
would like to spread across the remaining 11 disks, which gives each
disk in C an extra 9.09% load. Since this is less than the 10%
overload, there is one replica of each partition on each node.</p>
<p>However, this does mean that the disks in node C will have more data
on them than the disks in nodes A and B. If 80% full is the warning
threshold for the cluster, node C&#8217;s disks will reach 80% full while A
and B&#8217;s disks are only 72.7% full.</p>
</div>
</div>
<div class="section" id="partition-replica-terminology">
<h2>Partition &amp; Replica Terminology<a class="headerlink" href="#partition-replica-terminology" title="Permalink to this headline">¶</a></h2>
<p>All descriptions of consistent hashing describe the process of breaking the
keyspace up into multiple ranges (vnodes, buckets, etc.) - many more than the
number of &#8220;nodes&#8221; to which keys in the keyspace must be assigned.  Swift calls
these ranges <cite>partitions</cite> - they are partitions of the total keyspace.</p>
<p>Each partition will have multiple replicas.  Every replica of each partition
must be assigned to a device in the ring.  When a describing a specific
replica of a partition (like when it&#8217;s assigned a device) it is described as a
<cite>part-replica</cite> in that it is a specific <cite>replica</cite> of the specific <cite>partition</cite>.
A single device may be assigned different replicas from many parts, but it may
not be assigned multiple replicas of a single part.</p>
<p>The total number of partitions in a ring is calculated as <code class="docutils literal"><span class="pre">2</span> <span class="pre">**</span>
<span class="pre">&lt;part-power&gt;</span></code>.  The total number of part-replicas in a ring is calculated as
<code class="docutils literal"><span class="pre">&lt;replica-count&gt;</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">&lt;part-power&gt;</span></code>.</p>
<p>When considering a device&#8217;s <cite>weight</cite> it is useful to describe the number of
part-replicas it would like to be assigned.  A single device regardless of
weight will never hold more than <code class="docutils literal"><span class="pre">2</span> <span class="pre">**</span> <span class="pre">&lt;part-power&gt;</span></code> part-replicas because
it can not have more than one replica of any part assigned.  The number of
part-replicas a device can take by weights is calculated as it&#8217;s
<cite>parts_wanted</cite>.  The true number of part-replicas assigned to a device can be
compared to it&#8217;s parts wanted similarly to a calculation of percentage error -
this deviation in the observed result from the idealized target is called a
devices <cite>balance</cite>.</p>
<p>When considering a device&#8217;s <cite>failure domain</cite> it is useful to describe the
number of part-replicas it would like to be assigned.  The number of
part-replicas wanted in a failure domain of a tier is the sum of the
part-replicas wanted in the failure domains of it&#8217;s sub-tier.  However,
collectively when the total number of part-replicas in a failure domain
exceeds or is equal to <code class="docutils literal"><span class="pre">2</span> <span class="pre">**</span> <span class="pre">&lt;part-power&gt;</span></code> it is most obvious that it&#8217;s no
longer sufficient to consider only the number of total part-replicas, but
rather the fraction of each replica&#8217;s partitions.  Consider for example a ring
with <code class="docutils literal"><span class="pre">3</span></code> replicas and <code class="docutils literal"><span class="pre">3</span></code> servers, while it&#8217;s necessary for dispersion
that each server hold only <code class="docutils literal"><span class="pre">1/3</span></code> of the total part-replicas it is
additionally constrained to require <code class="docutils literal"><span class="pre">1.0</span></code> replica of <em>each</em> partition.  It
would not be sufficient to satisfy dispersion if two devices on one of the
servers each held a replica of a single partition, while another server held
none.  By considering a decimal fraction of one replica&#8217;s worth of parts in a
failure domain we can derive the total part-replicas wanted in a failure
domain (<code class="docutils literal"><span class="pre">1.0</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">&lt;part-power&gt;</span></code>).  Additionally we infer more about
<cite>which</cite> part-replicas must go in the failure domain.  Consider a ring with
three replicas, and two zones, each with two servers (four servers total).
The three replicas worth of partitions will be assigned into two failure
domains at the zone tier.  Each zone must hold more than one replica of some
parts.  We represent this improper faction of a replica&#8217;s worth of partitions
in decimal form as <code class="docutils literal"><span class="pre">1.5</span></code> (<code class="docutils literal"><span class="pre">3.0</span> <span class="pre">/</span> <span class="pre">2</span></code>).  This tells us not only the <em>number</em>
of total parts (<code class="docutils literal"><span class="pre">1.5</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">&lt;part-power&gt;</span></code>) but also that <em>each</em> partition
must have <cite>at least</cite> one replica in this failure domain (in fact <code class="docutils literal"><span class="pre">0.5</span></code> of
the partitions will have <code class="docutils literal"><span class="pre">2</span></code> replicas).  Within each zone the two servers
will hold <code class="docutils literal"><span class="pre">0.75</span></code> of a replica&#8217;s worth of partitions - this is equal both to
&#8220;the fraction of a replica&#8217;s worth of partitions assigned to each zone
(<code class="docutils literal"><span class="pre">1.5</span></code>) divided evenly among the number of failure domain&#8217;s in it&#8217;s sub-tier
(<code class="docutils literal"><span class="pre">2</span></code> servers in each zone, i.e.  <code class="docutils literal"><span class="pre">1.5</span> <span class="pre">/</span> <span class="pre">2</span></code>)&#8221; but <em>also</em> &#8220;the total number
of replicas (<code class="docutils literal"><span class="pre">3.0</span></code>) divided evenly among the total number of failure domains
in the server tier (<code class="docutils literal"><span class="pre">2</span></code> servers x <code class="docutils literal"><span class="pre">2</span></code> zones = <code class="docutils literal"><span class="pre">4</span></code>, i.e.  <code class="docutils literal"><span class="pre">3.0</span> <span class="pre">/</span> <span class="pre">4</span></code>)&#8221;.
It is useful to consider that each server in this ring will hold only <code class="docutils literal"><span class="pre">0.75</span></code>
of a replica&#8217;s worth of partitions which tells that any server should have <cite>at
most</cite> one replica of a given part assigned.  In the interests of brevity, some
variable names will often refer to the concept representing the fraction of a
replica&#8217;s worth of partitions in decimal form as <em>replicanths</em> - this is meant
to invoke connotations similar to ordinal numbers as applied to fractions, but
generalized to a replica instead of four*th* or a fif*th*.  The &#8216;n&#8217; was
probably thrown in because of Blade Runner.</p>
</div>
<div class="section" id="building-the-ring">
<h2>Building the Ring<a class="headerlink" href="#building-the-ring" title="Permalink to this headline">¶</a></h2>
<p>First the ring builder calculates the replicanths wanted at each tier in the
ring&#8217;s topology based on weight.</p>
<p>Then the ring builder calculates the replicanths wanted at each tier in the
ring&#8217;s topology based on dispersion.</p>
<p>Then the ring calculates the maximum deviation on a single device between it&#8217;s
weighted replicanths and wanted replicanths.</p>
<p>Next we interpolate between the two replicanth values (weighted &amp; wanted) at
each tier using the specified overload (up to the maximum required overload).
It&#8217;s a linear interpolation, similar to solving for a point on a line between
two points - we calculate the slope across the max required overload and then
calculate the intersection of the line with the desired overload.  This
becomes the target.</p>
<p>From the target we calculate the minimum and maximum number of replicas any
part may have in a tier.  This becomes the replica_plan.</p>
<p>Finally, we calculate the number of partitions that should ideally be assigned
to each device based the replica_plan.</p>
<p>On initial balance, the first time partitions are placed to generate a ring,
we must assign each replica of each partition to the device that desires the
most partitions excluding any devices that already have their maximum number
of replicas of that part assigned to some parent tier of that device&#8217;s failure
domain.</p>
<p>When building a new ring based on an old ring, the desired number of
partitions each device wants is recalculated from the current replica_plan.
Next the partitions to be reassigned are gathered up. Any removed devices have
all their assigned partitions unassigned and added to the gathered list. Any
partition replicas that (due to the addition of new devices) can be spread out
for better durability are unassigned and added to the gathered list. Any
devices that have more partitions than they now desire have random partitions
unassigned from them and added to the gathered list. Lastly, the gathered
partitions are then reassigned to devices using a similar method as in the
initial assignment described above.</p>
<p>Whenever a partition has a replica reassigned, the time of the reassignment is
recorded. This is taken into account when gathering partitions to reassign so
that no partition is moved twice in a configurable amount of time. This
configurable amount of time is known internally to the RingBuilder class as
min_part_hours. This restriction is ignored for replicas of partitions on
devices that have been removed, as removing a device only happens on device
failure and there&#8217;s no choice but to make a reassignment.</p>
<p>The above processes don&#8217;t always perfectly rebalance a ring due to the random
nature of gathering partitions for reassignment. To help reach a more balanced
ring, the rebalance process is repeated a fixed number of times until the
replica_plan is fulfilled or unable to be fulfilled (indicating we probably
can&#8217;t get perfect balance due to too many partitions recently moved).</p>
</div>
<div class="section" id="module-swift.cli.ring_builder_analyzer">
<span id="ring-builder-analyzer"></span><h2>Ring Builder Analyzer<a class="headerlink" href="#module-swift.cli.ring_builder_analyzer" title="Permalink to this headline">¶</a></h2>
<p>This is a tool for analyzing how well the ring builder performs its job
in a particular scenario. It is intended to help developers quantify any
improvements or regressions in the ring builder; it is probably not useful
to others.</p>
<p>The ring builder analyzer takes a scenario file containing some initial
parameters for a ring builder plus a certain number of rounds. In each
round, some modifications are made to the builder, e.g. add a device, remove
a device, change a device&#8217;s weight. Then, the builder is repeatedly
rebalanced until it settles down. Data about that round is printed, and the
next round begins.</p>
<p>Scenarios are specified in JSON. Example scenario for a gradual device
addition:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;part_power&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
    <span class="s2">&quot;replicas&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;overload&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s2">&quot;random_seed&quot;</span><span class="p">:</span> <span class="mi">203488</span><span class="p">,</span>

    <span class="s2">&quot;rounds&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.40:6200/sda&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.40:6200/sdb&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.40:6200/sdc&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.40:6200/sdd&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>

            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.41:6200/sda&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.41:6200/sdb&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.41:6200/sdc&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.41:6200/sdd&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>

            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.43:6200/sda&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.43:6200/sdb&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.43:6200/sdc&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.43:6200/sdd&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>

            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.44:6200/sda&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.44:6200/sdb&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.44:6200/sdc&quot;</span><span class="p">,</span> <span class="mi">8000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;add&quot;</span><span class="p">,</span> <span class="s2">&quot;r1z2-10.20.30.44:6200/sdd&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;remove&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">3000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">4000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">5000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">6000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7000</span><span class="p">]</span>
        <span class="p">],</span> <span class="p">[</span>
            <span class="p">[</span><span class="s2">&quot;set_weight&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">8000</span><span class="p">]</span>
        <span class="p">]]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="history">
<h2>History<a class="headerlink" href="#history" title="Permalink to this headline">¶</a></h2>
<p>The ring code went through many iterations before arriving at what it is now
and while it has largely been stable, the algorithm has seen a few tweaks or
perhaps even fundamentally changed as new ideas emerge. This section will try
to describe the previous ideas attempted and attempt to explain why they were
discarded.</p>
<p>A &#8220;live ring&#8221; option was considered where each server could maintain its own
copy of the ring and the servers would use a gossip protocol to communicate the
changes they made. This was discarded as too complex and error prone to code
correctly in the project time span available. One bug could easily gossip bad
data out to the entire cluster and be difficult to recover from. Having an
externally managed ring simplifies the process, allows full validation of data
before it&#8217;s shipped out to the servers, and guarantees each server is using a
ring from the same timeline. It also means that the servers themselves aren&#8217;t
spending a lot of resources maintaining rings.</p>
<p>A couple of &#8220;ring server&#8221; options were considered. One was where all ring
lookups would be done by calling a service on a separate server or set of
servers, but this was discarded due to the latency involved. Another was much
like the current process but where servers could submit change requests to the
ring server to have a new ring built and shipped back out to the servers. This
was discarded due to project time constraints and because ring changes are
currently infrequent enough that manual control was sufficient. However, lack
of quick automatic ring changes did mean that other parts of the system had to
be coded to handle devices being unavailable for a period of hours until
someone could manually update the ring.</p>
<p>The current ring process has each replica of a partition independently assigned
to a device. A version of the ring that used a third of the memory was tried,
where the first replica of a partition was directly assigned and the other two
were determined by &#8220;walking&#8221; the ring until finding additional devices in other
zones. This was discarded as control was lost as to how many replicas for a
given partition moved at once. Keeping each replica independent allows for
moving only one partition replica within a given time window (except due to
device failures). Using the additional memory was deemed a good trade-off for
moving data around the cluster much less often.</p>
<p>Another ring design was tried where the partition to device assignments weren&#8217;t
stored in a big list in memory but instead each device was assigned a set of
hashes, or anchors. The partition would be determined from the data item&#8217;s hash
and the nearest device anchors would determine where the replicas should be
stored. However, to get reasonable distribution of data each device had to have
a lot of anchors and walking through those anchors to find replicas started to
add up. In the end, the memory savings wasn&#8217;t that great and more processing
power was used, so the idea was discarded.</p>
<p>A completely non-partitioned ring was also tried but discarded as the
partitioning helps many other parts of the system, especially replication.
Replication can be attempted and retried in a partition batch with the other
replicas rather than each data item independently attempted and retried. Hashes
of directory structures can be calculated and compared with other replicas to
reduce directory walking and network traffic.</p>
<p>Partitioning and independently assigning partition replicas also allowed for
the best balanced cluster. The best of the other strategies tended to give
+-10% variance on device balance with devices of equal weight and +-15% with
devices of varying weights. The current strategy allows us to get +-3% and +-8%
respectively.</p>
<p>Various hashing algorithms were tried. SHA offers better security, but the ring
doesn&#8217;t need to be cryptographically secure and SHA is slower. Murmur was much
faster, but MD5 was built-in and hash computation is a small percentage of the
overall request handling time. In all, once it was decided the servers wouldn&#8217;t
be maintaining the rings themselves anyway and only doing hash lookups, MD5 was
chosen for its general availability, good distribution, and adequate speed.</p>
<p>The placement algorithm has seen a number of behavioral changes for
unbalanceable rings. The ring builder wants to keep replicas as far apart as
possible while still respecting device weights. In most cases, the ring
builder can achieve both, but sometimes they conflict.  At first, the behavior
was to keep the replicas far apart and ignore device weight, but that made it
impossible to gradually go from one region to two, or from two to three. Then
it was changed to favor device weight over dispersion, but that wasn&#8217;t so good
for rings that were close to balanceable, like 3 machines with 60TB, 60TB, and
57TB of disk space; operators were expecting one replica per machine, but
didn&#8217;t always get it. After that, overload was added to the ring builder so
that operators could choose a balance between dispersion and device weights.
In time the overload concept was improved and made more accurate.</p>
<p>For more background on consistent hashing rings, please see <a class="reference internal" href="ring_background.html"><em>Building a Consistent Hashing Ring</em></a>.</p>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">The Rings</a><ul>
<li><a class="reference internal" href="#ring-builder">Ring Builder</a></li>
<li><a class="reference internal" href="#ring-data-structure">Ring Data Structure</a><ul>
<li><a class="reference internal" href="#list-of-devices">List of Devices</a></li>
<li><a class="reference internal" href="#partition-assignment-list">Partition Assignment List</a></li>
<li><a class="reference internal" href="#partition-shift-value">Partition Shift Value</a></li>
<li><a class="reference internal" href="#fractional-replicas">Fractional Replicas</a></li>
<li><a class="reference internal" href="#dispersion">Dispersion</a></li>
<li><a class="reference internal" href="#overload">Overload</a></li>
</ul>
</li>
<li><a class="reference internal" href="#partition-replica-terminology">Partition &amp; Replica Terminology</a></li>
<li><a class="reference internal" href="#building-the-ring">Building the Ring</a></li>
<li><a class="reference internal" href="#module-swift.cli.ring_builder_analyzer">Ring Builder Analyzer</a></li>
<li><a class="reference internal" href="#history">History</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="overview_architecture.html"
                                  title="previous chapter">Swift Architectural Overview</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="overview_policies.html"
                                  title="next chapter">Storage Policies</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/swift
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/overview_ring.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="overview_policies.html" title="Storage Policies"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="overview_architecture.html" title="Swift Architectural Overview"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">swift 2.12.1.dev102 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2017, OpenStack Foundation.
      Last updated on &#39;Tue Feb 14 07:57:22 2017, commit 7cb6882&#39;.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/swift");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>