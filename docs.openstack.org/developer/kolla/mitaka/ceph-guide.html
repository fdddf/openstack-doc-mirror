<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Ceph in Kolla &mdash; kolla 2.0.3.dev72 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '2.0.3.dev72',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="kolla 2.0.3.dev72 documentation" href="index.html" />
    <link rel="next" title="Cinder in Kolla" href="cinder-guide.html" />
    <link rel="prev" title="Liberty 1.0.0 Deployment Warning" href="liberty-deployment-warning.html" /> 
  </head>
  <body>
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="ceph-in-kolla">
<h1>Ceph in Kolla<a class="headerlink" href="#ceph-in-kolla" title="Permalink to this headline">¶</a></h1>
<p>The out-of-the-box Ceph deployment requires 3 hosts with at least one block
device on each host that can be dedicated for sole use by Ceph. However, with
tweaks to the Ceph cluster you can deploy a &#8220;healthy&#8221; cluster with a single
host and a single block device.</p>
<div class="section" id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>A minimum of 3 hosts for a vanilla deploy</li>
<li>A minimum of 1 block device per host</li>
</ul>
</div>
<div class="section" id="preparation-and-deployment">
<h2>Preparation and Deployment<a class="headerlink" href="#preparation-and-deployment" title="Permalink to this headline">¶</a></h2>
<p>To prepare a disk for use as a
<a class="reference external" href="http://docs.ceph.com/docs/master/man/8/ceph-osd/">Ceph OSD</a> you must add a
special partition label to the disk. This partition label is how Kolla detects
the disks to format and bootstrap. Any disk with a matching partition label will
be reformatted so use caution.</p>
<p>To prepare an OSD as a storage drive, execute the following operations:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># &lt;WARNING ALL DATA ON $DISK will be LOST!&gt;
# where $DISK is /dev/sdb or something similar
parted $DISK -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1
</pre></div>
</div>
<p>The following shows an example of using parted to configure /dev/sdb for usage with Kolla.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>parted /dev/sdb -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_BOOTSTRAP 1 -1
parted /dev/sdb print
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 10.7GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Number  Start   End     Size    File system  Name                      Flags
     1      1049kB  10.7GB  10.7GB               KOLLA_CEPH_OSD_BOOTSTRAP
</pre></div>
</div>
<p>Edit the [storage] group in the inventory which contains the hostname of the
hosts that have the block devices you have prepped as shown above.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">storage</span><span class="p">]</span>
<span class="n">controller</span>
<span class="n">compute1</span>
</pre></div>
</div>
<p>Enable Ceph in /etc/kolla/globals.yml:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>enable_ceph: &quot;yes&quot;
</pre></div>
</div>
<p>RadosGW is optional, enable it in /etc/kolla/globals.yml:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>enable_ceph_rgw: &quot;yes&quot;
</pre></div>
</div>
<p>RGW requires a healthy cluster in order to be successfully deployed.
On initial start up, RGW will create several pools.
The first pool should be in an operational state to proceed with the second one, and so on.
So, in the case of an all-in-one deployment, it is necessary to change the default number of copies
for the pools before deployment. Modify the file /etc/kolla/config/ceph.conf and add the contents:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>[global]
osd pool default size = 1
osd pool default min size = 1
</pre></div>
</div>
<p>Finally deploy the Ceph-enabled OpenStack:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kolla-ansible deploy -i path/to/inventory
</pre></div>
</div>
</div>
<div class="section" id="using-a-cache-tier">
<h2>Using a Cache Tier<a class="headerlink" href="#using-a-cache-tier" title="Permalink to this headline">¶</a></h2>
<p>An optional
<a class="reference external" href="http://docs.ceph.com/docs/hammer/rados/operations/cache-tiering/">cache tier</a>
can be deployed by formatting at least one cache device and enabling cache
tiering in the globals.yml configuration file.</p>
<p>To prepare an OSD as a cache device, execute the following operations:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># &lt;WARNING ALL DATA ON $DISK will be LOST!&gt;
# where $DISK is /dev/sdb or something similar
parted $DISK -s -- mklabel gpt mkpart KOLLA_CEPH_OSD_CACHE_BOOTSTRAP 1 -1
</pre></div>
</div>
<p>Enable the Ceph cache tier in /etc/kolla/globals.yml:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>enable_ceph: &quot;yes&quot;
ceph_enable_cache: &quot;yes&quot;
# Valid options are [ forward, none, writeback ]
ceph_cache_mode: &quot;writeback&quot;
</pre></div>
</div>
<p>After this run the playbooks as you normally would. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kolla-ansible deploy -i path/to/inventory
</pre></div>
</div>
</div>
<div class="section" id="setting-up-an-erasure-coded-pool">
<h2>Setting up an Erasure Coded Pool<a class="headerlink" href="#setting-up-an-erasure-coded-pool" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://docs.ceph.com/docs/hammer/rados/operations/erasure-code/">Erasure code</a>
is the new big thing from Ceph. Kolla has the ability to setup your Ceph pools
as erasure coded pools. Due to technical limitations with Ceph, using erasure
coded pools as OpenStack uses them requires a cache tier. Additionally, you must
make the choice to use an erasure coded pool or a replicated pool (the default)
when you initially deploy. You cannot change this without completely removing
the pool and recreating it.</p>
<p>To enable erasure coded pools add the following options to your
/etc/kolla/globals.yml configuration file:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># A requirement for using the erasure-coded pools is you must setup a cache tier
# Valid options are [ erasure, replicated ]
ceph_pool_type: &quot;erasure&quot;
# Optionally, you can change the profile
#ceph_erasure_profile: &quot;k=4 m=2 ruleset-failure-domain=host&quot;
</pre></div>
</div>
</div>
<div class="section" id="managing-ceph">
<h2>Managing Ceph<a class="headerlink" href="#managing-ceph" title="Permalink to this headline">¶</a></h2>
<p>Check the Ceph status for more diagnostic information. The sample output below
indicates a healthy cluster:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>docker exec ceph_mon ceph -s
cluster 5fba2fbc-551d-11e5-a8ce-01ef4c5cf93c
 health HEALTH_OK
 monmap e1: 1 mons at {controller=10.0.0.128:6789/0}
        election epoch 2, quorum 0 controller
 osdmap e18: 2 osds: 2 up, 2 in
  pgmap v27: 64 pgs, 1 pools, 0 bytes data, 0 objects
        68676 kB used, 20390 MB / 20457 MB avail
              64 active+clean
</pre></div>
</div>
<p>If Ceph is run in an all-in-one deployment or with less than three storage nodes, further
configuration is required. It is necessary to change the default number of copies for the pool.
The following example demonstrates how to change the number of copies for the pool to 1:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>docker exec ceph_mon ceph osd pool set rbd size 1
</pre></div>
</div>
<p>All the pools must be modified if Glance, Nova, and Cinder have been deployed.
An example of modifying the pools to have 2 copies:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>for p in images vms volumes backups; do docker exec ceph_mon ceph osd pool set ${p} size 2; done
</pre></div>
</div>
<p>If using a cache tier, these changes must be made as well:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>for p in images vms volumes backups; do docker exec ceph_mon ceph osd pool set ${p}-cache size 2; done
</pre></div>
</div>
<p>The default pool Ceph creates is named &#8216;rbd&#8217;. It is safe to remove this pool:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>docker exec ceph_mon ceph osd pool delete rbd rbd --yes-i-really-really-mean-it
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Ceph in Kolla</a><ul>
<li><a class="reference internal" href="#requirements">Requirements</a></li>
<li><a class="reference internal" href="#preparation-and-deployment">Preparation and Deployment</a></li>
<li><a class="reference internal" href="#using-a-cache-tier">Using a Cache Tier</a></li>
<li><a class="reference internal" href="#setting-up-an-erasure-coded-pool">Setting up an Erasure Coded Pool</a></li>
<li><a class="reference internal" href="#managing-ceph">Managing Ceph</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="liberty-deployment-warning.html"
                                  title="previous chapter">Liberty 1.0.0 Deployment Warning</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="cinder-guide.html"
                                  title="next chapter">Cinder in Kolla</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/kolla
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/ceph-guide.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="cinder-guide.html" title="Cinder in Kolla"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="liberty-deployment-warning.html" title="Liberty 1.0.0 Deployment Warning"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">kolla 2.0.3.dev72 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer">
        &copy; Copyright 2013, OpenStack Foundation.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/kolla");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>