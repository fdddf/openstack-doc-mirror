<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Elastic Data Processing (EDP) &mdash; Sahara</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '6.0.0.0rc2.dev34',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Sahara" href="../index.html" />
    <link rel="next" title="Sahara REST API v1.1" href="../restapi.html" />
    <link rel="prev" title="MapR Distribution Plugin" href="mapr_plugin.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="elastic-data-processing-edp">
<h1>Elastic Data Processing (EDP)<a class="headerlink" href="#elastic-data-processing-edp" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Sahara&#8217;s Elastic Data Processing facility or <em class="dfn">EDP</em> allows the execution
of jobs on clusters created from sahara. EDP supports:</p>
<ul class="simple">
<li>Hive, Pig, MapReduce, MapReduce.Streaming, Java, and Shell job types on
Hadoop clusters</li>
<li>Spark jobs on Spark standalone clusters, MapR (v5.0.0 - v5.2.0) clusters,
Vanilla clusters (v2.7.1) and CDH clusters (v5.3.0 or higher).</li>
<li>storage of job binaries in the OpenStack Object Storage service (swift),
the OpenStack Shared file systems service (manila), or sahara&#8217;s own
database</li>
<li>access to input and output data sources in<ul>
<li>HDFS for all job types</li>
<li>swift for all types excluding Hive</li>
<li>manila (NFS shares only) for all types excluding Pig</li>
</ul>
</li>
<li>configuration of jobs at submission time</li>
<li>execution of jobs on existing clusters or transient clusters</li>
</ul>
</div>
<div class="section" id="interfaces">
<h2>Interfaces<a class="headerlink" href="#interfaces" title="Permalink to this headline">¶</a></h2>
<p>The EDP features can be used from the sahara web UI which is described in the
<a class="reference internal" href="../horizon/dashboard.user.guide.html"><em>Sahara (Data Processing) UI User Guide</em></a>.</p>
<p>The EDP features also can be used directly by a client through the
<a class="reference external" href="http://developer.openstack.org/api-ref/data-processing/">REST api</a></p>
</div>
<div class="section" id="edp-concepts">
<h2>EDP Concepts<a class="headerlink" href="#edp-concepts" title="Permalink to this headline">¶</a></h2>
<p>Sahara EDP uses a collection of simple objects to define and execute jobs.
These objects are stored in the sahara database when they are created,
allowing them to be reused. This modular approach with database persistence
allows code and data to be reused across multiple jobs.</p>
<p>The essential components of a job are:</p>
<ul class="simple">
<li>executable code to run</li>
<li>input and output data paths, as needed for the job</li>
<li>any additional configuration values needed for the job run</li>
</ul>
<p>These components are supplied through the objects described below.</p>
<div class="section" id="job-binaries">
<h3>Job Binaries<a class="headerlink" href="#job-binaries" title="Permalink to this headline">¶</a></h3>
<p>A <em class="dfn">Job Binary</em> object stores a URL to a single script or Jar file and
any credentials needed to retrieve the file.  The file itself may be stored
in the sahara internal database (but it is deprecated now), in swift,
or in manila.</p>
<p><strong>deprecated:</strong> Files in the sahara database are stored as raw bytes in a
<em class="dfn">Job Binary Internal</em> object. This object&#8217;s sole purpose is to store a
file for later retrieval. No extra credentials need to be supplied for files
stored internally.</p>
<p>Sahara requires credentials (username and password) to access files stored in
swift unless swift proxy users are configured as described in
<a class="reference internal" href="advanced.configuration.guide.html"><em>Sahara Advanced Configuration Guide</em></a>. The swift service must be
running in the same OpenStack installation referenced by sahara.</p>
<p>To reference a binary file stored in manila, create the job binary with the
URL <code class="docutils literal"><span class="pre">manila://{share_id}/{path}</span></code>. This assumes that you have already stored
that file in the appropriate path on the share. The share will be
automatically mounted to any cluster nodes which require access to the file,
if it is not mounted already.</p>
<p>There is a configurable limit on the size of a single job binary that may be
retrieved by sahara. This limit is 5MB and may be set with the
<em>job_binary_max_KB</em> setting in the <code class="file docutils literal"><span class="pre">sahara.conf</span></code> configuration file.</p>
</div>
<div class="section" id="jobs">
<h3>Jobs<a class="headerlink" href="#jobs" title="Permalink to this headline">¶</a></h3>
<p>A <em class="dfn">Job</em> object specifies the type of the job and lists all of the
individual Job Binary objects that are required for execution. An individual
Job Binary may be referenced by multiple Jobs.  A Job object specifies a main
binary and/or supporting libraries depending on its type:</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="27%" />
<col width="22%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Job type</th>
<th class="head">Main binary</th>
<th class="head">Libraries</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Hive</span></code></td>
<td>required</td>
<td>optional</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">Pig</span></code></td>
<td>required</td>
<td>optional</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">MapReduce</span></code></td>
<td>not used</td>
<td>required</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">MapReduce.Streaming</span></code></td>
<td>not used</td>
<td>optional</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Java</span></code></td>
<td>not used</td>
<td>required</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">Shell</span></code></td>
<td>required</td>
<td>optional</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Spark</span></code></td>
<td>required</td>
<td>optional</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">Storm</span></code></td>
<td>required</td>
<td>not used</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Storm</span> <span class="pre">Pyelus</span></code></td>
<td>required</td>
<td>not used</td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="data-sources">
<h3>Data Sources<a class="headerlink" href="#data-sources" title="Permalink to this headline">¶</a></h3>
<p>A <em class="dfn">Data Source</em> object stores a URL which designates the location of
input or output data and any credentials needed to access the location.</p>
<p>Sahara supports data sources in swift. The swift service must be running in
the same OpenStack installation referenced by sahara.</p>
<p>Sahara also supports data sources in HDFS. Any HDFS instance running on a
sahara cluster in the same OpenStack installation is accessible without
manual configuration. Other instances of HDFS may be used as well provided
that the URL is resolvable from the node executing the job.</p>
<p>Sahara supports data sources in manila as well. To reference a path on an NFS
share as a data source, create the data source with the URL
<code class="docutils literal"><span class="pre">manila://{share_id}/{path}</span></code>. As in the case of job binaries, the specified
share will be automatically mounted to your cluster&#8217;s nodes as needed to
access the data source.</p>
<p>Some job types require the use of data source objects to specify input and
output when a job is launched. For example, when running a Pig job the UI will
prompt the user for input and output data source objects.</p>
<p>Other job types like Java or Spark do not require the user to specify data
sources. For these job types, data paths are passed as arguments. For
convenience, sahara allows data source objects to be referenced by name or id.
The section <a class="reference internal" href="#using-data-source-references-as-arguments">Using Data Source References as Arguments</a> gives further
details.</p>
</div>
<div class="section" id="job-execution">
<h3>Job Execution<a class="headerlink" href="#job-execution" title="Permalink to this headline">¶</a></h3>
<p>Job objects must be <em>launched</em> or <em>executed</em> in order for them to run on the
cluster. During job launch, a user specifies execution details including data
sources, configuration values, and program arguments. The relevant details
will vary by job type. The launch will create a <em class="dfn">Job Execution</em> object in
sahara which is used to monitor and manage the job.</p>
<p>To execute Hadoop jobs, sahara generates an Oozie workflow and submits it to
the Oozie server running on the cluster. Familiarity with Oozie is not
necessary for using sahara but it may be beneficial to the user. A link to
the Oozie web console can be found in the sahara web UI in the cluster
details.</p>
<p>For Spark jobs, sahara uses the <em>spark-submit</em> shell script and executes the
Spark job from the master node in case of Spark cluster and from the Spark
Job History server in other cases. Logs of spark jobs run by sahara can be
found on this node under the <em>/tmp/spark-edp</em> directory.</p>
</div>
</div>
<div class="section" id="general-workflow">
<span id="edp-workflow"></span><h2>General Workflow<a class="headerlink" href="#general-workflow" title="Permalink to this headline">¶</a></h2>
<p>The general workflow for defining and executing a job in sahara is essentially
the same whether using the web UI or the REST API.</p>
<ol class="arabic simple">
<li>Launch a cluster from sahara if there is not one already available</li>
<li>Create all of the Job Binaries needed to run the job, stored in the sahara
database, in swift, or in manila<ul>
<li>When using the REST API and internal storage of job binaries, the Job
Binary Internal objects must be created first</li>
<li>Once the Job Binary Internal objects are created, Job Binary objects may
be created which refer to them by URL</li>
</ul>
</li>
<li>Create a Job object which references the Job Binaries created in step 2</li>
<li>Create an input Data Source which points to the data you wish to process</li>
<li>Create an output Data Source which points to the location for output data</li>
<li>Create a Job Execution object specifying the cluster and Job object plus
relevant data sources, configuration values, and program arguments<ul>
<li>When using the web UI this is done with the
<span class="guilabel">Launch On Existing Cluster</span> or
<span class="guilabel">Launch on New Cluster</span> buttons on the Jobs tab</li>
<li>When using the REST API this is done via the <em>/jobs/&lt;job_id&gt;/execute</em>
method</li>
</ul>
</li>
</ol>
<p>The workflow is simpler when using existing objects. For example, to
construct a new job which uses existing binaries and input data a user may
only need to perform steps 3, 5, and 6 above. Of course, to repeat the same
job multiple times a user would need only step 6.</p>
<div class="section" id="specifying-configuration-values-parameters-and-arguments">
<h3>Specifying Configuration Values, Parameters, and Arguments<a class="headerlink" href="#specifying-configuration-values-parameters-and-arguments" title="Permalink to this headline">¶</a></h3>
<p>Jobs can be configured at launch. The job type determines the kinds of values
that may be set:</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="41%" />
<col width="23%" />
<col width="19%" />
<col width="17%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Job type</th>
<th class="head">Configuration
Values</th>
<th class="head">Parameters</th>
<th class="head">Arguments</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Hive</span></code></td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">Pig</span></code></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">MapReduce</span></code></td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">MapReduce.Streaming</span></code></td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Java</span></code></td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">Shell</span></code></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Spark</span></code></td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">Storm</span></code></td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">Storm</span> <span class="pre">Pyelus</span></code></td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><em class="dfn">Configuration values</em> are key/value pairs.<ul>
<li>The EDP configuration values have names beginning with <em>edp.</em> and are
consumed by sahara</li>
<li>Other configuration values may be read at runtime by Hadoop jobs</li>
<li>Currently additional configuration values are not available to Spark jobs
at runtime</li>
</ul>
</li>
<li><em class="dfn">Parameters</em> are key/value pairs. They supply values for the Hive and
Pig parameter substitution mechanisms. In Shell jobs, they are passed as
environment variables.</li>
<li><em class="dfn">Arguments</em> are strings passed as command line arguments to a shell or
main program</li>
</ul>
<p>These values can be set on the <span class="guilabel">Configure</span> tab during job launch
through the web UI or through the <em>job_configs</em> parameter when using the
<em>/jobs/&lt;job_id&gt;/execute</em> REST method.</p>
<p>In some cases sahara generates configuration values or parameters
automatically. Values set explicitly by the user during launch will override
those generated by sahara.</p>
</div>
<div class="section" id="using-data-source-references-as-arguments">
<h3>Using Data Source References as Arguments<a class="headerlink" href="#using-data-source-references-as-arguments" title="Permalink to this headline">¶</a></h3>
<p>Sometimes it&#8217;s necessary or desirable to pass a data path as an argument to a
job. In these cases, a user may simply type out the path as an argument when
launching a job. If the path requires credentials, the user can manually add
the credentials as configuration values. However, if a data source object has
been created that contains the desired path and credentials there is no need
to specify this information manually.</p>
<p>As a convenience, sahara allows data source objects to be referenced by name
or id in arguments, configuration values, or parameters. When the job is
executed, sahara will replace the reference with the path stored in the data
source object and will add any necessary credentials to the job configuration.
Referencing an existing data source object is much faster than adding this
information by hand. This is particularly useful for job types like Java or
Spark that do not use data source objects directly.</p>
<p>There are two job configuration parameters that enable data source references.
They may be used with any job type and are set on the <code class="docutils literal"><span class="pre">Configuration</span></code> tab
when the job is launched:</p>
<ul>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.substitute_data_source_for_name</span></code> (default <strong>False</strong>) If set to
<strong>True</strong>, causes sahara to look for data source object name references in
configuration values, arguments, and parameters when a job is launched. Name
references have the form <strong>datasource://name_of_the_object</strong>.</p>
<p>For example, assume a user has a WordCount application that takes an input
path as an argument. If there is a data source object named <strong>my_input</strong>, a
user may simply set the <strong>edp.substitute_data_source_for_name</strong>
configuration parameter to <strong>True</strong> and add <strong>datasource://my_input</strong> as an
argument when launching the job.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.substitute_data_source_for_uuid</span></code> (default <strong>False</strong>) If set to
<strong>True</strong>, causes sahara to look for data source object ids in configuration
values, arguments, and parameters when a job is launched. A data source
object id is a uuid, so they are unique. The id of a data source object is
available through the UI or the sahara command line client. A user may
simply use the id as a value.</p>
</li>
</ul>
</div>
<div class="section" id="creating-an-interface-for-your-job">
<h3>Creating an Interface for Your Job<a class="headerlink" href="#creating-an-interface-for-your-job" title="Permalink to this headline">¶</a></h3>
<p>In order to better document your job for cluster operators (or for yourself
in the future), sahara allows the addition of an interface (or method
signature) to your job template. A sample interface for the Teragen Hadoop
example might be:</p>
<table border="1" class="docutils">
<colgroup>
<col width="13%" />
<col width="13%" />
<col width="15%" />
<col width="18%" />
<col width="14%" />
<col width="28%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Mapping
Type</th>
<th class="head">Location</th>
<th class="head">Value
Type</th>
<th class="head">Required</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Example
Class</td>
<td>args</td>
<td>0</td>
<td>string</td>
<td>false</td>
<td>teragen</td>
</tr>
<tr class="row-odd"><td>Rows</td>
<td>args</td>
<td>1</td>
<td>number</td>
<td>true</td>
<td>unset</td>
</tr>
<tr class="row-even"><td>Output
Path</td>
<td>args</td>
<td>2</td>
<td>data_source</td>
<td>false</td>
<td>hdfs://ip:port/path</td>
</tr>
<tr class="row-odd"><td>Mapper
Count</td>
<td>configs</td>
<td>mapred.
map.tasks</td>
<td>number</td>
<td>false</td>
<td>unset</td>
</tr>
</tbody>
</table>
<p>A &#8220;Description&#8221; field may also be added to each interface argument.</p>
<p>To create such an interface via the REST API, provide an &#8220;interface&#8221; argument,
the value of which consists of a list of JSON objects, as below:</p>
<div class="highlight-json"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
        <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Example Class&quot;</span><span class="p">,</span>
        <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Indicates which example job class should be used.&quot;</span><span class="p">,</span>
        <span class="nt">&quot;mapping_type&quot;</span><span class="p">:</span> <span class="s2">&quot;args&quot;</span><span class="p">,</span>
        <span class="nt">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span>
        <span class="nt">&quot;value_type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="nt">&quot;required&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
        <span class="nt">&quot;default&quot;</span><span class="p">:</span> <span class="s2">&quot;teragen&quot;</span>
    <span class="p">},</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Creating this interface would allow you to specify a configuration for any
execution of the job template by passing an &#8220;interface&#8221; map similar to:</p>
<div class="highlight-json"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;Rows&quot;</span><span class="p">:</span> <span class="s2">&quot;1000000&quot;</span><span class="p">,</span>
    <span class="nt">&quot;Mapper Count&quot;</span><span class="p">:</span> <span class="s2">&quot;3&quot;</span><span class="p">,</span>
    <span class="nt">&quot;Output Path&quot;</span><span class="p">:</span> <span class="s2">&quot;hdfs://mycluster:8020/user/myuser/teragen-output&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The specified arguments would be automatically placed into the args, configs,
and params for the job, according to the mapping type and location fields of
each interface argument. The final <code class="docutils literal"><span class="pre">job_configs</span></code> map would be:</p>
<div class="highlight-json"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;job_configs&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;configs&quot;</span><span class="p">:</span>
            <span class="p">{</span>
                <span class="nt">&quot;mapred.map.tasks&quot;</span><span class="p">:</span> <span class="s2">&quot;3&quot;</span>
            <span class="p">},</span>
        <span class="nt">&quot;args&quot;</span><span class="p">:</span>
            <span class="p">[</span>
                <span class="s2">&quot;teragen&quot;</span><span class="p">,</span>
                <span class="s2">&quot;1000000&quot;</span><span class="p">,</span>
                <span class="s2">&quot;hdfs://mycluster:8020/user/myuser/teragen-output&quot;</span>
            <span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Rules for specifying an interface are as follows:</p>
<ul class="simple">
<li>Mapping Type must be one of <code class="docutils literal"><span class="pre">configs</span></code>, <code class="docutils literal"><span class="pre">params</span></code>, or <code class="docutils literal"><span class="pre">args</span></code>. Only types
supported for your job type are allowed (see above.)</li>
<li>Location must be a string for <code class="docutils literal"><span class="pre">configs</span></code> and <code class="docutils literal"><span class="pre">params</span></code>, and an integer for
<code class="docutils literal"><span class="pre">args</span></code>. The set of <code class="docutils literal"><span class="pre">args</span></code> locations must be an unbroken series of
integers starting from 0.</li>
<li>Value Type must be one of <code class="docutils literal"><span class="pre">string</span></code>, <code class="docutils literal"><span class="pre">number</span></code>, or <code class="docutils literal"><span class="pre">data_source</span></code>. Data
sources may be passed as UUIDs or as valid paths (see above.) All values
should be sent as JSON strings. (Note that booleans and null values are
serialized differently in different languages. Please specify them as a
string representation of the appropriate constants for your data processing
engine.)</li>
<li><code class="docutils literal"><span class="pre">args</span></code> that are not required must be given a default value.</li>
</ul>
<p>The additional one-time complexity of specifying an interface on your template
allows a simpler repeated execution path, and also allows us to generate a
customized form for your job in the Horizon UI. This may be particularly
useful in cases in which an operator who is not a data processing job
developer will be running and administering the jobs.</p>
</div>
<div class="section" id="generation-of-swift-properties-for-data-sources">
<h3>Generation of Swift Properties for Data Sources<a class="headerlink" href="#generation-of-swift-properties-for-data-sources" title="Permalink to this headline">¶</a></h3>
<p>If swift proxy users are not configured (see
<a class="reference internal" href="advanced.configuration.guide.html"><em>Sahara Advanced Configuration Guide</em></a>) and a job is run with data
source objects containing swift paths, sahara will automatically generate
swift username and password configuration values based on the credentials
in the data sources. If the input and output data sources are both in swift,
it is expected that they specify the same credentials.</p>
<p>The swift credentials may be set explicitly with the following configuration
values:</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="100%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>fs.swift.service.sahara.username</td>
</tr>
<tr class="row-odd"><td>fs.swift.service.sahara.password</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>Setting the swift credentials explicitly is required when passing literal
swift paths as arguments instead of using data source references. When
possible, use data source references as described in
<a class="reference internal" href="#using-data-source-references-as-arguments">Using Data Source References as Arguments</a>.</p>
</div>
<div class="section" id="additional-details-for-hive-jobs">
<h3>Additional Details for Hive jobs<a class="headerlink" href="#additional-details-for-hive-jobs" title="Permalink to this headline">¶</a></h3>
<p>Sahara will automatically generate values for the <code class="docutils literal"><span class="pre">INPUT</span></code> and <code class="docutils literal"><span class="pre">OUTPUT</span></code>
parameters required by Hive based on the specified data sources.</p>
</div>
<div class="section" id="additional-details-for-pig-jobs">
<h3>Additional Details for Pig jobs<a class="headerlink" href="#additional-details-for-pig-jobs" title="Permalink to this headline">¶</a></h3>
<p>Sahara will automatically generate values for the <code class="docutils literal"><span class="pre">INPUT</span></code> and <code class="docutils literal"><span class="pre">OUTPUT</span></code>
parameters required by Pig based on the specified data sources.</p>
<p>For Pig jobs, <code class="docutils literal"><span class="pre">arguments</span></code> should be thought of as command line arguments
separated by spaces and passed to the <code class="docutils literal"><span class="pre">pig</span></code> shell.</p>
<p><code class="docutils literal"><span class="pre">Parameters</span></code> are a shorthand and are actually translated to the arguments
<code class="docutils literal"><span class="pre">-param</span> <span class="pre">name=value</span></code></p>
</div>
<div class="section" id="additional-details-for-mapreduce-jobs">
<h3>Additional Details for MapReduce jobs<a class="headerlink" href="#additional-details-for-mapreduce-jobs" title="Permalink to this headline">¶</a></h3>
<p><strong>Important!</strong></p>
<p>If the job type is MapReduce, the mapper and reducer classes <em>must</em> be
specified as configuration values.</p>
<p>Note that the UI will not prompt the user for these required values; they must
be added manually with the <code class="docutils literal"><span class="pre">Configure</span></code> tab.</p>
<p>Make sure to add these values with the correct names:</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="42%" />
<col width="58%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Example Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>mapred.mapper.new-api</td>
<td>true</td>
</tr>
<tr class="row-odd"><td>mapred.reducer.new-api</td>
<td>true</td>
</tr>
<tr class="row-even"><td>mapreduce.job.map.class</td>
<td>org.apache.oozie.example.SampleMapper</td>
</tr>
<tr class="row-odd"><td>mapreduce.job.reduce.class</td>
<td>org.apache.oozie.example.SampleReducer</td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="additional-details-for-mapreduce-streaming-jobs">
<h3>Additional Details for MapReduce.Streaming jobs<a class="headerlink" href="#additional-details-for-mapreduce-streaming-jobs" title="Permalink to this headline">¶</a></h3>
<p><strong>Important!</strong></p>
<p>If the job type is MapReduce.Streaming, the streaming mapper and reducer
classes <em>must</em> be specified.</p>
<p>In this case, the UI <em>will</em> prompt the user to enter mapper and reducer
values on the form and will take care of adding them to the job configuration
with the appropriate names. If using the python client, however, be certain to
add these values to the job configuration manually with the correct names:</p>
<blockquote>
<div><table border="1" class="docutils">
<colgroup>
<col width="63%" />
<col width="38%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Example Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>edp.streaming.mapper</td>
<td>/bin/cat</td>
</tr>
<tr class="row-odd"><td>edp.streaming.reducer</td>
<td>/usr/bin/wc</td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="additional-details-for-java-jobs">
<h3>Additional Details for Java jobs<a class="headerlink" href="#additional-details-for-java-jobs" title="Permalink to this headline">¶</a></h3>
<p>Data Source objects are not used directly with Java job types. Instead, any
input or output paths must be specified as arguments at job launch either
explicitly or by reference as described in
<a class="reference internal" href="#using-data-source-references-as-arguments">Using Data Source References as Arguments</a>. Using data source references is
the recommended way to pass paths to Java jobs.</p>
<p>If configuration values are specified, they must be added to the job&#8217;s
Hadoop configuration at runtime. There are two methods of doing this. The
simplest way is to use the <strong>edp.java.adapt_for_oozie</strong> option described
below. The other method is to use the code from
<a class="reference external" href="https://github.com/openstack/sahara/blob/master/etc/edp-examples/edp-java/README.rst">this example</a>
to explicitly load the values.</p>
<p>The following special configuration values are read by sahara and affect how
Java jobs are run:</p>
<ul>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.java.main_class</span></code> (required) Specifies the full name of the class
containing <code class="docutils literal"><span class="pre">main(String[]</span> <span class="pre">args)</span></code></p>
<p>A Java job will execute the <strong>main</strong> method of the specified main class. Any
arguments set during job launch will be passed to the program through the
<strong>args</strong> array.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">oozie.libpath</span></code> (optional) Specifies configuration values for the Oozie
share libs, these libs can be shared by different workflows</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.java.java_opts</span></code> (optional) Specifies configuration values for the JVM</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.java.adapt_for_oozie</span></code> (optional) Specifies that sahara should perform
special handling of configuration values and exit conditions. The default is
<strong>False</strong>.</p>
<p>If this configuration value is set to <strong>True</strong>, sahara will modify
the job&#8217;s Hadoop configuration before invoking the specified <strong>main</strong> method.
Any configuration values specified during job launch (excluding those
beginning with <strong>edp.</strong>) will be automatically set in the job&#8217;s Hadoop
configuration and will be available through standard methods.</p>
<p>Secondly, setting this option to <strong>True</strong> ensures that Oozie will handle
program exit conditions correctly.</p>
</li>
</ul>
<p>At this time, the following special configuration value only applies when
running jobs on a cluster generated by the Cloudera plugin with the
<strong>Enable Hbase Common Lib</strong> cluster config set to <strong>True</strong> (the default value):</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">edp.hbase_common_lib</span></code> (optional) Specifies that a common Hbase lib
generated by sahara in HDFS be added to the <strong>oozie.libpath</strong>. This for use
when an Hbase application is driven from a Java job. Default is <strong>False</strong>.</li>
</ul>
<p>The <strong>edp-wordcount</strong> example bundled with sahara shows how to use
configuration values, arguments, and swift data paths in a Java job type. Note
that the example does not use the <strong>edp.java.adapt_for_oozie</strong> option but
includes the code to load the configuration values explicitly.</p>
</div>
<div class="section" id="additional-details-for-shell-jobs">
<h3>Additional Details for Shell jobs<a class="headerlink" href="#additional-details-for-shell-jobs" title="Permalink to this headline">¶</a></h3>
<p>A shell job will execute the script specified as <code class="docutils literal"><span class="pre">main</span></code>, and will place any
files specified as <code class="docutils literal"><span class="pre">libs</span></code> in the same working directory (on both the
filesystem and in HDFS). Command line arguments may be passed to the script
through the <code class="docutils literal"><span class="pre">args</span></code> array, and any <code class="docutils literal"><span class="pre">params</span></code> values will be passed as
environment variables.</p>
<p>Data Source objects are not used directly with Shell job types but data source
references may be used as described in
<a class="reference internal" href="#using-data-source-references-as-arguments">Using Data Source References as Arguments</a>.</p>
<p>The <strong>edp-shell</strong> example bundled with sahara contains a script which will
output the executing user to a file specified by the first command line
argument.</p>
</div>
<div class="section" id="additional-details-for-spark-jobs">
<h3>Additional Details for Spark jobs<a class="headerlink" href="#additional-details-for-spark-jobs" title="Permalink to this headline">¶</a></h3>
<p>Data Source objects are not used directly with Spark job types. Instead, any
input or output paths must be specified as arguments at job launch either
explicitly or by reference as described in
<a class="reference internal" href="#using-data-source-references-as-arguments">Using Data Source References as Arguments</a>. Using data source references
is the recommended way to pass paths to Spark jobs.</p>
<p>Spark jobs use some special configuration values:</p>
<ul>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.java.main_class</span></code> (required) Specifies the full name of the class
containing the Java or Scala main method:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">main(String[]</span> <span class="pre">args)</span></code> for Java</li>
<li><code class="docutils literal"><span class="pre">main(args:</span> <span class="pre">Array[String]</span></code> for Scala</li>
</ul>
<p>A Spark job will execute the <strong>main</strong> method of the specified main class.
Any arguments set during job launch will be passed to the program through the
<strong>args</strong> array.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.spark.adapt_for_swift</span></code> (optional) If set to <strong>True</strong>, instructs
sahara to modify the job&#8217;s Hadoop configuration so that swift paths may be
accessed. Without this configuration value, swift paths will not be
accessible to Spark jobs. The default is <strong>False</strong>.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">edp.spark.driver.classpath</span></code> (optional) If set to empty string sahara
will use default classpath for the cluster during job execution.
Otherwise this will override default value for the cluster for particular
job execution.</p>
</li>
</ul>
<p>The <strong>edp-spark</strong> example bundled with sahara contains a Spark program for
estimating Pi.</p>
</div>
</div>
<div class="section" id="special-sahara-urls">
<h2>Special Sahara URLs<a class="headerlink" href="#special-sahara-urls" title="Permalink to this headline">¶</a></h2>
<p>Sahara uses custom URLs to refer to objects stored in swift, in manila, or in
the sahara internal database. These URLs are not meant to be used outside of
sahara.</p>
<p>Sahara swift URLs passed to running jobs as input or output sources include a
&#8221;.sahara&#8221; suffix on the container, for example:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">swift://container.sahara/object</span></code></div></blockquote>
<p>You may notice these swift URLs in job logs, however, you do not need to add
the suffix to the containers yourself. sahara will add the suffix if
necessary, so when using the UI or the python client you may write the above
URL simply as:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">swift://container/object</span></code></div></blockquote>
<p>Sahara internal database URLs have the form:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">internal-db://sahara-generated-uuid</span></code></div></blockquote>
<p>This indicates a file object in the sahara database which has the given uuid
as a key.</p>
<p>Manila NFS filesystem reference URLS take the form:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">manila://share-uuid/path</span></code></div></blockquote>
<p>This format should be used when referring to a job binary or a data source
stored in a manila NFS share.</p>
</div>
</div>
<div class="section" id="edp-requirements">
<h1>EDP Requirements<a class="headerlink" href="#edp-requirements" title="Permalink to this headline">¶</a></h1>
<p>The OpenStack installation and the cluster launched from sahara must meet the
following minimum requirements in order for EDP to function:</p>
<div class="section" id="openstack-services">
<h2>OpenStack Services<a class="headerlink" href="#openstack-services" title="Permalink to this headline">¶</a></h2>
<p>When a Hadoop job is executed, binaries are first uploaded to a cluster node
and then moved from the node local filesystem to HDFS. Therefore, there must
be an instance of HDFS available to the nodes in the sahara cluster.</p>
<p>If the swift service <em>is not</em> running in the OpenStack installation:</p>
<blockquote>
<div><ul class="simple">
<li>Job binaries may only be stored in the sahara internal database</li>
<li>Data sources require a long-running HDFS</li>
</ul>
</div></blockquote>
<p>If the swift service <em>is</em> running in the OpenStack installation:</p>
<blockquote>
<div><ul class="simple">
<li>Job binaries may be stored in swift or the sahara internal database</li>
<li>Data sources may be in swift or a long-running HDFS</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="cluster-processes">
<h2>Cluster Processes<a class="headerlink" href="#cluster-processes" title="Permalink to this headline">¶</a></h2>
<p>Requirements for EDP support depend on the EDP job type and plugin used for
the cluster. For example a Vanilla sahara cluster must run at least one
instance of these processes to support EDP:</p>
<ul class="simple">
<li>For Hadoop version 1:<ul>
<li>jobtracker</li>
<li>namenode</li>
<li>oozie</li>
<li>tasktracker</li>
<li>datanode</li>
</ul>
</li>
<li>For Hadoop version 2:<ul>
<li>namenode</li>
<li>datanode</li>
<li>resourcemanager</li>
<li>nodemanager</li>
<li>historyserver</li>
<li>oozie</li>
<li>spark history server</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="edp-technical-considerations">
<h1>EDP Technical Considerations<a class="headerlink" href="#edp-technical-considerations" title="Permalink to this headline">¶</a></h1>
<p>There are several things in EDP which require attention in order
to work properly. They are listed on this page.</p>
<div class="section" id="transient-clusters">
<h2>Transient Clusters<a class="headerlink" href="#transient-clusters" title="Permalink to this headline">¶</a></h2>
<p>EDP allows running jobs on transient clusters. In this case the cluster is
created specifically for the job and is shut down automatically once the job
is finished.</p>
<p>Two config parameters control the behaviour of periodic clusters:</p>
<blockquote>
<div><ul class="simple">
<li>periodic_enable - if set to &#8216;false&#8217;, sahara will do nothing to a transient
cluster once the job it was created for is completed. If it is set to
&#8216;true&#8217;, then the behaviour depends on the value of the next parameter.</li>
<li>use_identity_api_v3 - set it to &#8216;false&#8217; if your OpenStack installation
does not provide keystone API v3. In that case sahara will not terminate
unneeded clusters. Instead it will set their state to &#8216;AwaitingTermination&#8217;
meaning that they could be manually deleted by a user. If the parameter is
set to &#8216;true&#8217;, sahara will itself terminate the cluster. The limitation is
caused by lack of &#8216;trusts&#8217; feature in Keystone API older than v3.</li>
</ul>
</div></blockquote>
<p>If both parameters are set to &#8216;true&#8217;, sahara works with transient clusters in
the following manner:</p>
<blockquote>
<div><ol class="arabic simple">
<li>When a user requests for a job to be executed on a transient cluster,
sahara creates such a cluster.</li>
<li>Sahara drops the user&#8217;s credentials once the cluster is created but
prior to that it creates a trust allowing it to operate with the
cluster instances in the future without user credentials.</li>
<li>Once a cluster is not needed, sahara terminates its instances using the
stored trust. sahara drops the trust after that.</li>
</ol>
</div></blockquote>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="../index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Elastic Data Processing (EDP)</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#interfaces">Interfaces</a></li>
<li><a class="reference internal" href="#edp-concepts">EDP Concepts</a><ul>
<li><a class="reference internal" href="#job-binaries">Job Binaries</a></li>
<li><a class="reference internal" href="#jobs">Jobs</a></li>
<li><a class="reference internal" href="#data-sources">Data Sources</a></li>
<li><a class="reference internal" href="#job-execution">Job Execution</a></li>
</ul>
</li>
<li><a class="reference internal" href="#general-workflow">General Workflow</a><ul>
<li><a class="reference internal" href="#specifying-configuration-values-parameters-and-arguments">Specifying Configuration Values, Parameters, and Arguments</a></li>
<li><a class="reference internal" href="#using-data-source-references-as-arguments">Using Data Source References as Arguments</a></li>
<li><a class="reference internal" href="#creating-an-interface-for-your-job">Creating an Interface for Your Job</a></li>
<li><a class="reference internal" href="#generation-of-swift-properties-for-data-sources">Generation of Swift Properties for Data Sources</a></li>
<li><a class="reference internal" href="#additional-details-for-hive-jobs">Additional Details for Hive jobs</a></li>
<li><a class="reference internal" href="#additional-details-for-pig-jobs">Additional Details for Pig jobs</a></li>
<li><a class="reference internal" href="#additional-details-for-mapreduce-jobs">Additional Details for MapReduce jobs</a></li>
<li><a class="reference internal" href="#additional-details-for-mapreduce-streaming-jobs">Additional Details for MapReduce.Streaming jobs</a></li>
<li><a class="reference internal" href="#additional-details-for-java-jobs">Additional Details for Java jobs</a></li>
<li><a class="reference internal" href="#additional-details-for-shell-jobs">Additional Details for Shell jobs</a></li>
<li><a class="reference internal" href="#additional-details-for-spark-jobs">Additional Details for Spark jobs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#special-sahara-urls">Special Sahara URLs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#edp-requirements">EDP Requirements</a><ul>
<li><a class="reference internal" href="#openstack-services">OpenStack Services</a></li>
<li><a class="reference internal" href="#cluster-processes">Cluster Processes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#edp-technical-considerations">EDP Technical Considerations</a><ul>
<li><a class="reference internal" href="#transient-clusters">Transient Clusters</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="mapr_plugin.html"
                                  title="previous chapter">MapR Distribution Plugin</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="../restapi.html"
                                  title="next chapter">Sahara REST API v1.1</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/sahara
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../_sources/userdoc/edp.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../restapi.html" title="Sahara REST API v1.1"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="mapr_plugin.html" title="MapR Distribution Plugin"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Sahara</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2014, OpenStack Foundation.
      Last updated on &#39;Mon Feb 13 17:36:11 2017, commit 23ee18f&#39;.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/sahara");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>