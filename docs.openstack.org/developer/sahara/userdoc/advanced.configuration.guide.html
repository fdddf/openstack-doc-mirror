<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Sahara Advanced Configuration Guide &mdash; Sahara</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '6.0.0.0rc2.dev34',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Sahara" href="../index.html" />
    <link rel="next" title="Sahara Upgrade Guide" href="upgrade.guide.html" />
    <link rel="prev" title="OpenStack Dashboard Configuration Guide" href="dashboard.guide.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="sahara-advanced-configuration-guide">
<h1>Sahara Advanced Configuration Guide<a class="headerlink" href="#sahara-advanced-configuration-guide" title="Permalink to this headline">¶</a></h1>
<p>This guide addresses specific aspects of Sahara configuration that pertain to
advanced usage. It is divided into sections about various features that can be
utilized, and their related configurations.</p>
<div class="section" id="custom-network-topologies">
<span id="id1"></span><h2>Custom network topologies<a class="headerlink" href="#custom-network-topologies" title="Permalink to this headline">¶</a></h2>
<p>Sahara accesses instances at several stages of cluster spawning through
SSH and HTTP. Floating IPs and network namespaces
(see <a class="reference internal" href="configuration.guide.html#neutron-nova-network"><span>Networking configuration</span></a>) will be automatically used for
access when present. When floating IPs are not assigned to instances and
namespaces are not being used, sahara will need an alternative method to
reach them.</p>
<p>The <code class="docutils literal"><span class="pre">proxy_command</span></code> parameter of the configuration file can be used to
give sahara a command to access instances. This command is run on the
sahara host and must open a netcat socket to the instance destination
port. The <code class="docutils literal"><span class="pre">{host}</span></code> and <code class="docutils literal"><span class="pre">{port}</span></code> keywords should be used to describe the
destination, they will be substituted at runtime.  Other keywords that
can be used are: <code class="docutils literal"><span class="pre">{tenant_id}</span></code>, <code class="docutils literal"><span class="pre">{network_id}</span></code> and <code class="docutils literal"><span class="pre">{router_id}</span></code>.</p>
<p>For example, the following parameter in the sahara configuration file
would be used if instances are accessed through a relay machine:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">proxy_command</span><span class="o">=</span><span class="s">&#39;ssh relay-machine-{tenant_id} nc {host} {port}&#39;</span>
</pre></div>
</div>
<p>Whereas the following shows an example of accessing instances though
a custom network namespace:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">proxy_command</span><span class="o">=</span><span class="s">&#39;ip netns exec ns_for_{network_id} nc {host} {port}&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="dns-hostname-resolution">
<span id="id2"></span><h2>DNS Hostname Resolution<a class="headerlink" href="#dns-hostname-resolution" title="Permalink to this headline">¶</a></h2>
<p>Sahara can resolve hostnames of cluster instances by using DNS. For this Sahara
uses Designate. With this feature, for each instance of the cluster Sahara will
create two <code class="docutils literal"><span class="pre">A</span></code> records (for internal and external ips) under one hostname
and one <code class="docutils literal"><span class="pre">PTR</span></code> record. Also all links in the Sahara dashboard will be
displayed as hostnames instead of just ip addresses.</p>
<p>You should configure DNS server with Designate. Designate service should be
properly installed and registered in Keystone catalog. The detailed
instructions about Designate configuration can be found here: <a class="reference external" href="http://docs.openstack.org/developer/designate/install/ubuntu-liberty.html">Designate manual
installation</a> and here: <a class="reference external" href="http://docs.openstack.org/mitaka/networking-guide/adv-config-dns.html#configuring-openstack-networking-for-integration-with-an-external-dns-service">Configuring OpenStack Networking with Designate</a>.
Also if you use devstack you can just enable Designate plugin:
<a class="reference external" href="http://docs.openstack.org/developer/designate/devstack.html">Designate devstack</a>.</p>
<p>When Designate is configured you should create domain(s) for hostname
resolution. This can be done by using the Designate dashboard or by CLI. Also
you have to create <code class="docutils literal"><span class="pre">in-addr.arpa.</span></code> domain for reverse hostname resolution
because some plugins (e.g. <code class="docutils literal"><span class="pre">HDP</span></code>) determine hostname by ip.</p>
<p>Sahara also should be properly configured. In <code class="docutils literal"><span class="pre">sahara.conf</span></code> you must specify
two config properties:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="c1"># Use Designate for internal and external hostnames resolution:</span>
<span class="na">use_designate</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># IP addresses of Designate nameservers:</span>
<span class="na">nameservers</span><span class="o">=</span><span class="s">1.1.1.1,2.2.2.2</span>
</pre></div>
</div>
<p>An OpenStack operator should properly configure the network. It must enable
DHCP and specify DNS server ip addresses (e.g. 1.1.1.1 and 2.2.2.2) in
<code class="docutils literal"><span class="pre">DNS</span> <span class="pre">Name</span> <span class="pre">Servers</span></code> field in the <code class="docutils literal"><span class="pre">Subnet</span> <span class="pre">Details</span></code>. If the subnet already
exists and changing it or creating new one is impossible then Sahara will
manually change <code class="docutils literal"><span class="pre">/etc/resolv.conf</span></code> file on every instance of the cluster (if
<code class="docutils literal"><span class="pre">nameservers</span></code> list have been specified in <code class="docutils literal"><span class="pre">sahara.conf</span></code>). In this case,
though, Sahara cannot guarantee that these changes will not be overwritten by
DHCP or other services of the existing network. Sahara has a health check for
track this situation (and if it occurs the health status will be red).</p>
<p>In order to resolve hostnames from your local machine you should properly
change your <code class="docutils literal"><span class="pre">/etc/resolv.conf</span></code> file by adding appropriate ip addresses of
DNS servers (e.g. 1.1.1.1 and 2.2.2.2). Also the VMs with DNS servers should
be available from your local machine.</p>
</div>
<div class="section" id="data-locality-configuration">
<span id="id3"></span><h2>Data-locality configuration<a class="headerlink" href="#data-locality-configuration" title="Permalink to this headline">¶</a></h2>
<p>Hadoop provides the data-locality feature to enable task tracker and
data nodes the capability of spawning on the same rack, Compute node,
or virtual machine. Sahara exposes this functionality to the user
through a few configuration parameters and user defined topology files.</p>
<p>To enable data-locality, set the <code class="docutils literal"><span class="pre">enable_data_locality</span></code> parameter to
<code class="docutils literal"><span class="pre">true</span></code> in the sahara configuration file</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enable_data_locality</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<p>With data locality enabled, you must now specify the topology files
for the Compute and Object Storage services. These files are
specified in the sahara configuration file as follows:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">compute_topology_file</span><span class="o">=</span><span class="s">/etc/sahara/compute.topology</span>
<span class="na">swift_topology_file</span><span class="o">=</span><span class="s">/etc/sahara/swift.topology</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">compute_topology_file</span></code> should contain mappings between Compute
nodes and racks in the following format:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="na">compute1 /rack1</span>
<span class="na">compute2 /rack2</span>
<span class="na">compute3 /rack2</span>
</pre></div>
</div>
<p>Note that the Compute node names must be exactly the same as configured in
OpenStack (<code class="docutils literal"><span class="pre">host</span></code> column in admin list for instances).</p>
<p>The <code class="docutils literal"><span class="pre">swift_topology_file</span></code> should contain mappings between Object Storage
nodes and racks in the following format:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="na">node1 /rack1</span>
<span class="na">node2 /rack2</span>
<span class="na">node3 /rack2</span>
</pre></div>
</div>
<p>Note that the Object Storage node names must be exactly the same as
configured in the object ring. Also, you should ensure that instances
with the task tracker process have direct access to the Object Storage
nodes.</p>
<p>Hadoop versions after 1.2.0 support four-layer topology (for more detail
please see <a class="reference external" href="https://issues.apache.org/jira/browse/HADOOP-8468">HADOOP-8468 JIRA issue</a>). To enable this feature set the
<code class="docutils literal"><span class="pre">enable_hypervisor_awareness</span></code> parameter to <code class="docutils literal"><span class="pre">true</span></code> in the configuration
file. In this case sahara will add the Compute node ID as a second level of
topology for virtual machines.</p>
</div>
<div class="section" id="distributed-mode-configuration">
<span id="id4"></span><h2>Distributed mode configuration<a class="headerlink" href="#distributed-mode-configuration" title="Permalink to this headline">¶</a></h2>
<p>Sahara can be configured to run in a distributed mode that creates a
separation between the API and engine processes. This allows the API
process to remain relatively free to handle requests while offloading
intensive tasks to the engine processes.</p>
<p>The <code class="docutils literal"><span class="pre">sahara-api</span></code> application works as a front-end and serves user
requests. It offloads &#8216;heavy&#8217; tasks to the <code class="docutils literal"><span class="pre">sahara-engine</span></code> process
via RPC mechanisms. While the <code class="docutils literal"><span class="pre">sahara-engine</span></code> process could be loaded
with tasks, <code class="docutils literal"><span class="pre">sahara-api</span></code> stays free and hence may quickly respond to
user queries.</p>
<p>If sahara runs on several hosts, the API requests could be
balanced between several <code class="docutils literal"><span class="pre">sahara-api</span></code> hosts using a load balancer.
It is not required to balance load between different <code class="docutils literal"><span class="pre">sahara-engine</span></code>
hosts as this will be automatically done via the message broker.</p>
<p>If a single host becomes unavailable, other hosts will continue
serving user requests. Hence, a better scalability is achieved and some
fault tolerance as well. Note that distributed mode is not a true
high availability. While the failure of a single host does not
affect the work of the others, all of the operations running on
the failed host will stop. For example, if a cluster scaling is
interrupted, the cluster will be stuck in a half-scaled state. The
cluster might continue working, but it will be impossible to scale it
further or run jobs on it via EDP.</p>
<p>To run sahara in distributed mode pick several hosts on which
you want to run sahara services and follow these steps:</p>
<blockquote>
<div><ul>
<li><p class="first">On each host install and configure sahara using the
<a class="reference external" href="../installation.guide.html">installation guide</a>
except:</p>
<blockquote>
<div><ul class="simple">
<li>Do not run <code class="docutils literal"><span class="pre">sahara-db-manage</span></code> or launch sahara with <code class="docutils literal"><span class="pre">sahara-all</span></code></li>
<li>Ensure that each configuration file provides a database connection
string to a single database for all hosts.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Run <code class="docutils literal"><span class="pre">sahara-db-manage</span></code> as described in the installation guide,
but only on a single (arbitrarily picked) host.</p>
</li>
<li><p class="first">The <code class="docutils literal"><span class="pre">sahara-api</span></code> and <code class="docutils literal"><span class="pre">sahara-engine</span></code> processes use oslo.messaging to
communicate with each other. You will need to configure it properly on
each host (see below).</p>
</li>
<li><p class="first">Run <code class="docutils literal"><span class="pre">sahara-api</span></code> and <code class="docutils literal"><span class="pre">sahara-engine</span></code> on the desired hosts. You may
run both processes on the same or separate hosts as long as they are
configured to use the same message broker and database.</p>
</li>
</ul>
</div></blockquote>
<p>To configure oslo.messaging, first you will need to choose a message
broker driver. Currently there are two drivers provided: RabbitMQ
or ZeroMQ. For the RabbitMQ drivers please see the
<a class="reference internal" href="configuration.guide.html#notification-configuration"><span>Notifications configuration</span></a> documentation for an explanation of
common configuration options.</p>
<p>For an expanded view of all the options provided by each message broker
driver in oslo.messaging please refer to the options available in the
respective source trees:</p>
<blockquote>
<div><ul class="simple">
<li>For Rabbit MQ see<ul>
<li>rabbit_opts variable in <a class="reference external" href="https://git.openstack.org/cgit/openstack/oslo.messaging/tree/oslo/messaging/_drivers/impl_rabbit.py?id=1.4.0#n38">impl_rabbit.py</a></li>
<li>amqp_opts variable in <a class="reference external" href="https://git.openstack.org/cgit/openstack/oslo.messaging/tree/oslo/messaging/_drivers/amqp.py?id=1.4.0#n37">amqp.py</a></li>
</ul>
</li>
<li>For Zmq see<ul>
<li>zmq_opts variable in <a class="reference external" href="https://git.openstack.org/cgit/openstack/oslo.messaging/tree/oslo/messaging/_drivers/impl_zmq.py?id=1.4.0#n49">impl_zmq.py</a></li>
<li>matchmaker_opts variable in <a class="reference external" href="https://git.openstack.org/cgit/openstack/oslo.messaging/tree/oslo/messaging/_drivers/matchmaker.py?id=1.4.0#n27">matchmaker.py</a></li>
<li>matchmaker_redis_opts variable in <a class="reference external" href="https://git.openstack.org/cgit/openstack/oslo.messaging/tree/oslo/messaging/_drivers/matchmaker_redis.py?id=1.4.0#n26">matchmaker_redis.py</a></li>
<li>matchmaker_opts variable in <a class="reference external" href="https://git.openstack.org/cgit/openstack/oslo.messaging/tree/oslo/messaging/_drivers/matchmaker_ring.py?id=1.4.0#n27">matchmaker_ring.py</a></li>
</ul>
</li>
</ul>
</div></blockquote>
<p>These options will also be present in the generated sample configuration
file. For instructions on creating the configuration file please see the
<a class="reference internal" href="configuration.guide.html"><em>Sahara Configuration Guide</em></a>.</p>
</div>
<div class="section" id="distributed-periodic-tasks-configuration">
<span id="distributed-periodic-tasks"></span><h2>Distributed periodic tasks configuration<a class="headerlink" href="#distributed-periodic-tasks-configuration" title="Permalink to this headline">¶</a></h2>
<p>If sahara is configured to run in distributed mode (see
<a class="reference internal" href="#distributed-mode-configuration"><span>Distributed mode configuration</span></a>), periodic tasks can also be launched in
distributed mode. In this case tasks will be split across all <code class="docutils literal"><span class="pre">sahara-engine</span></code>
processes. This will reduce overall load.</p>
<p>Distributed periodic tasks are based on Hash Ring implementation and the Tooz
library that provides group membership support for a set of backends. In order
to use periodic tasks distribution, the following steps are required:</p>
<blockquote>
<div><ul>
<li><p class="first">One of the <a class="reference external" href="http://docs.openstack.org/developer/tooz/compatibility.html#driver-support">supported backends</a> should be configured and started.</p>
</li>
<li><p class="first">Backend URL should be set in the sahara configuration file with the
<code class="docutils literal"><span class="pre">periodic_coordinator_backend_url</span></code> parameter. For example, if the
ZooKeeper backend is being used:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">periodic_coordinator_backend_url</span><span class="o">=</span><span class="s">kazoo://IP:PORT</span>
</pre></div>
</div>
</li>
<li><p class="first">Tooz extras should be installed. When using Zookeeper as coordination
backend, <code class="docutils literal"><span class="pre">kazoo</span></code> library should be installed. It can be done with pip:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install tooz[zookeeper]</span>
</pre></div>
</div>
</li>
<li><p class="first">Periodic tasks can be performed in parallel. Number of threads to run
periodic tasks on a single engine can be set with
<code class="docutils literal"><span class="pre">periodic_workers_number</span></code> parameter (only 1 thread will be launched by
default). Example:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">periodic_workers_number</span><span class="o">=</span><span class="s">2</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">coordinator_heartbeat_interval</span></code> can be set to change the interval between
heartbeat execution (1 second by default). Heartbeats are needed to make
sure that connection to the coordination backend is active. Example:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">coordinator_heartbeat_interval</span><span class="o">=</span><span class="s">2</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">hash_ring_replicas_count</span></code> can be set to change the number of replicas for
each engine on a Hash Ring. Each replica is a point on a Hash Ring that
belongs to a particular engine. A larger number of replicas leads to better
task distribution across the set of engines. (40 by default). Example:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">hash_ring_replicas_count</span><span class="o">=</span><span class="s">100</span>
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="external-key-manager-usage">
<span id="id5"></span><h2>External key manager usage<a class="headerlink" href="#external-key-manager-usage" title="Permalink to this headline">¶</a></h2>
<p>Sahara generates and stores several passwords during the course of operation.
To harden sahara&#8217;s usage of passwords it can be instructed to use an
external key manager for storage and retrieval of these secrets. To enable
this feature there must first be an OpenStack Key Manager service deployed
within the stack.</p>
<p>With a Key Manager service deployed on the stack, sahara must be configured
to enable the external storage of secrets. Sahara uses the
<a class="reference external" href="http://docs.openstack.org/developer/castellan/">castellan</a> library
to interface with the OpenStack Key Manager service. This library provides
configurable access to a key manager. To configure sahara to use barbican as
the key manager, edit the sahara configuration file as follows:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">use_barbican_key_manager</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<p>Enabling the <code class="docutils literal"><span class="pre">use_barbican_key_manager</span></code> option will configure castellan
to use barbican as its key management implementation. By default it will
attempt to find barbican in the Identity service&#8217;s service catalog.</p>
<p>For added control of the barbican server location, optional configuration
values may be added to specify the URL for the barbican API server.</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[castellan]</span>
<span class="na">barbican_api_endpoint</span><span class="o">=</span><span class="s">http://{barbican controller IP:PORT}/</span>
<span class="na">barbican_api_version</span><span class="o">=</span><span class="s">v1</span>
</pre></div>
</div>
<p>The specific values for the barbican endpoint will be dictated by the
IP address of the controller for your installation.</p>
<p>With all of these values configured and the Key Manager service deployed,
sahara will begin storing its secrets in the external manager.</p>
</div>
<div class="section" id="indirect-instance-access-through-proxy-nodes">
<h2>Indirect instance access through proxy nodes<a class="headerlink" href="#indirect-instance-access-through-proxy-nodes" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The indirect VMs access feature is in alpha state. We do not
recommend using it in a production environment.</p>
</div>
<p>Sahara needs to access instances through SSH during cluster setup. This
access can be obtained a number of different ways (see
<a class="reference internal" href="configuration.guide.html#neutron-nova-network"><span>Networking configuration</span></a>, <a class="reference internal" href="configuration.guide.html#floating-ip-management"><span>Floating IP management</span></a>,
<a class="reference internal" href="#custom-network-topologies"><span>Custom network topologies</span></a>). Sometimes it is impossible to provide
access to all nodes (because of limited numbers of floating IPs or security
policies). In these cases access can be gained using other nodes of the
cluster as proxy gateways. To enable this set <code class="docutils literal"><span class="pre">is_proxy_gateway=true</span></code>
for the node group you want to use as proxy. Sahara will communicate with
all other cluster instances through the instances of this node group.</p>
<p>Note, if <code class="docutils literal"><span class="pre">use_floating_ips=true</span></code> and the cluster contains a node group with
<code class="docutils literal"><span class="pre">is_proxy_gateway=true</span></code>, the requirement to have <code class="docutils literal"><span class="pre">floating_ip_pool</span></code>
specified is applied only to the proxy node group. Other instances will be
accessed through proxy instances using the standard private network.</p>
<p>Note, the Cloudera Hadoop plugin doesn&#8217;t support access to Cloudera manager
through a proxy node. This means that for CDH clusters only nodes with
the Cloudera manager can be designated as proxy gateway nodes.</p>
</div>
<div class="section" id="multi-region-deployment">
<h2>Multi region deployment<a class="headerlink" href="#multi-region-deployment" title="Permalink to this headline">¶</a></h2>
<p>Sahara supports multi region deployment. To enable this option each
instance of sahara should have the <code class="docutils literal"><span class="pre">os_region_name=&lt;region&gt;</span></code>
parameter set in the configuration file. The following example demonstrates
configuring sahara to use the <code class="docutils literal"><span class="pre">RegionOne</span></code> region:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">os_region_name</span><span class="o">=</span><span class="s">RegionOne</span>
</pre></div>
</div>
</div>
<div class="section" id="non-root-users">
<span id="id6"></span><h2>Non-root users<a class="headerlink" href="#non-root-users" title="Permalink to this headline">¶</a></h2>
<p>In cases where a proxy command is being used to access cluster instances
(for example, when using namespaces or when specifying a custom proxy
command), rootwrap functionality is provided to allow users other than
<code class="docutils literal"><span class="pre">root</span></code> access to the needed operating system facilities. To use rootwrap
the following configuration parameter is required to be set:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">use_rootwrap</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<p>Assuming you elect to leverage the default rootwrap command
(<code class="docutils literal"><span class="pre">sahara-rootwrap</span></code>), you will need to perform the following additional setup
steps:</p>
<ul class="simple">
<li>Copy the provided sudoers configuration file from the local project file
<code class="docutils literal"><span class="pre">etc/sudoers.d/sahara-rootwrap</span></code> to the system specific location, usually
<code class="docutils literal"><span class="pre">/etc/sudoers.d</span></code>. This file is setup to allow a user named <code class="docutils literal"><span class="pre">sahara</span></code>
access to the rootwrap script. It contains the following:</li>
</ul>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="na">sahara ALL</span> <span class="o">=</span> <span class="s">(root) NOPASSWD: /usr/bin/sahara-rootwrap /etc/sahara/rootwrap.conf *</span>
</pre></div>
</div>
<p>When using devstack to deploy sahara, please pay attention that you need to
change user in script from <code class="docutils literal"><span class="pre">sahara</span></code> to <code class="docutils literal"><span class="pre">stack</span></code>.</p>
<ul class="simple">
<li>Copy the provided rootwrap configuration file from the local project file
<code class="docutils literal"><span class="pre">etc/sahara/rootwrap.conf</span></code> to the system specific location, usually
<code class="docutils literal"><span class="pre">/etc/sahara</span></code>. This file contains the default configuration for rootwrap.</li>
<li>Copy the provided rootwrap filters file from the local project file
<code class="docutils literal"><span class="pre">etc/sahara/rootwrap.d/sahara.filters</span></code> to the location specified in the
rootwrap configuration file, usually <code class="docutils literal"><span class="pre">/etc/sahara/rootwrap.d</span></code>. This file
contains the filters that will allow the <code class="docutils literal"><span class="pre">sahara</span></code> user to access the
<code class="docutils literal"><span class="pre">ip</span> <span class="pre">netns</span> <span class="pre">exec</span></code>, <code class="docutils literal"><span class="pre">nc</span></code>, and <code class="docutils literal"><span class="pre">kill</span></code> commands through the rootwrap
(depending on <code class="docutils literal"><span class="pre">proxy_command</span></code> you may need to set additional filters).
It should look similar to the followings:</li>
</ul>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[Filters]</span>
<span class="na">ip: IpNetnsExecFilter, ip, root</span>
<span class="na">nc: CommandFilter, nc, root</span>
<span class="na">kill: CommandFilter, kill, root</span>
</pre></div>
</div>
<p>If you wish to use a rootwrap command other than <code class="docutils literal"><span class="pre">sahara-rootwrap</span></code> you can
set the following parameter in your sahara configuration file:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">rootwrap_command</span><span class="o">=</span><span class="s">&#39;sudo sahara-rootwrap /etc/sahara/rootwrap.conf&#39;</span>
</pre></div>
</div>
<p>For more information on rootwrap please refer to the
<a class="reference external" href="https://wiki.openstack.org/wiki/Rootwrap">official Rootwrap documentation</a></p>
</div>
<div class="section" id="object-storage-access-using-proxy-users">
<h2>Object Storage access using proxy users<a class="headerlink" href="#object-storage-access-using-proxy-users" title="Permalink to this headline">¶</a></h2>
<p>To improve security for clusters accessing files in Object Storage,
sahara can be configured to use proxy users and delegated trusts for
access. This behavior has been implemented to reduce the need for
storing and distributing user credentials.</p>
<p>The use of proxy users involves creating an Identity domain that will be
designated as the home for these users. Proxy users will be
created on demand by sahara and will only exist during a job execution
which requires Object Storage access. The domain created for the
proxy users must be backed by a driver that allows sahara&#8217;s admin user to
create new user accounts. This new domain should contain no roles, to limit
the potential access of a proxy user.</p>
<p>Once the domain has been created, sahara must be configured to use it by
adding the domain name and any potential delegated roles that must be used
for Object Storage access to the sahara configuration file. With the
domain enabled in sahara, users will no longer be required to enter
credentials for their data sources and job binaries referenced in
Object Storage.</p>
<div class="section" id="detailed-instructions">
<h3>Detailed instructions<a class="headerlink" href="#detailed-instructions" title="Permalink to this headline">¶</a></h3>
<p>First a domain must be created in the Identity service to hold proxy
users created by sahara. This domain must have an identity backend driver
that allows for sahara to create new users. The default SQL engine is
sufficient but if your keystone identity is backed by LDAP or similar
then domain specific configurations should be used to ensure sahara&#8217;s
access. Please see the <a class="reference external" href="http://docs.openstack.org/developer/keystone/configuration.html#domain-specific-drivers">Keystone documentation</a> for more information.</p>
<p>With the domain created, sahara&#8217;s configuration file should be updated to
include the new domain name and any potential roles that will be needed. For
this example let&#8217;s assume that the name of the proxy domain is
<code class="docutils literal"><span class="pre">sahara_proxy</span></code> and the roles needed by proxy users will be <code class="docutils literal"><span class="pre">Member</span></code> and
<code class="docutils literal"><span class="pre">SwiftUser</span></code>.</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">use_domain_for_proxy_users</span><span class="o">=</span><span class="s">true</span>
<span class="na">proxy_user_domain_name</span><span class="o">=</span><span class="s">sahara_proxy</span>
<span class="na">proxy_user_role_names</span><span class="o">=</span><span class="s">Member,SwiftUser</span>
</pre></div>
</div>
<p>A note on the use of roles. In the context of the proxy user, any roles
specified here are roles intended to be delegated to the proxy user from the
user with access to Object Storage. More specifically, any roles that
are required for Object Storage access by the project owning the object
store must be delegated to the proxy user for authentication to be
successful.</p>
<p>Finally, the stack administrator must ensure that images registered with
sahara have the latest version of the Hadoop swift filesystem plugin
installed. The sources for this plugin can be found in the
<a class="reference external" href="http://github.com/openstack/sahara-extra">sahara extra repository</a>. For more information on images or swift
integration see the sahara documentation sections
<a class="reference internal" href="vanilla_imagebuilder.html#diskimage-builder-label"><span>Building Images for Vanilla Plugin</span></a> and <a class="reference internal" href="hadoop-swift.html#swift-integration-label"><span>Swift Integration</span></a>.</p>
</div>
</div>
<div class="section" id="volume-instance-locality-configuration">
<span id="id7"></span><h2>Volume instance locality configuration<a class="headerlink" href="#volume-instance-locality-configuration" title="Permalink to this headline">¶</a></h2>
<p>The Block Storage service provides the ability to define volume instance
locality to ensure that instance volumes are created on the same host
as the hypervisor. The <code class="docutils literal"><span class="pre">InstanceLocalityFilter</span></code> provides the mechanism
for the selection of a storage provider located on the same physical
host as an instance.</p>
<p>To enable this functionality for instances of a specific node group, the
<code class="docutils literal"><span class="pre">volume_local_to_instance</span></code> field in the node group template should be
set to <code class="docutils literal"><span class="pre">true</span></code> and some extra configurations are needed:</p>
<ul>
<li><p class="first">The cinder-volume service should be launched on every physical host and at
least one physical host should run both cinder-scheduler and
cinder-volume services.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">InstanceLocalityFilter</span></code> should be added to the list of default filters
(<code class="docutils literal"><span class="pre">scheduler_default_filters</span></code> in cinder) for the Block Storage
configuration.</p>
</li>
<li><p class="first">The Extended Server Attributes extension needs to be active in the Compute
service (this is true by default in nova), so that the
<code class="docutils literal"><span class="pre">OS-EXT-SRV-ATTR:host</span></code> property is returned when requesting instance
info.</p>
</li>
<li><p class="first">The user making the call needs to have sufficient rights for the property to
be returned by the Compute service.
This can be done by:</p>
<ul>
<li><p class="first">by changing nova&#8217;s <code class="docutils literal"><span class="pre">policy.json</span></code> to allow the user access to the
<code class="docutils literal"><span class="pre">extended_server_attributes</span></code> option.</p>
</li>
<li><p class="first">by designating an account with privileged rights in the cinder
configuration:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="na">os_privileged_user_name</span> <span class="o">=</span>
<span class="na">os_privileged_user_password</span> <span class="o">=</span>
<span class="na">os_privileged_user_tenant</span> <span class="o">=</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
<p>It should be noted that in a situation when the host has no space for volume
creation, the created volume will have an <code class="docutils literal"><span class="pre">Error</span></code> state and can not be used.</p>
</div>
<div class="section" id="autoconfiguration-for-templates">
<h2>Autoconfiguration for templates<a class="headerlink" href="#autoconfiguration-for-templates" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="configs_recommendations.html"><em>Autoconfiguring templates</em></a></p>
</div>
<div class="section" id="ntp-service-configuration">
<h2>NTP service configuration<a class="headerlink" href="#ntp-service-configuration" title="Permalink to this headline">¶</a></h2>
<p>By default sahara will enable the NTP service on all cluster instances if the
NTP package is included in the image (the sahara disk image builder will
include NTP in all images it generates). The default NTP server will be
<code class="docutils literal"><span class="pre">pool.ntp.org</span></code>; this can be overridden using the <code class="docutils literal"><span class="pre">default_ntp_server</span></code>
setting in the <code class="docutils literal"><span class="pre">DEFAULT</span></code> section of the sahara configuration file.</p>
<p>If you are creating cluster templates using the sahara UI and would like to
specify a different NTP server for a particular cluster template, use the <code class="docutils literal"><span class="pre">URL</span>
<span class="pre">of</span> <span class="pre">NTP</span> <span class="pre">server</span></code> setting in the <code class="docutils literal"><span class="pre">General</span> <span class="pre">Parameters</span></code> section when you create
the template. If you would like to disable NTP for a particular cluster
template, deselect the <code class="docutils literal"><span class="pre">Enable</span> <span class="pre">NTP</span> <span class="pre">service</span></code> checkbox in the <code class="docutils literal"><span class="pre">General</span>
<span class="pre">Parameters</span></code> section when you create the template.</p>
<p>If you are creating clusters using the sahara CLI, you can specify another NTP
server or disable NTP service using the examples below.</p>
<p>If you want to enable configuring the NTP service, you should specify the
following configs for the cluster:</p>
<div class="highlight-json"><div class="highlight"><pre><span></span>cluster_configs: {
    &quot;general&quot;: {
        &quot;URL of NTP server&quot;: &quot;your_server.net&quot;,
    }
}
</pre></div>
</div>
<p>If you want to disable configuring NTP service, you should specify following
configs for the cluster:</p>
<div class="highlight-json"><div class="highlight"><pre><span></span>&quot;cluster_configs&quot;: {
    &quot;general&quot;: {
        &quot;Enable NTP service&quot;: false,
    }
}
</pre></div>
</div>
</div>
<div class="section" id="cors-cross-origin-resource-sharing-configuration">
<h2>CORS (Cross Origin Resource Sharing) Configuration<a class="headerlink" href="#cors-cross-origin-resource-sharing-configuration" title="Permalink to this headline">¶</a></h2>
<p>Sahara provides direct API access to user-agents (browsers) via the HTTP
CORS protocol. Detailed documentation, as well as troubleshooting examples,
may be found in the OpenStack <a class="reference external" href="http://docs.openstack.org/admin-guide/cross_project_cors.html">Administrator Guide</a>.</p>
<p>To get started quickly, use the example configuration block below, replacing
the <code class="code docutils literal"><span class="pre">allowed</span> <span class="pre">origin</span></code> field with the host(s) from which your API expects
access.</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[cors]</span>
<span class="na">allowed_origin</span><span class="o">=</span><span class="s">https://we.example.com:443</span>
<span class="na">max_age</span><span class="o">=</span><span class="s">3600</span>
<span class="na">allow_credentials</span><span class="o">=</span><span class="s">true</span>

<span class="k">[cors.additional_domain_1]</span>
<span class="na">allowed_origin</span><span class="o">=</span><span class="s">https://additional_domain_1.example.com:443</span>

<span class="k">[cors.additional_domain_2]</span>
<span class="na">allowed_origin</span><span class="o">=</span><span class="s">https://additional_domain_2.example.com:443</span>
</pre></div>
</div>
<p>For more information on Cross Origin Resource Sharing, please review the <a class="reference external" href="http://www.w3.org/TR/cors/">W3C
CORS specification</a>.</p>
</div>
<div class="section" id="cleanup-time-for-incomplete-clusters">
<h2>Cleanup time for incomplete clusters<a class="headerlink" href="#cleanup-time-for-incomplete-clusters" title="Permalink to this headline">¶</a></h2>
<p>Sahara provides maximal time (in hours) for clusters allowed to be in states
other than &#8220;Active&#8221;, &#8220;Deleting&#8221; or &#8220;Error&#8221;. If a cluster is not in &#8220;Active&#8221;,
&#8220;Deleting&#8221; or &#8220;Error&#8221; state and last update of it was longer than
<code class="docutils literal"><span class="pre">cleanup_time_for_incomplete_clusters</span></code> hours ago then it will be deleted
automatically. You can enable this feature by adding appropriate config
property in the <code class="docutils literal"><span class="pre">DEFAULT</span></code> section (by default it set up to <code class="docutils literal"><span class="pre">0</span></code> value which
means that automatic clean up is disabled). For example, if you want cluster to
be deleted after 3 hours if it didn&#8217;t leave &#8220;Starting&#8221; state then you should
specify:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">cleanup_time_for_incomplete_clusters</span> <span class="o">=</span> <span class="s">3</span>
</pre></div>
</div>
</div>
<div class="section" id="security-group-rules-configuration">
<h2>Security Group Rules Configuration<a class="headerlink" href="#security-group-rules-configuration" title="Permalink to this headline">¶</a></h2>
<p>When auto_security_group is used, the amount of created security group rules
may be bigger than the default values configured in <code class="docutils literal"><span class="pre">neutron.conf</span></code>. Then the
default limit should be raised up to some bigger value which is proportional to
the number of cluster node groups. You can change it in <code class="docutils literal"><span class="pre">neutron.conf</span></code> file:</p>
<div class="highlight-cfg"><div class="highlight"><pre><span></span><span class="k">[quotas]</span>
<span class="na">quota_security_group</span> <span class="o">=</span> <span class="s">1000</span>
<span class="na">quota_security_group_rule</span> <span class="o">=</span> <span class="s">10000</span>
</pre></div>
</div>
<p>Or you can execute openstack CLI command:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">openstack quota set --secgroups 1000 --secgroup-rules 10000 $PROJECT_ID</span>
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="../index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Sahara Advanced Configuration Guide</a><ul>
<li><a class="reference internal" href="#custom-network-topologies">Custom network topologies</a></li>
<li><a class="reference internal" href="#dns-hostname-resolution">DNS Hostname Resolution</a></li>
<li><a class="reference internal" href="#data-locality-configuration">Data-locality configuration</a></li>
<li><a class="reference internal" href="#distributed-mode-configuration">Distributed mode configuration</a></li>
<li><a class="reference internal" href="#distributed-periodic-tasks-configuration">Distributed periodic tasks configuration</a></li>
<li><a class="reference internal" href="#external-key-manager-usage">External key manager usage</a></li>
<li><a class="reference internal" href="#indirect-instance-access-through-proxy-nodes">Indirect instance access through proxy nodes</a></li>
<li><a class="reference internal" href="#multi-region-deployment">Multi region deployment</a></li>
<li><a class="reference internal" href="#non-root-users">Non-root users</a></li>
<li><a class="reference internal" href="#object-storage-access-using-proxy-users">Object Storage access using proxy users</a><ul>
<li><a class="reference internal" href="#detailed-instructions">Detailed instructions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#volume-instance-locality-configuration">Volume instance locality configuration</a></li>
<li><a class="reference internal" href="#autoconfiguration-for-templates">Autoconfiguration for templates</a></li>
<li><a class="reference internal" href="#ntp-service-configuration">NTP service configuration</a></li>
<li><a class="reference internal" href="#cors-cross-origin-resource-sharing-configuration">CORS (Cross Origin Resource Sharing) Configuration</a></li>
<li><a class="reference internal" href="#cleanup-time-for-incomplete-clusters">Cleanup time for incomplete clusters</a></li>
<li><a class="reference internal" href="#security-group-rules-configuration">Security Group Rules Configuration</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="dashboard.guide.html"
                                  title="previous chapter">OpenStack Dashboard Configuration Guide</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="upgrade.guide.html"
                                  title="next chapter">Sahara Upgrade Guide</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/sahara
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../_sources/userdoc/advanced.configuration.guide.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="upgrade.guide.html" title="Sahara Upgrade Guide"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="dashboard.guide.html" title="OpenStack Dashboard Configuration Guide"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Sahara</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2014, OpenStack Foundation.
      Last updated on &#39;Mon Feb 13 17:36:11 2017, commit 23ee18f&#39;.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/sahara");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>