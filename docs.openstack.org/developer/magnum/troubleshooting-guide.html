<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Magnum Troubleshooting Guide &mdash; magnum 4.0.1.dev40 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '4.0.1.dev40',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="magnum 4.0.1.dev40 documentation" href="index.html" />
    <link rel="next" title="Magnum User Guide" href="userguide.html" />
    <link rel="prev" title="Guru Meditation Reports" href="gmr.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="magnum-troubleshooting-guide">
<h1>Magnum Troubleshooting Guide<a class="headerlink" href="#magnum-troubleshooting-guide" title="Permalink to this headline">¶</a></h1>
<p>This guide is intended for users who use Magnum to deploy and manage
clusters of hosts for a Container Orchestration Engine.  It describes
common failure conditions and techniques for troubleshooting.  To help
the users quickly identify the relevant information, the guide is
organized as a list of failure symptoms: each has some suggestions
with pointers to the details for troubleshooting.</p>
<p>A separate section <a class="reference internal" href="#for-developers">for developers</a> describes useful techniques such as
debugging unit tests and gate tests.</p>
</div>
<div class="section" id="failure-symptoms">
<h1>Failure symptoms<a class="headerlink" href="#failure-symptoms" title="Permalink to this headline">¶</a></h1>
<dl class="docutils">
<dt>My cluster-create takes a really long time</dt>
<dd>If you are using devstack on a small VM, cluster-create will take a long
time and may eventually fail because of insufficient resources.
Another possible reason is that a process on one of the nodes is hung
and heat is still waiting on the signal.  In this case, it will eventually
fail with a timeout, but since heat has a long default timeout, you can
look at the <a class="reference internal" href="#heat-stacks">heat stacks</a> and check the WaitConditionHandle resources.</dd>
<dt>My cluster-create fails with error: &#8220;Failed to create trustee XXX in domain XXX&#8221;</dt>
<dd>Check the <a class="reference internal" href="#trustee-for-cluster">trustee for cluster</a></dd>
<dt>Kubernetes cluster-create fails</dt>
<dd>Check the <a class="reference internal" href="#heat-stacks">heat stacks</a>, log into the master nodes and check the
<a class="reference internal" href="#kubernetes-services">Kubernetes services</a> and <a class="reference internal" href="#etcd-service">etcd service</a>.</dd>
<dt>Swarm cluster-create fails</dt>
<dd>Check the <a class="reference internal" href="#heat-stacks">heat stacks</a>, log into the master nodes and check the <a class="reference internal" href="#swarm-services">Swarm
services</a> and <a class="reference internal" href="#etcd-service">etcd service</a>.</dd>
<dt>Mesos cluster-create fails</dt>
<dd>Check the <a class="reference internal" href="#heat-stacks">heat stacks</a>, log into the master nodes and check the <a class="reference internal" href="#mesos-services">Mesos
services</a>.</dd>
<dt>I get the error &#8220;Timed out waiting for a reply&#8221; when deploying a pod</dt>
<dd>Verify the <a class="reference internal" href="#kubernetes-services">Kubernetes services</a> and <a class="reference internal" href="#etcd-service">etcd service</a> are running on the
master nodes.</dd>
<dt>I deploy pods on Kubernetes cluster but the status stays &#8220;Pending&#8221;</dt>
<dd>The pod status is &#8220;Pending&#8221; while the Docker image is being downloaded,
so if the status does not change for a long time, log into the minion
node and check for <a class="reference internal" href="#cluster-internet-access">Cluster internet access</a>.</dd>
<dt>I deploy pods and services on Kubernetes cluster but the app is not working</dt>
<dd>The pods and services are running and the status looks correct, but
if the app is performing communication between pods through services,
verify <a class="reference internal" href="#kubernetes-networking">Kubernetes networking</a>.</dd>
<dt>Swarm cluster is created successfully but I cannot deploy containers</dt>
<dd>Check the <a class="reference internal" href="#swarm-services">Swarm services</a> and <a class="reference internal" href="#etcd-service">etcd service</a> on the master nodes.</dd>
<dt>Mesos cluster is created successfully but I cannot deploy containers on Marathon</dt>
<dd>Check the <a class="reference internal" href="#mesos-services">Mesos services</a> on the master node.</dd>
<dt>I get a &#8220;Protocol violation&#8221; error when deploying a container</dt>
<dd>For Kubernetes, check the <a class="reference internal" href="#kubernetes-services">Kubernetes services</a> to verify that
kube-apiserver is running to accept the request.
Check <a class="reference internal" href="#tls">TLS</a> and <a class="reference internal" href="#barbican-service">Barbican service</a>.</dd>
<dt>My cluster-create fails with a resource error on docker_volume</dt>
<dd>Check for available volume space on Cinder and the <a class="reference internal" href="#request-volume-size">request volume
size</a> in the heat template.
Run &#8220;nova volume-list&#8221; to check the volume status.</dd>
</dl>
</div>
<div class="section" id="troubleshooting-details">
<h1>Troubleshooting details<a class="headerlink" href="#troubleshooting-details" title="Permalink to this headline">¶</a></h1>
<div class="section" id="heat-stacks">
<h2>Heat stacks<a class="headerlink" href="#heat-stacks" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
<p>A cluster is deployed by a set of heat stacks:  one top level stack and several
nested stack.  The stack names are prefixed with the cluster name and the
nested stack names contain descriptive internal names like <em>kube_masters</em>,
<em>kube_minions</em>.</p>
<p>To list the status of all the stacks for a cluster:</p>
<blockquote>
<div>heat stack-list -n | grep <em>cluster-name</em></div></blockquote>
<p>If the cluster has failed, then one or more of the heat stacks would have
failed. From the stack list above, look for the stacks that failed, then
look for the particular resource(s) that failed in the failed stack by:</p>
<blockquote>
<div>heat resource-list <em>failed-stack-name</em> | grep &#8220;FAILED&#8221;</div></blockquote>
<p>The resource_type of the failed resource should point to the OpenStack
service, e.g. OS::Cinder::Volume.  Check for more details on the failure by:</p>
<blockquote>
<div>heat resource-show <em>failed-stack-name</em> <em>failed-resource-name</em></div></blockquote>
<p>The resource_status_reason may give an indication on the failure, although
in some cases it may only say &#8220;Unknown&#8221;.</p>
<p>If the failed resource is OS::Heat::WaitConditionHandle, this indicates that
one of the services that are being started on the node is hung.  Log into the
node where the failure occurred and check the respective <a class="reference internal" href="#kubernetes-services">Kubernetes
services</a>, <a class="reference internal" href="#swarm-services">Swarm services</a> or <a class="reference internal" href="#mesos-services">Mesos services</a>.  If the failure is in
other scripts, look for them as <a class="reference internal" href="#heat-software-resource-scripts">Heat software resource scripts</a>.</p>
</div>
<div class="section" id="trustee-for-cluster">
<h2>Trustee for cluster<a class="headerlink" href="#trustee-for-cluster" title="Permalink to this headline">¶</a></h2>
<p>When a user creates a cluster, Magnum will dynamically create a service account
for the cluster. The service account will be used by the cluster to
access the OpenStack services (i.e. Neutron, Swift, etc.). A trust relationship
will be created between the user who created the cluster (the &#8220;trustor&#8221;) and
the service account created for the cluster (the &#8220;trustee&#8221;). For details,
please refer
&lt;<a class="reference external" href="http://git.openstack.org/cgit/openstack/magnum/tree/specs/create-trustee-user-for-each-cluster.rst">http://git.openstack.org/cgit/openstack/magnum/tree/specs/create-trustee-user-for-each-cluster.rst</a>&gt;`_.</p>
<p>If Magnum fails to create the trustee, check the magnum config file (usually
in /etc/magnum/magnum.conf). Make sure &#8216;trustee_*&#8217; and &#8216;auth_uri&#8217; are set and
their values are correct:</p>
<blockquote>
<div><p>[keystone_authtoken]
auth_uri = <a class="reference external" href="http://controller:5000/v3">http://controller:5000/v3</a>
...</p>
<p>[trust]
trustee_domain_admin_password = XXX
trustee_domain_admin_id = XXX
trustee_domain_id = XXX</p>
</div></blockquote>
<p>If the &#8216;trust&#8217; group is missing, you might need to create the trustee domain
and the domain admin:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nb">source</span> /opt/stack/devstack/accrc/admin/admin
<span class="nb">export</span> <span class="nv">OS_IDENTITY_API_VERSION</span><span class="o">=</span><span class="m">3</span>
<span class="nb">unset</span> OS_AUTH_TYPE
openstack domain create magnum
openstack user create trustee_domain_admin --password<span class="o">=</span>secret <span class="se">\</span>
    --domain<span class="o">=</span>magnum
openstack role add --user<span class="o">=</span>trustee_domain_admin --user-domain magnum --domain<span class="o">=</span>magnum admin

<span class="nb">source</span> /opt/stack/devstack/functions
<span class="nb">export</span> <span class="nv">MAGNUM_CONF</span><span class="o">=</span>/etc/magnum/magnum.conf
iniset <span class="nv">$MAGNUM_CONF</span> trust trustee_domain_id <span class="se">\</span>
    <span class="k">$(</span>openstack domain show magnum <span class="p">|</span> awk <span class="s1">&#39;/ id /{print $4}&#39;</span><span class="k">)</span>
iniset <span class="nv">$MAGNUM_CONF</span> trust trustee_domain_admin_id <span class="se">\</span>
    <span class="k">$(</span>openstack user show trustee_domain_admin <span class="p">|</span> awk <span class="s1">&#39;/ id /{print $4}&#39;</span><span class="k">)</span>
iniset <span class="nv">$MAGNUM_CONF</span> trust trustee_domain_admin_password secret
</pre></div>
</div>
<p>Then, restart magnum-api and magnum-cond to pick up the new configuration.
If the problem still exists, you might want to manually verify your domain
admin credential to ensure it has the right privilege. To do that, run the
script below with the credentials replaced (you must use the IDs where
specified). If it fails, that means the credential you provided is invalid.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keystoneauth1.identity</span> <span class="kn">import</span> <span class="n">v3</span> <span class="k">as</span> <span class="n">ka_v3</span>
<span class="kn">from</span> <span class="nn">keystoneauth1</span> <span class="kn">import</span> <span class="n">session</span> <span class="k">as</span> <span class="n">ka_session</span>
<span class="kn">from</span> <span class="nn">keystoneclient.v3</span> <span class="kn">import</span> <span class="n">client</span> <span class="k">as</span> <span class="n">kc_v3</span>

<span class="n">auth</span> <span class="o">=</span> <span class="n">ka_v3</span><span class="o">.</span><span class="n">Password</span><span class="p">(</span>
    <span class="n">auth_url</span><span class="o">=</span><span class="n">YOUR_AUTH_URI</span><span class="p">,</span>
    <span class="n">user_id</span><span class="o">=</span><span class="n">YOUR_TRUSTEE_DOMAIN_ADMIN_ID</span><span class="p">,</span>
    <span class="n">domain_id</span><span class="o">=</span><span class="n">YOUR_TRUSTEE_DOMAIN_ID</span><span class="p">,</span>
    <span class="n">password</span><span class="o">=</span><span class="n">YOUR_TRUSTEE_DOMAIN_ADMIN_PASSWORD</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">ka_session</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">auth</span><span class="o">=</span><span class="n">auth</span><span class="p">)</span>
<span class="n">domain_admin_client</span> <span class="o">=</span> <span class="n">kc_v3</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>
<span class="n">user</span> <span class="o">=</span> <span class="n">domain_admin_client</span><span class="o">.</span><span class="n">users</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;anyname&#39;</span><span class="p">,</span>
    <span class="n">password</span><span class="o">=</span><span class="s1">&#39;anypass&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="tls">
<h2>TLS<a class="headerlink" href="#tls" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="barbican-service">
<h2>Barbican service<a class="headerlink" href="#barbican-service" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="cluster-internet-access">
<h2>Cluster internet access<a class="headerlink" href="#cluster-internet-access" title="Permalink to this headline">¶</a></h2>
<p>The nodes for Kubernetes, Swarm and Mesos are connected to a private
Neutron network, so to provide access to the external internet, a router
connects the private network to a public network.  With devstack, the
default public network is &#8220;public&#8221;, but this can be replaced by the
parameter &#8220;external-network&#8221; in the ClusterTemplate.  The &#8220;public&#8221; network
with devstack is actually not a real external network, so it is in turn
routed to the network interface of the host for devstack.  This is
configured in the file local.conf with the variable PUBLIC_INTERFACE,
for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">PUBLIC_INTERFACE</span><span class="o">=</span><span class="n">eth1</span>
</pre></div>
</div>
<p>If the route to the external internet is not set up properly, the ectd
discovery would fail (if using public discovery) and container images
cannot be downloaded, among other failures.</p>
<p>First, check for connectivity to the external internet by pinging
an external IP (the IP shown here is an example; use an IP that
works in your case):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>ping 8.8.8.8
</pre></div>
</div>
<p>If the ping fails, there is no route to the external internet.
Check the following:</p>
<ul class="simple">
<li>Is PUBLIC_INTERFACE in devstack/local.conf the correct network
interface?  Does this interface have a route to the external internet?</li>
<li>If &#8220;external-network&#8221; is specified in the ClusterTemplate, does this
network have a route to the external internet?</li>
<li>Is your devstack environment behind a firewall?  This can be the case for some
enterprises or countries.  In this case, consider using a <a class="reference external" href="https://github.com/openstack/magnum/blob/master/doc/source/magnum-proxy.rst">proxy server</a>.</li>
<li>Is the traffic blocked by the security group? Check the
<a class="reference external" href="http://docs.openstack.org/ops-guide/ops-user-facing-operations.html#security-groups">rules of security group</a>.</li>
<li>Is your host NAT&#8217;ing your internal network correctly? Check your host
<a class="reference external" href="http://docs.openstack.org/ops-guide/ops-network-troubleshooting.html#iptables">iptables</a>.</li>
<li>Use <em>tcpdump</em> for <a class="reference external" href="http://docs.openstack.org/ops-guide/ops-network-troubleshooting.html#tcpdump">networking troubleshooting</a>.
You can run <em>tcpdump</em> on the interface <em>docker0, flannel0</em> and <em>eth0</em> on the
node and then run <em>ping</em> to see the path of the message from the container.</li>
</ul>
<p>If ping is successful, check that DNS is working:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>wget google.com
</pre></div>
</div>
<p>If DNS works, you should get back a few lines of HTML text.</p>
<p>If the name lookup fails, check the following:</p>
<ul class="simple">
<li>Is the DNS entry correct in the subnet?  Try &#8220;neutron subnet-show
&lt;subnet-id&gt;&#8221; for the private subnet and check dns_nameservers.
The IP should be either the default public DNS 8.8.8.8 or the value
specified by &#8220;dns-nameserver&#8221; in the ClusterTemplate.</li>
<li>If you are using your own DNS server by specifying &#8220;dns-nameserver&#8221;
in the ClusterTemplate, is it reachable and working?</li>
<li>More help on <a class="reference external" href="http://docs.openstack.org/ops-guide/ops-network-troubleshooting.html#debugging-dns-issues">DNS troubleshooting</a>.</li>
</ul>
</div>
<div class="section" id="kubernetes-networking">
<h2>Kubernetes networking<a class="headerlink" href="#kubernetes-networking" title="Permalink to this headline">¶</a></h2>
<p>The networking between pods is different and separate from the neutron
network set up for the cluster.
Kubernetes presents a flat network space for the pods and services
and uses different network drivers to provide this network model.</p>
<p>It is possible for the pods to come up correctly and be able to connect
to the external internet, but they cannot reach each other.
In this case, the app in the pods may not be working as expected.
For example, if you are trying the <a class="reference external" href="https://github.com/kubernetes/kubernetes/blob/release-1.1/examples/redis/README.md">redis example</a>,
the key:value may not be replicated correctly.  In this case, use the
following steps to verify the inter-pods networking and pinpoint problems.</p>
<p>Since the steps are specific to the network drivers, refer to the
particular driver being used for the cluster.</p>
<div class="section" id="using-flannel-as-network-driver">
<h3>Using Flannel as network driver<a class="headerlink" href="#using-flannel-as-network-driver" title="Permalink to this headline">¶</a></h3>
<p>Flannel is the default network driver for Kubernetes clusters.  Flannel is
an overlay network that runs on top of the neutron network.  It works by
encapsulating the messages between pods and forwarding them to the
correct node that hosts the target pod.</p>
<p>First check the connectivity at the node level.  Log into two
different minion nodes, e.g. node A and node B, run a docker container
on each node, attach to the container and find the IP.</p>
<p>For example, on node A:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo docker run -it alpine
# ip -f inet -o a | grep eth0 | awk &#39;{print $4}&#39;
10.100.54.2/24
</pre></div>
</div>
<p>Similarly, on node B:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo docker run -it alpine
# ip -f inet -o a | grep eth0 | awk &#39;{print $4}&#39;
10.100.49.3/24
</pre></div>
</div>
<p>Check that the containers can see each other by pinging from one to another.</p>
<p>On node A:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># ping 10.100.49.3
PING 10.100.49.3 (10.100.49.3): 56 data bytes
64 bytes from 10.100.49.3: seq=0 ttl=60 time=1.868 ms
64 bytes from 10.100.49.3: seq=1 ttl=60 time=1.108 ms
</pre></div>
</div>
<p>Similarly, on node B:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># ping 10.100.54.2
PING 10.100.54.2 (10.100.54.2): 56 data bytes
64 bytes from 10.100.54.2: seq=0 ttl=60 time=2.678 ms
64 bytes from 10.100.54.2: seq=1 ttl=60 time=1.240 ms
</pre></div>
</div>
<p>If the ping is not successful, check the following:</p>
<ul>
<li><p class="first">Is neutron working properly?  Try pinging between the VMs.</p>
</li>
<li><p class="first">Are the docker0 and flannel0 interfaces configured correctly on the
nodes? Log into each node and find the Flannel CIDR by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cat /run/flannel/subnet.env | grep FLANNEL_SUBNET
FLANNEL_SUBNET=10.100.54.1/24
</pre></div>
</div>
<p>Then check the interfaces by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>ifconfig flannel0
ifconfig docker0
</pre></div>
</div>
<p>The correct configuration should assign flannel0 with the &#8220;0&#8221; address
in the subnet, like <em>10.100.54.0</em>, and docker0 with the &#8220;1&#8221; address, like
<em>10.100.54.1</em>.</p>
</li>
<li><p class="first">Verify the IP&#8217;s assigned to the nodes as found above are in the correct
Flannel subnet.  If this is not correct, the docker daemon is not configured
correctly with the parameter <em>&#8211;bip</em>.  Check the systemd service for docker.</p>
</li>
<li><p class="first">Is Flannel running properly?  check the <a class="reference internal" href="#running-flannel">Running Flannel</a>.</p>
</li>
<li><p class="first">Ping and try <a class="reference external" href="http://docs.openstack.org/ops-guide/ops-network-troubleshooting.html#tcpdump">tcpdump</a>
on each network interface along the path between two nodes
to see how far the message is able to travel.
The message path should be as follows:</p>
<ol class="arabic simple">
<li>Source node: docker0</li>
<li>Source node: flannel0</li>
<li>Source node: eth0</li>
<li>Target node: eth0</li>
<li>Target node: flannel0</li>
<li>Target node: docker0</li>
</ol>
</li>
</ul>
<p>If ping works, this means the flannel overlay network is functioning
correctly.</p>
<p>The containers created by Kubernetes for pods will be on the same IP
subnet as the containers created directly in Docker as above, so they
will have the same connectivity.  However, the pods still may not be
able to reach each other because normally they connect through some
Kubernetes services rather than directly.  The services are supported
by the kube-proxy and rules inserted into the iptables, therefore
their networking paths have some extra hops and there may be problems
here.</p>
<p>To check the connectivity at the Kubernetes pod level, log into the
master node and create two pods and a service for one of the pods.
You can use the examples provided in the directory
<em>/etc/kubernetes/examples/</em> for the first pod and service.  This will
start up an nginx container and a Kubernetes service to expose the
endpoint.  Create another manifest for a second pod to test the
endpoint:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cat &gt; alpine.yaml &lt;&lt; END
apiVersion: v1
kind: Pod
metadata:
  name: alpine
spec:
  containers:
  - name: alpine
    image: alpine
    args:
    - sleep
    - &quot;1000000&quot;
END

kubectl create -f /etc/kubernetes/examples/pod-nginx-with-label.yaml
kubectl create -f /etc/kubernetes/examples/service.yaml
kubectl create -f alpine.yaml
</pre></div>
</div>
<p>Get the endpoint for the nginx-service, which should route message to the pod
nginx:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl describe service nginx-service | grep -e IP: -e Port:
IP:                     10.254.21.158
Port:                   &lt;unnamed&gt;       8000/TCP
</pre></div>
</div>
<p>Note the IP and port to use for checking below.  Log into the node
where the <em>alpine</em> pod is running.  You can find the hosting node by
running this command on the master node:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl get pods -o wide  | grep alpine | awk &#39;{print $6}&#39;
k8-gzvjwcooto-0-gsrxhmyjupbi-kube-minion-br73i6ans2b4
</pre></div>
</div>
<p>To get the IP of the node, query Nova on devstack:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>nova list
</pre></div>
</div>
<p>On this hosting node, attach to the <em>alpine</em> container:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>export DOCKER_ID=`sudo docker ps | grep k8s_alpine | awk &#39;{print $1}&#39;`
sudo docker exec -it $DOCKER_ID sh
</pre></div>
</div>
<p>From the <em>alpine</em> pod, you can try to reach the nginx pod through the nginx
service using the IP and Port found above:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>wget 10.254.21.158:8000
</pre></div>
</div>
<p>If the connection is successful, you should receive the file <em>index.html</em> from
nginx.</p>
<p>If the connection is not successful, you will get an error message like::xs</p>
<blockquote>
<div>wget: can&#8217;t connect to remote host (10.100.54.9): No route to host</div></blockquote>
<p>In this case, check the following:</p>
<ul>
<li><p class="first">Is kube-proxy running on the nodes? It runs as a container on each node.
check by logging in the minion nodes and run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo docker ps | grep k8s_kube-proxy
</pre></div>
</div>
</li>
<li><p class="first">Check the log from kube-proxy by running on the minion nodes:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>export PROXY=`sudo docker ps | grep &quot;hyperkube proxy&quot; | awk &#39;{print $1}&#39;`
sudo docker logs $PROXY
</pre></div>
</div>
</li>
<li><p class="first">Try additional <a class="reference external" href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/user-guide/debugging-services.md">service debugging</a>.
To see what&#8217;s going during provisioning:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl get events
</pre></div>
</div>
<p>To get information on a service in question:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl describe services &lt;service_name&gt;
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="etcd-service">
<h2>etcd service<a class="headerlink" href="#etcd-service" title="Permalink to this headline">¶</a></h2>
<p>The etcd service is used by many other components for key/value pair
management, therefore if it fails to start, these other components
will not be running correctly either.
Check that etcd is running on the master nodes by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo service etcd status -l
</pre></div>
</div>
<p>If it is running correctly, you should see that the service is
successfully deployed:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Active: active (running) since ....
</pre></div>
</div>
<p>The log message should show the service being published:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>etcdserver: published {Name:10.0.0.5 ClientURLs:[http://10.0.0.5:2379]} to cluster 3451e4c04ec92893
</pre></div>
</div>
<p>In some cases, the service may show as <em>active</em> but may still be stuck
in discovery mode and not fully operational.  The log message may show
something like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>discovery: waiting for other nodes: error connecting to https://discovery.etcd.io, retrying in 8m32s
</pre></div>
</div>
<p>If this condition persists, check for <a class="reference internal" href="#cluster-internet-access">Cluster internet access</a>.</p>
<p>If the daemon is not running, the status will show the service as failed,
something like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Active: failed (Result: timeout)
</pre></div>
</div>
<p>In this case, try restarting etcd by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo service etcd start
</pre></div>
</div>
<p>If etcd continues to fail, check the following:</p>
<ul>
<li><p class="first">Check the log for etcd:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo journalctl -u etcd
</pre></div>
</div>
</li>
<li><p class="first">etcd requires discovery, and the default discovery method is the
public discovery service provided by etcd.io; therefore, a common
cause of failure is that this public discovery service is not
reachable.  Check by running on the master nodes:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>source /etc/sysconfig/heat-params
curl $ETCD_DISCOVERY_URL
</pre></div>
</div>
<p>You should receive something like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;action&quot;</span><span class="p">:</span><span class="s2">&quot;get&quot;</span><span class="p">,</span>
 <span class="s2">&quot;node&quot;</span><span class="p">:{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span><span class="s2">&quot;/_etcd/registry/00a6b00064174c92411b0f09ad5466c6&quot;</span><span class="p">,</span>
         <span class="s2">&quot;dir&quot;</span><span class="p">:</span><span class="n">true</span><span class="p">,</span>
         <span class="s2">&quot;nodes&quot;</span><span class="p">:[</span>
           <span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span><span class="s2">&quot;/_etcd/registry/00a6b00064174c92411b0f09ad5466c6/7d8a68781a20c0a5&quot;</span><span class="p">,</span>
            <span class="s2">&quot;value&quot;</span><span class="p">:</span><span class="s2">&quot;10.0.0.5=http://10.0.0.5:2380&quot;</span><span class="p">,</span>
            <span class="s2">&quot;modifiedIndex&quot;</span><span class="p">:</span><span class="mi">978239406</span><span class="p">,</span>
            <span class="s2">&quot;createdIndex&quot;</span><span class="p">:</span><span class="mi">978239406</span><span class="p">}],</span>
         <span class="s2">&quot;modifiedIndex&quot;</span><span class="p">:</span><span class="mi">978237118</span><span class="p">,</span>
         <span class="s2">&quot;createdIndex&quot;</span><span class="p">:</span><span class="mi">978237118</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The list of master IP is provided by Magnum during cluster deployment,
therefore it should match the current IP of the master nodes.
If the public discovery service is not reachable, check the
<a class="reference internal" href="#cluster-internet-access">Cluster internet access</a>.</p>
</li>
</ul>
</div>
<div class="section" id="running-flannel">
<h2>Running Flannel<a class="headerlink" href="#running-flannel" title="Permalink to this headline">¶</a></h2>
<p>When deploying a COE, Flannel is available as a network driver for
certain COE type.  Magnum currently supports Flannel for a Kubernetes
or Swarm cluster.</p>
<p>Flannel provides a flat network space for the containers in the cluster:
they are allocated IP in this network space and they will have connectivity
to each other.  Therefore, if Flannel fails, some containers will not
be able to access services from other containers in the cluster.  This can be
confirmed by running <em>ping</em> or <em>curl</em> from one container to another.</p>
<p>The Flannel daemon is run as a systemd service on each node of the cluster.
To check Flannel, run on each node:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo service flanneld status
</pre></div>
</div>
<p>If the daemon is running, you should see that the service is successfully
deployed:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Active: active (running) since ....
</pre></div>
</div>
<p>If the daemon is not running, the status will show the service as failed,
something like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Active: failed (Result: timeout) ....
</pre></div>
</div>
<p>or:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Active: inactive (dead) ....
</pre></div>
</div>
<p>Flannel daemon may also be running but not functioning correctly.
Check the following:</p>
<ul>
<li><p class="first">Check the log for Flannel:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo journalctl -u flanneld
</pre></div>
</div>
</li>
<li><p class="first">Since Flannel relies on etcd, a common cause for failure is that the
etcd service is not running on the master nodes.  Check the <a class="reference internal" href="#etcd-service">etcd service</a>.
If the etcd service failed, once it has been restored successfully, the
Flannel service can be restarted by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo service flanneld restart
</pre></div>
</div>
</li>
<li><p class="first">Magnum writes the configuration for Flannel in a local file on each master
node.  Check for this file on the master nodes by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cat</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">sysconfig</span><span class="o">/</span><span class="n">flannel</span><span class="o">-</span><span class="n">network</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>The content should be something like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;Network&quot;</span><span class="p">:</span> <span class="s2">&quot;10.100.0.0/16&quot;</span><span class="p">,</span>
  <span class="s2">&quot;Subnetlen&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
  <span class="s2">&quot;Backend&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;Type&quot;</span><span class="p">:</span> <span class="s2">&quot;udp&quot;</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>where the values for the parameters must match the corresponding
parameters from the ClusterTemplate.</p>
<p>Magnum also loads this configuration into etcd, therefore, verify
the configuration in etcd by running <em>etcdctl</em> on the master nodes:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>. /etc/sysconfig/flanneld
etcdctl get $FLANNEL_ETCD_KEY/config
</pre></div>
</div>
</li>
<li><p class="first">Each node is allocated a segment of the network space.  Check
for this segment on each node by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>grep FLANNEL_SUBNET /run/flannel/subnet.env
</pre></div>
</div>
<p>The containers on this node should be assigned an IP in this range.
The nodes negotiate for their segment through etcd, and you can use
<em>etcdctl</em> on the master node to query the network segment associated
with each node:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>. /etc/sysconfig/flanneld
for s in `etcdctl ls $FLANNEL_ETCD_KEY/subnets`
do
echo $s
etcdctl get $s
done

/atomic.io/network/subnets/10.100.14.0-24
{&quot;PublicIP&quot;:&quot;10.0.0.5&quot;}
/atomic.io/network/subnets/10.100.61.0-24
{&quot;PublicIP&quot;:&quot;10.0.0.6&quot;}
/atomic.io/network/subnets/10.100.92.0-24
{&quot;PublicIP&quot;:&quot;10.0.0.7&quot;}
</pre></div>
</div>
<p>Alternatively, you can read the full record in ectd by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>curl http://&lt;master_node_ip&gt;:2379/v2/keys/coreos.com/network/subnets
</pre></div>
</div>
<p>You should receive a JSON snippet that describes all the segments
allocated.</p>
</li>
<li><p class="first">This network segment is passed to Docker via the parameter <em>&#8211;bip</em>.
If this is not configured correctly, Docker would not assign the correct
IP in the Flannel network segment to the container.  Check by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cat /run/flannel/docker
ps -aux | grep docker
</pre></div>
</div>
</li>
<li><p class="first">Check the interface for Flannel:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>ifconfig flannel0
</pre></div>
</div>
<p>The IP should be the first address in the Flannel subnet for this node.</p>
</li>
<li><p class="first">Flannel has several different backend implementations and they have
specific requirements.  The <em>udp</em> backend is the most general and have
no requirement on the network.  The <em>vxlan</em> backend requires vxlan
support in the kernel, so ensure that the image used does provide
vxlan support.  The <em>host-gw</em> backend requires that all the hosts are
on the same L2 network.  This is currently met by the private Neutron
subnet created by Magnum;  however, if other network topology is used
instead, ensure that this requirement is met if <em>host-gw</em> is used.</p>
</li>
</ul>
<p>Current known limitation:  the image fedora-21-atomic-5.qcow2 has
Flannel version 0.5.0.  This version has known bugs that prevent the
backend vxland and host-gw to work correctly.  Only the backend udp
works for this image.  Version 0.5.3 and later should work correctly.
The image fedora-21-atomic-7.qcow2 has Flannel version 0.5.5.</p>
</div>
<div class="section" id="kubernetes-services">
<h2>Kubernetes services<a class="headerlink" href="#kubernetes-services" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
<p>(How to introspect k8s when heat works and k8s does not)</p>
<p>Additional <a class="reference external" href="http://kubernetes.io/v1.0/docs/troubleshooting.html">Kubenetes troubleshooting guide</a> is available.</p>
</div>
<div class="section" id="swarm-services">
<h2>Swarm services<a class="headerlink" href="#swarm-services" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
<p>(How to check on a swarm cluster: see membership information, view master,
agent containers)</p>
</div>
<div class="section" id="mesos-services">
<h2>Mesos services<a class="headerlink" href="#mesos-services" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="barbican-issues">
<h2>Barbican issues<a class="headerlink" href="#barbican-issues" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="docker-cli">
<h2>Docker CLI<a class="headerlink" href="#docker-cli" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="request-volume-size">
<h2>Request volume size<a class="headerlink" href="#request-volume-size" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="heat-software-resource-scripts">
<h2>Heat software resource scripts<a class="headerlink" href="#heat-software-resource-scripts" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
</div>
<div class="section" id="for-developers">
<h1>For Developers<a class="headerlink" href="#for-developers" title="Permalink to this headline">¶</a></h1>
<p>This section is intended to help with issues that developers may
run into in the course of their development adventures in Magnum.</p>
<div class="section" id="troubleshooting-in-gate">
<h2>Troubleshooting in Gate<a class="headerlink" href="#troubleshooting-in-gate" title="Permalink to this headline">¶</a></h2>
<dl class="docutils">
<dt>Simulating gate tests</dt>
<dd><p class="first"><em>Note</em>: This is adapted from Devstack Gate&#8217;s <a class="reference external" href="https://github.com/openstack-infra/devstack-gate/blob/master/README.rst#simulating-devstack-gate-testsP">README</a> which
is worth a quick read to better understand the following)</p>
<ol class="last arabic">
<li><p class="first">Boot a VM like described in the Devstack Gate&#8217;s <a class="reference external" href="https://github.com/openstack-infra/devstack-gate/blob/master/README.rst#simulating-devstack-gate-testsP">README</a> .</p>
</li>
<li><p class="first">Provision this VM like so:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>apt-get update \
&amp;&amp; apt-get upgrade -y \ # Kernel upgrade, as recommended by README, select to keep existing grub config
&amp;&amp; apt-get install -y git tmux vim \
&amp;&amp; git clone https://git.openstack.org/openstack-infra/system-config \
&amp;&amp; system-config/install_puppet.sh &amp;&amp; system-config/install_modules.sh \
&amp;&amp; puppet apply \
--modulepath=/root/system-config/modules:/etc/puppet/modules \
-e &quot;class { openstack_project::single_use_slave: install_users =&gt; false,
ssh_key =&gt; \&quot;$( cat .ssh/authorized_keys | awk &#39;{print $2}&#39; )\&quot; }&quot; \
&amp;&amp; echo &quot;jenkins ALL=(ALL) NOPASSWD:ALL&quot; &gt;&gt; /etc/sudoers \
&amp;&amp; cat ~/.ssh/authorized_keys &gt;&gt; /home/jenkins/.ssh/authorized_keys
</pre></div>
</div>
</li>
<li><p class="first">Compare <code class="docutils literal"><span class="pre">~/.ssh/authorized_keys</span></code> and <code class="docutils literal"><span class="pre">/home/jenkins/.ssh/authorized_keys</span></code>.  Your original public SSH key should now be in <code class="docutils literal"><span class="pre">/home/jenkins/.ssh/authorized_keys</span></code>.  If it&#8217;s not, explicitly copy it (this can happen if you spin up a using <code class="docutils literal"><span class="pre">--key-name</span> <span class="pre">&lt;name&gt;</span></code>, for example).</p>
</li>
<li><p class="first">Assuming all is well up to this point, now it&#8217;s time to <code class="docutils literal"><span class="pre">reboot</span></code> into the latest kernel</p>
</li>
<li><p class="first">Once you&#8217;re done booting into the new kernel, log back in as <code class="docutils literal"><span class="pre">jenkins</span></code> user to continue with setting up the simulation.</p>
</li>
<li><p class="first">Now it&#8217;s time to set up the workspace:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>export REPO_URL=https://git.openstack.org
export WORKSPACE=/home/jenkins/workspace/testing
export ZUUL_URL=/home/jenkins/workspace-cache2
export ZUUL_REF=HEAD
export ZUUL_BRANCH=master
export ZUUL_PROJECT=openstack/magnum
mkdir -p $WORKSPACE
git clone $REPO_URL/$ZUUL_PROJECT $ZUUL_URL/$ZUUL_PROJECT \
&amp;&amp; cd $ZUUL_URL/$ZUUL_PROJECT \
&amp;&amp; git checkout remotes/origin/$ZUUL_BRANCH
</pre></div>
</div>
</li>
<li><p class="first">At this point, you may be wanting to test a specific change. If so, you can pull down the changes in <code class="docutils literal"><span class="pre">$ZUUL_URL/$ZUUL_PROJECT</span></code> directory:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cd $ZUUL_URL/$ZUUL_PROJECT \
&amp;&amp; git fetch https://review.openstack.org/openstack/magnum refs/changes/83/247083/12 &amp;&amp; git checkout FETCH_HEAD
</pre></div>
</div>
</li>
<li><p class="first">Now you&#8217;re ready to pull down the <code class="docutils literal"><span class="pre">devstack-gate</span></code> scripts that will let you run the gate job on your own VM:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cd $WORKSPACE \
&amp;&amp; git clone --depth 1 $REPO_URL/openstack-infra/devstack-gate
</pre></div>
</div>
</li>
<li><p class="first">And now you can kick off the job using the following script (the <code class="docutils literal"><span class="pre">devstack-gate</span></code> documentation suggests just copying from the job which can be found in the <a class="reference external" href="https://github.com/openstack-infra/project-config">project-config</a> repository), naturally it should be executable (<code class="docutils literal"><span class="pre">chmod</span> <span class="pre">u+x</span> <span class="pre">&lt;filename&gt;</span></code>):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>#!/bin/bash -xe
cat &gt; clonemap.yaml &lt;&lt; EOF
clonemap:
  - name: openstack-infra/devstack-gate
    dest: devstack-gate
EOF
/usr/zuul-env/bin/zuul-cloner -m clonemap.yaml --cache-dir /opt/git \
    git://git.openstack.org \
    openstack-infra/devstack-gate
export PYTHONUNBUFFERED=true
export DEVSTACK_GATE_TIMEOUT=240 # bump this if you see timeout issues.  Default is 120
export DEVSTACK_GATE_TEMPEST=0
export DEVSTACK_GATE_NEUTRON=1
# Enable tempest for tempest plugin
export ENABLED_SERVICES=tempest
export BRANCH_OVERRIDE=&quot;default&quot;
if [ &quot;$BRANCH_OVERRIDE&quot; != &quot;default&quot; ] ; then
    export OVERRIDE_ZUUL_BRANCH=$BRANCH_OVERRIDE
fi
export PROJECTS=&quot;openstack/magnum $PROJECTS&quot;
export PROJECTS=&quot;openstack/python-magnumclient $PROJECTS&quot;
export PROJECTS=&quot;openstack/barbican $PROJECTS&quot;
export DEVSTACK_LOCAL_CONFIG=&quot;enable_plugin magnum git://git.openstack.org/openstack/magnum&quot;
export DEVSTACK_LOCAL_CONFIG+=$&#39;\n&#39;&quot;enable_plugin ceilometer git://git.openstack.org/openstack/ceilometer&quot;
# Keep localrc to be able to set some vars in post_test_hook
export KEEP_LOCALRC=1
function gate_hook {
     cd /opt/stack/new/magnum/
    ./magnum/tests/contrib/gate_hook.sh api # change this to swarm to run swarm functional tests or k8s to run kubernetes functional tests
}
export -f gate_hook
function post_test_hook {
    source $BASE/new/devstack/accrc/admin/admin
    cd /opt/stack/new/magnum/
    ./magnum/tests/contrib/post_test_hook.sh api # change this to swarm to run swarm functional tests or k8s to run kubernetes functional tests
}
export -f post_test_hook
cp devstack-gate/devstack-vm-gate-wrap.sh ./safe-devstack-vm-gate-wrap.sh
./safe-devstack-vm-gate-wrap.sh
</pre></div>
</div>
</li>
</ol>
</dd>
<dt>Helpful nuances about the Devstack Gate</dt>
<dd><ul class="first last simple">
<li>Main job is in <code class="docutils literal"><span class="pre">project-config</span></code>&#8216;s <a class="reference external" href="https://github.com/openstack-infra/project-config/blob/master/jenkins/jobs/magnum.yaml">magnum.yaml</a>.<ul>
<li>Must modify parameters passed in since those are escaped:<ul>
<li>Anything with <code class="docutils literal"><span class="pre">{}</span></code> should be set as an environment variable</li>
<li>Anything with <code class="docutils literal"><span class="pre">{{</span> <span class="pre">}}</span></code> should have those brackets changed to
single brackets - <code class="docutils literal"><span class="pre">{}</span></code>.</li>
<li>As with the documentation for Devstack Gate, you can just create
a new file for the job you want, paste in what you want, then
<code class="docutils literal"><span class="pre">chmod</span> <span class="pre">u+x</span> <span class="pre">&lt;filename&gt;</span></code> and run it.</li>
</ul>
</li>
<li>Parameters can be found in <a class="reference external" href="https://github.com/openstack-infra/project-config/blob/master/jenkins/jobs/projects.yaml">projects.yaml</a>.
This file changes a lot, so it&#8217;s more reliable to say that you can
search for the magnum jobs where you&#8217;ll see examples of what
gets passed in.</li>
</ul>
</li>
<li>Three jobs are usually run as a part of Magnum gate, all of with are found in <code class="docutils literal"><span class="pre">project-config</span></code>&#8216;s <a class="reference external" href="https://github.com/openstack-infra/project-config/blob/master/jenkins/jobs/macros.yaml">macros.yml</a>:<ul>
<li>link-logs</li>
<li>net-info</li>
<li>devstack-checkout</li>
</ul>
</li>
<li>After you run a job, it&#8217;s ideal to clean up and start over with a
fresh VM to best simulate the Devstack Gate environment.</li>
</ul>
</dd>
</dl>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Magnum Troubleshooting Guide</a></li>
<li><a class="reference internal" href="#failure-symptoms">Failure symptoms</a></li>
<li><a class="reference internal" href="#troubleshooting-details">Troubleshooting details</a><ul>
<li><a class="reference internal" href="#heat-stacks">Heat stacks</a></li>
<li><a class="reference internal" href="#trustee-for-cluster">Trustee for cluster</a></li>
<li><a class="reference internal" href="#tls">TLS</a></li>
<li><a class="reference internal" href="#barbican-service">Barbican service</a></li>
<li><a class="reference internal" href="#cluster-internet-access">Cluster internet access</a></li>
<li><a class="reference internal" href="#kubernetes-networking">Kubernetes networking</a><ul>
<li><a class="reference internal" href="#using-flannel-as-network-driver">Using Flannel as network driver</a></li>
</ul>
</li>
<li><a class="reference internal" href="#etcd-service">etcd service</a></li>
<li><a class="reference internal" href="#running-flannel">Running Flannel</a></li>
<li><a class="reference internal" href="#kubernetes-services">Kubernetes services</a></li>
<li><a class="reference internal" href="#swarm-services">Swarm services</a></li>
<li><a class="reference internal" href="#mesos-services">Mesos services</a></li>
<li><a class="reference internal" href="#barbican-issues">Barbican issues</a></li>
<li><a class="reference internal" href="#docker-cli">Docker CLI</a></li>
<li><a class="reference internal" href="#request-volume-size">Request volume size</a></li>
<li><a class="reference internal" href="#heat-software-resource-scripts">Heat software resource scripts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#for-developers">For Developers</a><ul>
<li><a class="reference internal" href="#troubleshooting-in-gate">Troubleshooting in Gate</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="gmr.html"
                                  title="previous chapter">Guru Meditation Reports</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="userguide.html"
                                  title="next chapter">Magnum User Guide</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/magnum
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/troubleshooting-guide.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="userguide.html" title="Magnum User Guide"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="gmr.html" title="Guru Meditation Reports"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">magnum 4.0.1.dev40 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2013, OpenStack Foundation.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/magnum");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>