<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Magnum User Guide &mdash; magnum 4.0.1.dev40 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '4.0.1.dev40',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="magnum 4.0.1.dev40 documentation" href="index.html" />
    <link rel="next" title="Configuration" href="configuring.html" />
    <link rel="prev" title="Magnum Troubleshooting Guide" href="troubleshooting-guide.html" /> 
  </head>
  <body role="document">
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
      <li><a href="http://www.openstack.org/" title="Go to the Home page" class="link">Home</a></li>
      <li><a href="http://www.openstack.org/projects/" title="Go to the OpenStack Projects page">Projects</a></li>
      <li><a href="http://www.openstack.org/user-stories/" title="Go to the User Stories page" class="link">User Stories</a></li>
      <li><a href="http://www.openstack.org/community/" title="Go to the Community page" class="link">Community</a></li>
      <li><a href="http://www.openstack.org/blog/" title="Go to the OpenStack Blog">Blog</a></li>
      <li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
      <li><a href="http://docs.openstack.org/" title="Go to OpenStack Documentation" class="current">Documentation</a></li>
      
    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="magnum-user-guide">
<h1>Magnum User Guide<a class="headerlink" href="#magnum-user-guide" title="Permalink to this headline">¶</a></h1>
<p>This guide is intended for users who use Magnum to deploy and manage clusters
of hosts for a Container Orchestration Engine.  It describes the infrastructure
that Magnum creates and how to work with them.</p>
<p>Section 1-3 describe Magnum itself, including an overview, the CLI and
Horizon interface.  Section 4-9 describe the Container Orchestration
Engine (COE) supported along with a guide on how to select one that
best meets your needs and how to develop a driver for a new COE.
Section 10-15 describe the low level OpenStack infrastructure that is
created and managed by Magnum to support the COE&#8217;s.</p>
</div>
<div class="section" id="contents">
<h1>Contents<a class="headerlink" href="#contents" title="Permalink to this headline">¶</a></h1>
<ol class="arabic simple">
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#python-client">Python Client</a></li>
<li><a class="reference internal" href="#horizon-interface">Horizon Interface</a></li>
<li><a class="reference internal" href="#cluster-drivers">Cluster Drivers</a></li>
<li><a class="reference internal" href="#choosing-a-coe">Choosing a COE</a></li>
<li><a class="reference internal" href="#native-clients">Native Clients</a></li>
<li><a class="reference internal" href="#kubernetes">Kubernetes</a></li>
<li><a class="reference internal" href="#swarm">Swarm</a></li>
<li><a class="reference internal" href="#mesos">Mesos</a></li>
<li><a class="reference internal" href="#transport-layer-security">Transport Layer Security</a></li>
<li><a class="reference internal" href="#networking">Networking</a></li>
<li><a class="reference internal" href="#high-availability">High Availability</a></li>
<li><a class="reference internal" href="#scaling">Scaling</a></li>
<li><a class="reference internal" href="#storage">Storage</a></li>
<li><a class="reference internal" href="#image-management">Image Management</a></li>
<li><a class="reference internal" href="#notification">Notification</a></li>
</ol>
</div>
<div class="section" id="terminology">
<h1>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h1>
<dl class="docutils">
<dt>Cluster (previously Bay)</dt>
<dd>A cluster is the construct in which Magnum launches container orchestration
engines. After a cluster has been created the user is able to add containers
to it either directly, or in the case of the Kubernetes container
orchestration engine within pods - a logical construct specific to that
implementation. A cluster is created based on a ClusterTemplate.</dd>
<dt>ClusterTemplate (previously BayModel)</dt>
<dd>A ClusterTemplate in Magnum is roughly equivalent to a flavor in Nova. It
acts as a template that defines options such as the container orchestration
engine, keypair and image for use when Magnum is creating clusters using
the given ClusterTemplate.</dd>
<dt>Container Orchestration Engine (COE)</dt>
<dd>A container orchestration engine manages the lifecycle of one or more
containers, logically represented in Magnum as a cluster. Magnum supports a
number of container orchestration engines, each with their own pros and cons,
including Docker Swarm, Kubernetes, and Mesos.</dd>
</dl>
</div>
<div class="section" id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<p><em>To be filled in</em></p>
<p>Magnum rationale, concept, compelling features</p>
</div>
<div class="section" id="clustertemplate">
<h1>ClusterTemplate<a class="headerlink" href="#clustertemplate" title="Permalink to this headline">¶</a></h1>
<p>A ClusterTemplate (previously known as BayModel) is a collection of parameters
to describe how a cluster can be constructed.  Some parameters are relevant to
the infrastructure of the cluster, while others are for the particular COE.  In
a typical workflow, a user would create a ClusterTemplate, then create one or
more clusters using the ClusterTemplate.  A cloud provider can also define a
number of ClusterTemplates and provide them to the users.  A ClusterTemplate
cannot be updated or deleted if a cluster using this ClusterTemplate still
exists.</p>
<p>The definition and usage of the parameters of a ClusterTemplate are as follows.
They are loosely grouped as: mandatory, infrastructure, COE specific.</p>
<dl class="docutils">
<dt>&#8211;coe &lt;coe&gt;</dt>
<dd>Specify the Container Orchestration Engine to use.  Supported
COE&#8217;s include &#8216;kubernetes&#8217;, &#8216;swarm&#8217;, &#8216;mesos&#8217;.  If your environment
has additional cluster drivers installed, refer to the cluster driver
documentation for the new COE names.  This is a mandatory parameter
and there is no default value.</dd>
<dt>&#8211;image &lt;image&gt;</dt>
<dd><p class="first">The name or UUID of the base image in Glance to boot the servers for
the cluster.  The image must have the attribute &#8216;os-distro&#8217; defined
as appropriate for the cluster driver.  For the currently supported
images, the os-distro names are:</p>
<table border="1" class="docutils">
<colgroup>
<col width="32%" />
<col width="68%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">COE</th>
<th class="head">os-distro</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Kubernetes</td>
<td>Fedora-atomic, CoreOS</td>
</tr>
<tr class="row-odd"><td>Swarm</td>
<td>Fedora-atomic</td>
</tr>
<tr class="row-even"><td>Mesos</td>
<td>Ubuntu</td>
</tr>
</tbody>
</table>
<p class="last">This is a mandatory parameter and there is no default value.</p>
</dd>
<dt>&#8211;keypair &lt;keypair&gt;</dt>
<dd>The name or UUID of the SSH keypair to configure in the cluster servers
for ssh access.  You will need the key to be able to ssh to the
servers in the cluster.  The login name is specific to the cluster
driver. If keypair is not provided in template it will be required at
Cluster create. This value will be overridden by any keypair value that
is provided during Cluster create.</dd>
<dt>&#8211;external-network &lt;external-network&gt;</dt>
<dd>The name or network ID of a Neutron network to provide connectivity
to the external internet for the cluster.  This network must be an
external network, i.e. its attribute &#8216;router:external&#8217; must be
&#8216;True&#8217;.  The servers in the cluster will be connected to a private
network and Magnum will create a router between this private network
and the external network.  This will allow the servers to download
images, access discovery service, etc, and the containers to install
packages, etc.  In the opposite direction, floating IP&#8217;s will be
allocated from the external network to provide access from the
external internet to servers and the container services hosted in
the cluster.  This is a mandatory parameter and there is no default
value.</dd>
<dt>&#8211;name &lt;name&gt;</dt>
<dd>Name of the ClusterTemplate to create.  The name does not have to be
unique.  If multiple ClusterTemplates have the same name, you will need to
use the UUID to select the ClusterTemplate when creating a cluster or
updating, deleting a ClusterTemplate.  If a name is not specified, a random
name will be generated using a string and a number, for example
&#8220;pi-13-model&#8221;.</dd>
</dl>
<table class="docutils option-list" frame="void" rules="none">
<col class="option" />
<col class="description" />
<tbody valign="top">
<tr><td class="option-group">
<kbd><span class="option">--public</span></kbd></td>
<td>Access to a ClusterTemplate is normally limited to the admin, owner or users
within the same tenant as the owners.  Setting this flag
makes the ClusterTemplate public and accessible by other users.  The default
is not public.</td></tr>
</tbody>
</table>
<dl class="docutils">
<dt>&#8211;server-type &lt;server-type&gt;</dt>
<dd>The servers in the cluster can be VM or baremetal.  This parameter selects
the type of server to create for the cluster.  The default is &#8216;vm&#8217;. Possible
values are &#8216;vm&#8217;, &#8216;bm&#8217;.</dd>
<dt>&#8211;network-driver &lt;network-driver&gt;</dt>
<dd><p class="first">The name of a network driver for providing the networks for the
containers.  Note that this is different and separate from the Neutron
network for the cluster.  The operation and networking model are specific
to the particular driver; refer to the <a class="reference internal" href="#networking">Networking</a> section for more
details.  Supported network drivers and the default driver are:</p>
<table border="1" class="last docutils">
<colgroup>
<col width="31%" />
<col width="47%" />
<col width="22%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">COE</th>
<th class="head">Network-Driver</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Kubernetes</td>
<td>Flannel</td>
<td>Flannel</td>
</tr>
<tr class="row-odd"><td>Swarm</td>
<td>Docker, Flannel</td>
<td>Flannel</td>
</tr>
<tr class="row-even"><td>Mesos</td>
<td>Docker</td>
<td>Docker</td>
</tr>
</tbody>
</table>
</dd>
<dt>&#8211;volume-driver &lt;volume-driver&gt;</dt>
<dd><p class="first">The name of a volume driver for managing the persistent storage for
the containers.  The functionality supported are specific to the
driver.  Supported volume drivers and the default driver are:</p>
<table border="1" class="last docutils">
<colgroup>
<col width="35%" />
<col width="35%" />
<col width="30%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">COE</th>
<th class="head">Volume-Driver</th>
<th class="head">Default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Kubernetes</td>
<td>Cinder</td>
<td>No Driver</td>
</tr>
<tr class="row-odd"><td>Swarm</td>
<td>Rexray</td>
<td>No Driver</td>
</tr>
<tr class="row-even"><td>Mesos</td>
<td>Rexray</td>
<td>No Driver</td>
</tr>
</tbody>
</table>
</dd>
<dt>&#8211;dns-nameserver &lt;dns-nameserver&gt;</dt>
<dd>The DNS nameserver for the servers and containers in the cluster to use.
This is configured in the private Neutron network for the cluster.  The
default is &#8216;8.8.8.8&#8217;.</dd>
<dt>&#8211;flavor &lt;flavor&gt;</dt>
<dd>The nova flavor id for booting the node servers.  The default
is &#8216;m1.small&#8217;.</dd>
<dt>&#8211;master-flavor &lt;master-flavor&gt;</dt>
<dd>The nova flavor id for booting the master or manager servers.  The
default is &#8216;m1.small&#8217;.</dd>
<dt>&#8211;http-proxy &lt;http-proxy&gt;</dt>
<dd>The IP address for a proxy to use when direct http access from the
servers to sites on the external internet is blocked.  This may
happen in certain countries or enterprises, and the proxy allows the
servers and containers to access these sites.  The format is a URL
including a port number.  The default is &#8216;None&#8217;.</dd>
<dt>&#8211;https-proxy &lt;https-proxy&gt;</dt>
<dd>The IP address for a proxy to use when direct https access from the
servers to sites on the external internet is blocked.  This may
happen in certain countries or enterprises, and the proxy allows the
servers and containers to access these sites.  The format is a URL
including a port number.  The default is &#8216;None&#8217;.</dd>
<dt>&#8211;no-proxy &lt;no-proxy&gt;</dt>
<dd>When a proxy server is used, some sites should not go through the
proxy and should be accessed normally.  In this case, you can
specify these sites as a comma separated list of IP&#8217;s.  The default
is &#8216;None&#8217;.</dd>
<dt>&#8211;docker-volume-size &lt;docker-volume-size&gt;</dt>
<dd>If specified, container images will be stored in a cinder volume of the
specified size in GB. Each cluster node will have a volume attached of
the above size. If not specified, images will be stored in the compute
instance&#8217;s local disk. For the &#8216;devicemapper&#8217; storage driver, the minimum
value is 3GB. For the &#8216;overlay&#8217; storage driver, the minimum value is 1GB.</dd>
<dt>&#8211;docker-storage-driver &lt;docker-storage-driver&gt;</dt>
<dd>The name of a driver to manage the storage for the images and the
container&#8217;s writable layer.  The supported drivers are &#8216;devicemapper&#8217;
and &#8216;overlay&#8217;.  The default is &#8216;devicemapper&#8217;.</dd>
<dt>&#8211;labels &lt;KEY1=VALUE1,KEY2=VALUE2;KEY3=VALUE3...&gt;</dt>
<dd>Arbitrary labels in the form of key=value pairs.  The accepted keys
and valid values are defined in the cluster drivers.  They are used as a
way to pass additional parameters that are specific to a cluster driver.
Refer to the subsection on labels for a list of the supported
key/value pairs and their usage.</dd>
</dl>
<table class="docutils option-list" frame="void" rules="none">
<col class="option" />
<col class="description" />
<tbody valign="top">
<tr><td class="option-group">
<kbd><span class="option">--tls-disabled</span></kbd></td>
<td>Transport Layer Security (TLS) is normally enabled to secure the
cluster.  In some cases, users may want to disable TLS in the cluster,
for instance during development or to troubleshoot certain problems.
Specifying this parameter will disable TLS so that users can access
the COE endpoints without a certificate.  The default is TLS
enabled.</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--registry-enabled</span></kbd></td>
</tr>
<tr><td>&nbsp;</td><td>Docker images by default are pulled from the public Docker registry,
but in some cases, users may want to use a private registry.  This
option provides an alternative registry based on the Registry V2:
Magnum will create a local registry in the cluster backed by swift to
host the images.  Refer to
<a class="reference external" href="https://github.com/docker/distribution">Docker Registry 2.0</a>
for more details.  The default is to use the public registry.</td></tr>
<tr><td class="option-group" colspan="2">
<kbd><span class="option">--master-lb-enabled</span></kbd></td>
</tr>
<tr><td>&nbsp;</td><td>Since multiple masters may exist in a bay, a load balancer is
created to provide the API endpoint for the bay and to direct
requests to the masters.  In some cases, such as when the LBaaS
service is not available, this option can be set to &#8216;false&#8217; to
create a bay without the load balancer.  In this case, one of the
masters will serve as the API endpoint.  The default is &#8216;true&#8217;,
i.e. to create the load balancer for the bay.</td></tr>
</tbody>
</table>
<div class="section" id="labels">
<h2>Labels<a class="headerlink" href="#labels" title="Permalink to this headline">¶</a></h2>
<p>Labels is a general method to specify supplemental parameters that are
specific to certain COE or associated with certain options.  Their
format is key/value pair and their meaning is interpreted by the
drivers that uses them.  The drivers do validate the key/value pairs.
Their usage is explained in details in the appropriate sections,
however, since there are many possible labels, the following table
provides a summary to help give a clearer picture.  The label keys in
the table are linked to more details elsewhere in the user guide.</p>
<table border="1" class="docutils">
<colgroup>
<col width="53%" />
<col width="27%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">label key</th>
<th class="head">label value</th>
<th class="head">default</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference internal" href="#flannel-network-cidr">flannel_network_cidr</a></td>
<td>IPv4 CIDR</td>
<td>10.100.0.0/16</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#flannel-backend">flannel_backend</a></td>
<td><ul class="first last simple">
<li>udp</li>
<li>vxlan</li>
<li>host-gw</li>
</ul>
</td>
<td>udp</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#flannel-network-subnetlen">flannel_network_subnetlen</a></td>
<td>size of subnet to
assign to node</td>
<td>24</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#rexray-preempt">rexray_preempt</a></td>
<td><ul class="first last simple">
<li>true</li>
<li>false</li>
</ul>
</td>
<td>false</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mesos-slave-isolation">mesos_slave_isolation</a></td>
<td><ul class="first last simple">
<li>filesystem/posix</li>
<li>filesystem/linux</li>
<li>filesystem/shared</li>
<li>posix/cpu</li>
<li>posix/mem</li>
<li>posix/disk</li>
<li>cgroups/cpu</li>
<li>cgroups/mem</li>
<li>docker/runtime</li>
<li>namespaces/pid</li>
</ul>
</td>
<td>&#8220;&#8221;</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mesos-slave-image-providers">mesos_slave_image_providers</a></td>
<td><ul class="first last simple">
<li>appc</li>
<li>docker</li>
<li>appc,docker</li>
</ul>
</td>
<td>&#8220;&#8221;</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mesos-slave-work-dir">mesos_slave_work_dir</a></td>
<td>(directory name)</td>
<td>&#8220;&#8221;</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#mesos-slave-executor-env-variables">mesos_slave_executor_env_variables</a></td>
<td>(file name)</td>
<td>&#8220;&#8221;</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#swarm-strategy">swarm_strategy</a></td>
<td><ul class="first last simple">
<li>spread</li>
<li>binpack</li>
<li>random</li>
</ul>
</td>
<td>spread</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#admission-control-list">admission_control_list</a></td>
<td>see below</td>
<td>see below</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="cluster">
<h1>Cluster<a class="headerlink" href="#cluster" title="Permalink to this headline">¶</a></h1>
<p>A cluster (previously known as bay) is an instance of the ClusterTemplate
of a COE.  Magnum deploys a cluster by referring to the attributes
defined in the particular ClusterTemplate as well as a few additional
parameters for the cluster.  Magnum deploys the orchestration templates
provided by the cluster driver to create and configure all the necessary
infrastructure.  When ready, the cluster is a fully operational COE that
can host containers.</p>
<div class="section" id="infrastructure">
<h2>Infrastructure<a class="headerlink" href="#infrastructure" title="Permalink to this headline">¶</a></h2>
<p>The infrastructure of the cluster consists of the resources provided by
the various OpenStack services.  Existing infrastructure, including
infrastructure external to OpenStack, can also be used by the cluster,
such as DNS, public network, public discovery service, Docker registry.
The actual resources created depends on the COE type and the options
specified; therefore you need to refer to the cluster driver documentation
of the COE for specific details.  For instance, the option
&#8216;&#8211;master-lb-enabled&#8217; in the ClusterTemplate will cause a load balancer pool
along with the health monitor and floating IP to be created.  It is
important to distinguish resources in the IaaS level from resources in
the PaaS level.  For instance, the infrastructure networking in
OpenStack IaaS is different and separate from the container networking
in Kubernetes or Swarm PaaS.</p>
<p>Typical infrastructure includes the following.</p>
<dl class="docutils">
<dt>Servers</dt>
<dd>The servers host the containers in the cluster and these servers can be
VM or bare metal.  VM&#8217;s are provided by Nova.  Since multiple VM&#8217;s
are hosted on a physical server, the VM&#8217;s provide the isolation
needed for containers between different tenants running on the same
physical server.  Bare metal servers are provided by Ironic and are
used when peak performance with virtually no overhead is needed for
the containers.</dd>
<dt>Identity</dt>
<dd>Keystone provides the authentication and authorization for managing
the cluster infrastructure.</dd>
<dt>Network</dt>
<dd>Networking among the servers is provided by Neutron.  Since COE
currently are not multi-tenant, isolation for multi-tenancy on the
networking level is done by using a private network for each cluster.
As a result, containers belonging to one tenant will not be
accessible to containers or servers of another tenant.  Other
networking resources may also be used, such as load balancer and
routers.  Networking among containers can be provided by Kuryr if
needed.</dd>
<dt>Storage</dt>
<dd>Cinder provides the block storage that can be used to host the
containers and as persistent storage for the containers.</dd>
<dt>Security</dt>
<dd>Barbican provides the storage of secrets such as certificates used
for Transport Layer Security (TLS) within the cluster.</dd>
</dl>
</div>
<div class="section" id="life-cycle">
<h2>Life cycle<a class="headerlink" href="#life-cycle" title="Permalink to this headline">¶</a></h2>
<p>The set of life cycle operations on the cluster is one of the key value
that Magnum provides, enabling clusters to be managed painlessly on
OpenStack.  The current operations are the basic CRUD operations, but
more advanced operations are under discussion in the community and
will be implemented as needed.</p>
<p><strong>NOTE</strong> The OpenStack resources created for a cluster are fully
accessible to the cluster owner.  Care should be taken when modifying or
reusing these resources to avoid impacting Magnum operations in
unexpected manners.  For instance, if you launch your own Nova
instance on the bay private network, Magnum would not be aware of this
instance.  Therefore, the cluster-delete operation will fail because
Magnum would not delete the extra Nova instance and the private Neutron
network cannot be removed while a Nova instance is still attached.</p>
<p><strong>NOTE</strong> Currently Heat nested templates are used to create the
resources; therefore if an error occurs, you can troubleshoot through
Heat.  For more help on Heat stack troubleshooting, refer to the
<a class="reference external" href="https://github.com/openstack/magnum/blob/master/doc/source/troubleshooting-guide.rst#heat-stacks">Troubleshooting Guide</a>.</p>
<div class="section" id="create">
<h3>Create<a class="headerlink" href="#create" title="Permalink to this headline">¶</a></h3>
<p><strong>NOTE</strong> bay-&lt;command&gt; are the deprecated versions of these commands and are
still support in current release. They will be removed in a future version.
Any references to the term bay will be replaced in the parameters when using
the &#8216;bay&#8217; versions of the commands. For example, in &#8216;bay-create&#8217; &#8211;baymodel
is used as the baymodel parameter for this command instead of
&#8211;cluster-template.</p>
<p>The &#8216;cluster-create&#8217; command deploys a cluster, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-create --name mycluster \
                  --cluster-template mytemplate \
                  --node-count 8 \
                  --master-count 3
</pre></div>
</div>
<p>The &#8216;cluster-create&#8217; operation is asynchronous; therefore you can initiate
another &#8216;cluster-create&#8217; operation while the current cluster is being created.
If the cluster fails to be created, the infrastructure created so far may
be retained or deleted depending on the particular orchestration
engine.  As a common practice, a failed cluster is retained during
development for troubleshooting, but they are automatically deleted in
production.  The current cluster drivers use Heat templates and the
resources of a failed &#8216;cluster-create&#8217; are retained.</p>
<p>The definition and usage of the parameters for &#8216;cluster-create&#8217; are as
follows:</p>
<dl class="docutils">
<dt>&#8211;cluster-template &lt;cluster-template&gt;</dt>
<dd>The ID or name of the ClusterTemplate to use.  This is a mandatory
parameter.  Once a ClusterTemplate is used to create a cluster, it cannot
be deleted or modified until all clusters that use the ClusterTemplate have
been deleted.</dd>
<dt>&#8211;name &lt;name&gt;</dt>
<dd>Name of the cluster to create.  If a name is not specified, a random
name will be generated using a string and a number, for example
&#8220;gamma-7-cluster&#8221;.</dd>
<dt>&#8211;keypair &lt;keypair&gt;</dt>
<dd>The name or UUID of the SSH keypair to configure in the cluster servers
for ssh access.  You will need the key to be able to ssh to the
servers in the cluster.  The login name is specific to the cluster
driver. If keypair is not provided it will attempt to use the value in
the ClusterTemplate. If the ClusterTemplate is also missing a keypair value
then an error will be returned.  The keypair value provided here will
override the keypair value from the ClusterTemplate.</dd>
<dt>&#8211;node-count &lt;node-count&gt;</dt>
<dd>The number of servers that will serve as node in the cluster.
The default is 1.</dd>
<dt>&#8211;master-count &lt;master-count&gt;</dt>
<dd>The number of servers that will serve as master for the cluster.
The default is 1.  Set to more than 1 master to enable High
Availability.  If the option &#8216;&#8211;master-lb-enabled&#8217; is specified in
the ClusterTemplate, the master servers will be placed in a load balancer
pool.</dd>
<dt>&#8211;discovery-url &lt;discovery-url&gt;</dt>
<dd><p class="first">The custom discovery url for node discovery.  This is used by the
COE to discover the servers that have been created to host the
containers.  The actual discovery mechanism varies with the COE.  In
some cases, Magnum fills in the server info in the discovery
service.  In other cases, if the discovery-url is not specified,
Magnum will use the public discovery service at:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>https://discovery.etcd.io
</pre></div>
</div>
<p class="last">In this case, Magnum will generate a unique url here for each cluster
and store the info for the servers.</p>
</dd>
<dt>&#8211;timeout &lt;timeout&gt;</dt>
<dd>The timeout for cluster creation in minutes. The value expected is a
positive integer and the default is 60 minutes.  If the timeout is
reached during cluster-create, the operation will be aborted and the
cluster status will be set to &#8216;CREATE_FAILED&#8217;.</dd>
</dl>
</div>
<div class="section" id="list">
<h3>List<a class="headerlink" href="#list" title="Permalink to this headline">¶</a></h3>
<p>The &#8216;cluster-list&#8217; command lists all the clusters that belong to the tenant,
for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-list
</pre></div>
</div>
</div>
<div class="section" id="show">
<h3>Show<a class="headerlink" href="#show" title="Permalink to this headline">¶</a></h3>
<p>The &#8216;cluster-show&#8217; command prints all the details of a cluster, for
example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-show mycluster
</pre></div>
</div>
<p>The properties include those not specified by users that have been
assigned default values and properties from new resources that
have been created for the cluster.</p>
</div>
<div class="section" id="update">
<h3>Update<a class="headerlink" href="#update" title="Permalink to this headline">¶</a></h3>
<p>A cluster can be modified using the &#8216;cluster-update&#8217; command, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-update mycluster replace node_count=8
</pre></div>
</div>
<p>The parameters are positional and their definition and usage are as
follows.</p>
<dl class="docutils">
<dt>&lt;cluster&gt;</dt>
<dd>This is the first parameter, specifying the UUID or name of the cluster
to update.</dd>
<dt>&lt;op&gt;</dt>
<dd>This is the second parameter, specifying the desired change to be
made to the cluster attributes.  The allowed changes are &#8216;add&#8217;,
&#8216;replace&#8217; and &#8216;remove&#8217;.</dd>
<dt>&lt;attribute=value&gt;</dt>
<dd><p class="first">This is the third parameter, specifying the targeted attributes in
the cluster as a list separated by blank space.  To add or replace an
attribute, you need to specify the value for the attribute.  To
remove an attribute, you only need to specify the name of the
attribute.  Currently the only attribute that can be replaced or
removed is &#8216;node_count&#8217;.  The attributes &#8216;name&#8217;, &#8216;master_count&#8217; and
&#8216;discovery_url&#8217; cannot be replaced or delete.  The table below
summarizes the possible change to a cluster.</p>
<table border="1" class="last docutils">
<colgroup>
<col width="25%" />
<col width="8%" />
<col width="30%" />
<col width="38%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Attribute</th>
<th class="head">add</th>
<th class="head">replace</th>
<th class="head">remove</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>node_count</td>
<td>no</td>
<td>add/remove nodes</td>
<td>reset to default of 1</td>
</tr>
<tr class="row-odd"><td>master_count</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
<tr class="row-even"><td>name</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
<tr class="row-odd"><td>discovery_url</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p>The &#8216;cluster-update&#8217; operation cannot be initiated when another operation
is in progress.</p>
<p><strong>NOTE:</strong> The attribute names in cluster-update are slightly different
from the corresponding names in the cluster-create command: the dash &#8216;-&#8216;
is replaced by an underscore &#8216;_&#8217;.  For instance, &#8216;node-count&#8217; in
cluster-create is &#8216;node_count&#8217; in cluster-update.</p>
</div>
<div class="section" id="scale">
<h3>Scale<a class="headerlink" href="#scale" title="Permalink to this headline">¶</a></h3>
<p>Scaling a cluster means adding servers to or removing servers from the cluster.
Currently, this is done through the &#8216;cluster-update&#8217; operation by modifying
the node-count attribute, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-update mycluster replace node_count=2
</pre></div>
</div>
<p>When some nodes are removed, Magnum will attempt to find nodes with no
containers to remove.  If some nodes with containers must be removed,
Magnum will log a warning message.</p>
</div>
<div class="section" id="delete">
<h3>Delete<a class="headerlink" href="#delete" title="Permalink to this headline">¶</a></h3>
<p>The &#8216;cluster-delete&#8217; operation removes the cluster by deleting all resources
such as servers, network, storage;  for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-delete mycluster
</pre></div>
</div>
<p>The only parameter for the cluster-delete command is the ID or name of the
cluster to delete.  Multiple clusters can be specified, separated by a blank
space.</p>
<p>If the operation fails, there may be some remaining resources that
have not been deleted yet.  In this case, you can troubleshoot through
Heat.  If the templates are deleted manually in Heat, you can delete
the cluster in Magnum to clean up the cluster from Magnum database.</p>
<p>The &#8216;cluster-delete&#8217; operation can be initiated when another operation is
still in progress.</p>
</div>
</div>
</div>
<div class="section" id="python-client">
<h1>Python Client<a class="headerlink" href="#python-client" title="Permalink to this headline">¶</a></h1>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Follow the instructions in the OpenStack Installation Guide to enable the
repositories for your distribution:</p>
<ul class="simple">
<li><a class="reference external" href="http://docs.openstack.org/liberty/install-guide-rdo/">RHEL/CentOS/Fedora</a></li>
<li><a class="reference external" href="http://docs.openstack.org/liberty/install-guide-ubuntu/">Ubuntu/Debian</a></li>
<li><a class="reference external" href="http://docs.openstack.org/liberty/install-guide-obs/">openSUSE/SUSE Linux Enterprise</a></li>
</ul>
<p>Install using distribution packages for RHEL/CentOS/Fedora:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ sudo yum install python-magnumclient
</pre></div>
</div>
<p>Install using distribution packages for Ubuntu/Debian:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ sudo apt-get install python-magnumclient
</pre></div>
</div>
<p>Install using distribution packages for OpenSuSE and SuSE Enterprise Linux:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ sudo zypper install python-magnumclient
</pre></div>
</div>
</div>
<div class="section" id="verifying-installation">
<h2>Verifying installation<a class="headerlink" href="#verifying-installation" title="Permalink to this headline">¶</a></h2>
<p>Execute the <cite>magnum</cite> command with the <cite>&#8211;version</cite> argument to confirm that the
client is installed and in the system path:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ magnum --version
1.1.0
</pre></div>
</div>
<p>Note that the version returned may differ from the above, 1.1.0 was the latest
available version at the time of writing.</p>
</div>
<div class="section" id="using-the-command-line-client">
<h2>Using the command-line client<a class="headerlink" href="#using-the-command-line-client" title="Permalink to this headline">¶</a></h2>
<p>Refer to the <a class="reference external" href="http://docs.openstack.org/cli-reference/magnum.html">OpenStack Command-Line Interface Reference</a> for a full list of the
commands supported by the <cite>magnum</cite> command-line client.</p>
</div>
</div>
<div class="section" id="horizon-interface">
<h1>Horizon Interface<a class="headerlink" href="#horizon-interface" title="Permalink to this headline">¶</a></h1>
<p>Magnum provides a Horizon plugin so that users can access the Container
Infrastructure Management service through the OpenStack browser-based
graphical UI.  The plugin is available from
<a class="reference external" href="https://github.com/openstack/magnum-ui">magnum-ui</a>.  It is not
installed by default in the standard Horizon service, but you can
follow the instruction for <a class="reference external" href="http://docs.openstack.org/developer/horizon/tutorials/plugin.html#installing-your-plugin">installing a Horizon plugin</a>.</p>
<p>In Horizon, the container infrastructure panel is part of the
&#8216;Project&#8217; view and it currently supports the following operations:</p>
<ul class="simple">
<li>View list of cluster templates</li>
<li>View details of a cluster template</li>
<li>Create a cluster template</li>
<li>Delete a cluster template</li>
<li>View list of clusters</li>
<li>View details of a cluster</li>
<li>Create a cluster</li>
<li>Delete a cluster</li>
<li>Get the Certificate Authority for a cluster</li>
<li>Sign a user key and obtain a signed certificate for accessing the secured
COE API endpoint in a cluster.</li>
</ul>
<p>Other operations are not yet supported and the CLI should be used for these.</p>
<p>Following is the screenshot of the Horizon view showing the list of cluster
templates.</p>
<img alt="_images/cluster-template.png" src="_images/cluster-template.png" />
<p>Following is the screenshot of the Horizon view showing the details of a
cluster template.</p>
<img alt="_images/cluster-template-details.png" src="_images/cluster-template-details.png" />
<p>Following is the screenshot of the dialog to create a new cluster.</p>
<img alt="_images/cluster-create.png" src="_images/cluster-create.png" />
</div>
<div class="section" id="cluster-drivers">
<h1>Cluster Drivers<a class="headerlink" href="#cluster-drivers" title="Permalink to this headline">¶</a></h1>
<p>A cluster driver is a collection of python code, heat templates, scripts,
images, and documents for a particular COE on a particular
distro.  Magnum presents the concept of ClusterTemplates and clusters.  The
implementation for a particular cluster type is provided by the cluster driver.
In other words, the cluster driver provisions and manages the infrastructure
for the COE.  Magnum includes default drivers for the following
COE and distro pairs:</p>
<table border="1" class="docutils">
<colgroup>
<col width="44%" />
<col width="56%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">COE</th>
<th class="head">distro</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Kubernetes</td>
<td>Fedora Atomic</td>
</tr>
<tr class="row-odd"><td>Kubernetes</td>
<td>CoreOS</td>
</tr>
<tr class="row-even"><td>Swarm</td>
<td>Fedora Atomic</td>
</tr>
<tr class="row-odd"><td>Mesos</td>
<td>Ubuntu</td>
</tr>
</tbody>
</table>
<p>Magnum is designed to accommodate new cluster drivers to support custom
COE&#8217;s and this section describes how a new cluster driver can be
constructed and enabled in Magnum.</p>
<div class="section" id="directory-structure">
<h2>Directory structure<a class="headerlink" href="#directory-structure" title="Permalink to this headline">¶</a></h2>
<p>Magnum expects the components to be organized in the following
directory structure under the directory &#8216;drivers&#8217;:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>COE_Distro/
   image/
   templates/
   api.py
   driver.py
   monitor.py
   scale.py
   template_def.py
   version.py
</pre></div>
</div>
<p>The minimum required components are:</p>
<dl class="docutils">
<dt>driver.py</dt>
<dd>Python code that implements the controller operations for
the particular COE.  The driver must implement:
Currently supported:
<code class="docutils literal"><span class="pre">cluster_create</span></code>, <code class="docutils literal"><span class="pre">cluster_update</span></code>, <code class="docutils literal"><span class="pre">cluster_delete</span></code>.</dd>
<dt>templates</dt>
<dd>A directory of orchestration templates for managing the lifecycle
of clusters, including creation, configuration, update, and deletion.
Currently only Heat templates are supported, but in the future
other orchestration mechanism such as Ansible may be supported.</dd>
<dt>template_def.py</dt>
<dd>Python code that maps the parameters from the ClusterTemplate to the
input parameters for the orchestration and invokes
the orchestration in the templates directory.</dd>
<dt>version.py</dt>
<dd>Tracks the latest version of the driver in this directory.
This is defined by a <code class="docutils literal"><span class="pre">version</span></code> attribute and is represented in the
form of <code class="docutils literal"><span class="pre">1.0.0</span></code>. It should also include a <code class="docutils literal"><span class="pre">Driver</span></code> attribute with
descriptive name such as <code class="docutils literal"><span class="pre">fedora_swarm_atomic</span></code>.</dd>
</dl>
<p>The remaining components are optional:</p>
<dl class="docutils">
<dt>image</dt>
<dd>Instructions for obtaining or building an image suitable for the COE.</dd>
<dt>api.py</dt>
<dd>Python code to interface with the COE.</dd>
<dt>monitor.py</dt>
<dd>Python code to monitor the resource utilization of the cluster.</dd>
<dt>scale.py</dt>
<dd>Python code to scale the cluster by adding or removing nodes.</dd>
</dl>
</div>
<div class="section" id="sample-cluster-driver">
<h2>Sample cluster driver<a class="headerlink" href="#sample-cluster-driver" title="Permalink to this headline">¶</a></h2>
<p>To help developers in creating new COE drivers, a minimal cluster driver
is provided as an example.  The &#8216;docker&#8217; cluster driver will simply deploy
a single VM running Ubuntu with the latest Docker version installed.
It is not a true cluster, but the simplicity will help to illustrate
the key concepts.</p>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="installing-a-cluster-driver">
<h2>Installing a cluster driver<a class="headerlink" href="#installing-a-cluster-driver" title="Permalink to this headline">¶</a></h2>
<p><em>To be filled in</em></p>
</div>
</div>
<div class="section" id="choosing-a-coe">
<h1>Choosing a COE<a class="headerlink" href="#choosing-a-coe" title="Permalink to this headline">¶</a></h1>
<p>Magnum supports a variety of COE options, and allows more to be added over time
as they gain popularity. As an operator, you may choose to support the full
variety of options, or you may want to offer a subset of the available choices.
Given multiple choices, your users can run one or more clusters, and each may
use a different COE. For example, I might have multiple clusters that use
Kubernetes, and just one cluster that uses Swarm. All of these clusters can
run concurrently, even though they use different COE software.</p>
<p>Choosing which COE to use depends on what tools you want to use to manage your
containers once you start your app. If you want to use the Docker tools, you
may want to use the Swarm cluster type. Swarm will spread your containers
across the various nodes in your cluster automatically. It does not monitor
the health of your containers, so it can&#8217;t restart them for you if they stop.
It will not automatically scale your app for you (as of Swarm version 1.2.2).
You may view this as a plus. If you prefer to manage your application yourself,
you might prefer swarm over the other COE options.</p>
<p>Kubernetes (as of v1.2) is more sophisticated than Swarm (as of v1.2.2). It
offers an attractive YAML file description of a pod, which is a grouping of
containers that run together as part of a distributed application. This file
format allows you to model your application deployment using a declarative
style. It has support for auto scaling and fault recovery, as well as features
that allow for sophisticated software deployments, including canary deploys
and blue/green deploys. Kubernetes is very popular, especially for web
applications.</p>
<p>Apache Mesos is a COE that has been around longer than Kubernetes or Swarm. It
allows for a variety of different frameworks to be used along with it,
including Marathon, Aurora, Chronos, Hadoop, and <a class="reference external" href="http://mesos.apache.org/documentation/latest/frameworks/">a number of others.</a></p>
<p>The Apache Mesos framework design can be used to run alternate COE software
directly on Mesos. Although this approach is not widely used yet, it may soon
be possible to run Mesos with Kubernetes and Swarm as frameworks, allowing
you to share the resources of a cluster between multiple different COEs. Until
this option matures, we encourage Magnum users to create multiple clusters, and
use the COE in each cluster that best fits the anticipated workload.</p>
<p>Finding the right COE for your workload is up to you, but Magnum offers you a
choice to select among the prevailing leading options. Once you decide, see
the next sections for examples of how to create a cluster with your desired
COE.</p>
</div>
<div class="section" id="native-clients">
<h1>Native Clients<a class="headerlink" href="#native-clients" title="Permalink to this headline">¶</a></h1>
<p>Magnum preserves the native user experience with a COE and does not
provide a separate API or client.  This means you will need to use the
native client for the particular cluster type to interface with the
clusters.  In the typical case, there are two clients to consider:</p>
<dl class="docutils">
<dt>COE level</dt>
<dd>This is the orchestration or management level such as Kubernetes,
Swarm, Mesos and its frameworks.</dd>
<dt>Container level</dt>
<dd>This is the low level container operation.  Currently it is
Docker for all clusters.</dd>
</dl>
<p>The clients can be CLI and/or browser-based.  You will need to refer
to the documentation for the specific native client and appropriate
version for details, but following are some pointers for reference.</p>
<p>Kubernetes CLI is the tool &#8216;kubectl&#8217;, which can be simply copied from
a node in the cluster or downloaded from the Kubernetes release.  For
instance, if the cluster is running Kubernetes release 1.2.0, the
binary for &#8216;kubectl&#8217; can be downloaded as and set up locally as
follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>curl -O https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl
</pre></div>
</div>
<p>Kubernetes also provides a browser UI if the cluster has the
Kubernetes UI running; it can be accessed at:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>http://&lt;api_address&gt;/UI
where the api_address can obtained from the command &#39;cluster-show&#39;.
</pre></div>
</div>
<p>For Swarm, the main CLI is &#8216;docker&#8217;, along with associated tools
such as &#8216;docker-compose&#8217;, etc.  Specific version of the binaries can
be obtained from the <a class="reference external" href="https://docs.docker.com/engine/installation/binaries/">Docker Engine installation</a>.</p>
<p>Mesos cluster uses the Marathon framework and details on the Marathon
UI can be found in the section <a class="reference internal" href="#using-marathon">Using Marathon</a>.</p>
<p>Depending on the client requirement, you may need to use a version of
the client that matches the version in the cluster.  To determine the
version of the COE and container, use the command &#8216;cluster-show&#8217; and
look for the attribute <em>coe_version</em> and <em>container_version</em>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-show k8s-cluster
+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_COMPLETE                                            |
| uuid               | 04952c60-a338-437f-a7e7-d016d1d00e65                       |
| stack_id           | b7bf72ce-b08e-4768-8201-e63a99346898                       |
| status_reason      | Stack CREATE completed successfully                        |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | 2016-07-25T23:14:10+00:00                                  |
| create_timeout     | 60                                                         |
| coe_version        | v1.2.0                                                     |
| api_address        | https://192.168.19.86:6443                                 |
| cluster_template_id| da2825a0-6d09-4208-b39e-b2db666f1118                       |
| master_addresses   | [&#39;192.168.19.87&#39;]                                          |
| node_count         | 1                                                          |
| node_addresses     | [&#39;192.168.19.88&#39;]                                          |
| master_count       | 1                                                          |
| container_version  | 1.9.1                                                      |
| discovery_url      | https://discovery.etcd.io/3b7fb09733429d16679484673ba3bfd5 |
| name               | k8s-cluster                                                |
+--------------------+------------------------------------------------------------+
</pre></div>
</div>
</div>
<div class="section" id="kubernetes">
<h1>Kubernetes<a class="headerlink" href="#kubernetes" title="Permalink to this headline">¶</a></h1>
<p>Kubernetes uses a range of terminology that we refer to in this guide. We
define these common terms for your reference:</p>
<dl class="docutils">
<dt>Pod</dt>
<dd>When using the Kubernetes container orchestration engine, a pod is the
smallest deployable unit that can be created and managed. A pod is a
co-located group of application containers that run with a shared context.
When using Magnum, pods are created and managed within clusters. Refer to the
<a class="reference external" href="http://kubernetes.io/v1.0/docs/user-guide/pods.html">pods section</a> in the <a class="reference external" href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes
User Guide</a> for more information.</dd>
<dt>Replication controller</dt>
<dd>A replication controller is used to ensure that at any given time a certain
number of replicas of a pod are running. Pods are automatically created and
deleted by the replication controller as necessary based on a template to
ensure that the defined number of replicas exist. Refer to the <a class="reference external" href="http://kubernetes.io/v1.0/docs/user-guide/replication-controller.html">replication
controller section</a> in
the <a class="reference external" href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes User Guide</a> for more information.</dd>
<dt>Service</dt>
<dd>A service is an additional layer of abstraction provided by the Kubernetes
container orchestration engine which defines a logical set of pods and a
policy for accessing them. This is useful because pods are created and
deleted by a replication controller, for example, other pods needing to
discover them can do so via the service abstraction. Refer to the
<a class="reference external" href="http://kubernetes.io/v1.0/docs/user-guide/services.html">services section</a> in the
<a class="reference external" href="http://kubernetes.io/v1.0/docs/user-guide/">Kubernetes User Guide</a> for more information.</dd>
</dl>
<p>When Magnum deploys a Kubernetes cluster, it uses parameters defined in the
ClusterTemplate and specified on the cluster-create command, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-template-create --name k8s-cluster-template \
                           --image fedora-atomic-latest \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --network-driver flannel \
                           --coe kubernetes

magnum cluster-create --name k8s-cluster \
                      --cluster-template k8s-cluster-template \
                      --master-count 3 \
                      --node-count 8
</pre></div>
</div>
<p>Refer to the <a class="reference internal" href="#clustertemplate">ClusterTemplate</a> and <a class="reference internal" href="#cluster">Cluster</a> sections for the full list of
parameters. Following are further details relevant to a Kubernetes cluster:</p>
<dl class="docutils">
<dt>Number of masters (master-count)</dt>
<dd>Specified in the cluster-create command to indicate how many servers will
run as master in the cluster.  Having more than one will provide high
availability.  The masters will be in a load balancer pool and the
virtual IP address (VIP) of the load balancer will serve as the
Kubernetes API endpoint.  For external access, a floating IP
associated with this VIP is available and this is the endpoint
shown for Kubernetes in the &#8216;cluster-show&#8217; command.</dd>
<dt>Number of nodes (node-count)</dt>
<dd>Specified in the cluster-create command to indicate how many servers will
run as node in the cluster to host the users&#8217; pods.  The nodes are registered
in Kubernetes using the Nova instance name.</dd>
<dt>Network driver (network-driver)</dt>
<dd>Specified in the ClusterTemplate to select the network driver.
The supported and default network driver is &#8216;flannel&#8217;, an overlay
network providing a flat network for all pods.  Refer to the
<a class="reference internal" href="#networking">Networking</a> section for more details.</dd>
<dt>Volume driver (volume-driver)</dt>
<dd>Specified in the ClusterTemplate to select the volume driver.  The supported
volume driver is &#8216;cinder&#8217;, allowing Cinder volumes to be mounted in
containers for use as persistent storage.  Data written to these volumes
will persist after the container exits and can be accessed again from other
containers, while data written to the union file system hosting the container
will be deleted.  Refer to the <a class="reference internal" href="#storage">Storage</a> section for more details.</dd>
<dt>Storage driver (docker-storage-driver)</dt>
<dd>Specified in the ClusterTemplate to select the Docker storage driver.  The
supported storage drivers are &#8216;devicemapper&#8217; and &#8216;overlay&#8217;, with
&#8216;devicemapper&#8217; being the default. Refer to the <a class="reference internal" href="#storage">Storage</a> section for more
details.</dd>
<dt>Image (image)</dt>
<dd>Specified in the ClusterTemplate to indicate the image to boot the servers.
The image binary is loaded in Glance with the attribute
&#8216;os_distro = fedora-atomic&#8217;.
Current supported images are Fedora Atomic (download from <a class="reference external" href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images">Fedora</a> )
and CoreOS (download from <a class="reference external" href="http://beta.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2">CoreOS</a> )</dd>
<dt>TLS (tls-disabled)</dt>
<dd>Transport Layer Security is enabled by default, so you need a key and
signed certificate to access the Kubernetes API and CLI.  Magnum
handles its own key and certificate when interfacing with the
Kubernetes cluster.  In development mode, TLS can be disabled.  Refer to
the &#8216;Transport Layer Security&#8217;_ section for more details.</dd>
<dt>What runs on the servers</dt>
<dd>The servers for Kubernetes master host containers in the &#8216;kube-system&#8217;
name space to run the Kubernetes proxy, scheduler and controller manager.
The masters will not host users&#8217; pods.  Kubernetes API server, docker
daemon, etcd and flannel run as systemd services.  The servers for
Kubernetes node also host a container in the &#8216;kube-system&#8217; name space
to run the Kubernetes proxy, while Kubernetes kubelet, docker daemon
and flannel run as systemd services.</dd>
<dt>Log into the servers</dt>
<dd>You can log into the master servers using the login &#8216;fedora&#8217; and the
keypair specified in the ClusterTemplate.</dd>
</dl>
<p>In addition to the common attributes in the ClusterTemplate, you can specify
the following attributes that are specific to Kubernetes by using the
labels attribute.</p>
<dl class="docutils">
<dt><span class="target" id="admission-control-list">admission_control_list</span></dt>
<dd>This label corresponds to Kubernetes parameter for the API server &#8216;&#8211;admission-control&#8217;.
For more details, refer to the <a class="reference external" href="https://kubernetes.io/docs/admin/admission-controllers//">Admission Controllers</a>.
The default value corresponds to the one recommended in this doc
for our current Kubernetes version.</dd>
</dl>
<div class="section" id="external-load-balancer-for-services">
<h2>External load balancer for services<a class="headerlink" href="#external-load-balancer-for-services" title="Permalink to this headline">¶</a></h2>
<p>All Kubernetes pods and services created in the cluster are assigned IP
addresses on a private container network so they can access each other
and the external internet.  However, these IP addresses are not
accessible from an external network.</p>
<p>To publish a service endpoint externally so that the service can be
accessed from the external network, Kubernetes provides the external
load balancer feature.  This is done by simply specifying in the
service manifest the attribute &#8220;type: LoadBalancer&#8221;.  Magnum enables
and configures the Kubernetes plugin for OpenStack so that it can
interface with Neutron and manage the necessary networking resources.</p>
<p>When the service is created, Kubernetes will add an external load
balancer in front of the service so that the service will have an
external IP address in addition to the internal IP address on the
container network.  The service endpoint can then be accessed with
this external IP address.  Kubernetes handles all the life cycle
operations when pods are modified behind the service and when the
service is deleted.</p>
<p>Refer to the document <a class="reference external" href="https://github.com/openstack/magnum/blob/master/doc/source/dev/kubernetes-load-balancer.rst">Kubernetes external load balancer</a>
for more details.</p>
</div>
</div>
<div class="section" id="swarm">
<h1>Swarm<a class="headerlink" href="#swarm" title="Permalink to this headline">¶</a></h1>
<p>A Swarm cluster is a pool of servers running Docker daemon that is
managed as a single Docker host.  One or more Swarm managers accepts
the standard Docker API and manage this pool of servers.
Magnum deploys a Swarm cluster using parameters defined in
the ClusterTemplate and specified on the &#8216;cluster-create&#8217; command, for
example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-template-create --name swarm-cluster-template \
                           --image fedora-atomic-latest \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --coe swarm

magnum cluster-create --name swarm-cluster \
                  --cluster-template swarm-cluster-template \
                  --master-count 3 \
                  --node-count 8
</pre></div>
</div>
<p>Refer to the <a class="reference internal" href="#clustertemplate">ClusterTemplate</a> and <a class="reference internal" href="#cluster">Cluster</a> sections for the full list of
parameters. Following are further details relevant to Swarm:</p>
<dl class="docutils">
<dt>What runs on the servers</dt>
<dd>There are two types of servers in the Swarm cluster: managers and nodes.
The Docker daemon runs on all servers.  On the servers for manager,
the Swarm manager is run as a Docker container on port 2376 and this
is initiated by the systemd service swarm-manager.  Etcd is also run
on the manager servers for discovery of the node servers in the cluster.
On the servers for node, the Swarm agent is run as a Docker
container on port 2375 and this is initiated by the systemd service
swarm-agent.  On start up, the agents will register themselves in
etcd and the managers will discover the new node to manage.</dd>
<dt>Number of managers (master-count)</dt>
<dd>Specified in the cluster-create command to indicate how many servers will
run as managers in the cluster.  Having more than one will provide high
availability.  The managers will be in a load balancer pool and the
load balancer virtual IP address (VIP) will serve as the Swarm API
endpoint.  A floating IP associated with the load balancer VIP will
serve as the external Swarm API endpoint.  The managers accept
the standard Docker API and perform the corresponding operation on the
servers in the pool.  For instance, when a new container is created,
the managers will select one of the servers based on some strategy
and schedule the containers there.</dd>
<dt>Number of nodes (node-count)</dt>
<dd>Specified in the cluster-create command to indicate how many servers will
run as nodes in the cluster to host your Docker containers.  These servers
will register themselves in etcd for discovery by the managers, and
interact with the managers.  Docker daemon is run locally to host
containers from users.</dd>
<dt>Network driver (network-driver)</dt>
<dd>Specified in the ClusterTemplate to select the network driver.  The supported
drivers are &#8216;docker&#8217; and &#8216;flannel&#8217;, with &#8216;docker&#8217; as the default.
With the &#8216;docker&#8217; driver, containers are connected to the &#8216;docker0&#8217;
bridge on each node and are assigned local IP address.  With the
&#8216;flannel&#8217; driver, containers are connected to a flat overlay network
and are assigned IP address by Flannel.  Refer to the <a class="reference internal" href="#networking">Networking</a>
section for more details.</dd>
<dt>Volume driver (volume-driver)</dt>
<dd>Specified in the ClusterTemplate to select the volume driver to provide
persistent storage for containers.  The supported volume driver is
&#8216;rexray&#8217;.  The default is no volume driver.  When &#8216;rexray&#8217; or other
volume driver is deployed, you can use the Docker &#8216;volume&#8217; command to
create, mount, unmount, delete volumes in containers.  Cinder block
storage is used as the backend to support this feature.
Refer to the <a class="reference internal" href="#storage">Storage</a> section for more details.</dd>
<dt>Storage driver (docker-storage-driver)</dt>
<dd>Specified in the ClusterTemplate to select the Docker storage driver.  The
supported storage driver are &#8216;devicemapper&#8217; and &#8216;overlay&#8217;, with
&#8216;devicemapper&#8217; being the default. Refer to the <a class="reference internal" href="#storage">Storage</a> section for more
details.</dd>
<dt>Image (image)</dt>
<dd>Specified in the ClusterTemplate to indicate the image to boot the servers
for the Swarm manager and node.
The image binary is loaded in Glance with the attribute
&#8216;os_distro = fedora-atomic&#8217;.
Current supported image is Fedora Atomic (download from <a class="reference external" href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images">Fedora</a> )</dd>
<dt>TLS (tls-disabled)</dt>
<dd>Transport Layer Security is enabled by default to secure the Swarm API for
access by both the users and Magnum.  You will need a key and a
signed certificate to access the Swarm API and CLI.  Magnum
handles its own key and certificate when interfacing with the
Swarm cluster.  In development mode, TLS can be disabled.  Refer to
the &#8216;Transport Layer Security&#8217;_ section for details on how to create your
key and have Magnum sign your certificate.</dd>
<dt>Log into the servers</dt>
<dd>You can log into the manager and node servers with the account &#8216;fedora&#8217; and
the keypair specified in the ClusterTemplate.</dd>
</dl>
<p>In addition to the common attributes in the ClusterTemplate, you can specify
the following attributes that are specific to Swarm by using the
labels attribute.</p>
<dl class="docutils">
<dt><span class="target" id="swarm-strategy">swarm_strategy</span></dt>
<dd><p class="first">This label corresponds to Swarm parameter for master &#8216;&#8211;strategy&#8217;.
For more details, refer to the <a class="reference external" href="https://docs.docker.com/swarm/scheduler/strategy/">Swarm Strategy</a>.
Valid values for this label are:</p>
<ul class="last simple">
<li>spread</li>
<li>binpack</li>
<li>random</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="mesos">
<h1>Mesos<a class="headerlink" href="#mesos" title="Permalink to this headline">¶</a></h1>
<p>A Mesos cluster consists of a pool of servers running as Mesos slaves,
managed by a set of servers running as Mesos masters.  Mesos manages
the resources from the slaves but does not itself deploy containers.
Instead, one of more Mesos frameworks running on the Mesos cluster would
accept user requests on their own endpoint, using their particular
API.  These frameworks would then negotiate the resources with Mesos
and the containers are deployed on the servers where the resources are
offered.</p>
<p>Magnum deploys a Mesos cluster using parameters defined in the ClusterTemplate
and specified on the &#8216;cluster-create&#8217; command, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-template-create --name mesos-cluster-template \
                       --image ubuntu-mesos \
                       --keypair testkey \
                       --external-network public \
                       --dns-nameserver 8.8.8.8 \
                       --flavor m1.small \
                       --coe mesos

magnum cluster-create --name mesos-cluster \
                  --cluster-template mesos-cluster-template \
                  --master-count 3 \
                  --node-count 8
</pre></div>
</div>
<p>Refer to the <a class="reference internal" href="#clustertemplate">ClusterTemplate</a> and <a class="reference internal" href="#cluster">Cluster</a> sections for the full list of
parameters.  Following are further details relevant to Mesos:</p>
<dl class="docutils">
<dt>What runs on the servers</dt>
<dd>There are two types of servers in the Mesos cluster: masters and slaves.
The Docker daemon runs on all servers.  On the servers for master,
the Mesos master is run as a process on port 5050 and this is
initiated by the upstart service &#8216;mesos-master&#8217;.  Zookeeper is also
run on the master servers, initiated by the upstart service
&#8216;zookeeper&#8217;.  Zookeeper is used by the master servers for electing
the leader among the masters, and by the slave servers and
frameworks to determine the current leader.  The framework Marathon
is run as a process on port 8080 on the master servers, initiated by
the upstart service &#8216;marathon&#8217;.  On the servers for slave, the Mesos
slave is run as a process initiated by the upstart service
&#8216;mesos-slave&#8217;.</dd>
<dt>Number of master (master-count)</dt>
<dd>Specified in the cluster-create command to indicate how many servers
will run as masters in the cluster.  Having more than one will provide
high availability.  If the load balancer option is specified, the
masters will be in a load balancer pool and the load balancer
virtual IP address (VIP) will serve as the Mesos API endpoint.  A
floating IP associated with the load balancer VIP will serve as the
external Mesos API endpoint.</dd>
<dt>Number of agents (node-count)</dt>
<dd>Specified in the cluster-create command to indicate how many servers
will run as Mesos slave in the cluster.  Docker daemon is run locally to
host containers from users.  The slaves report their available
resources to the master and accept request from the master to deploy
tasks from the frameworks.  In this case, the tasks will be to
run Docker containers.</dd>
<dt>Network driver (network-driver)</dt>
<dd>Specified in the ClusterTemplate to select the network driver.  Currently
&#8216;docker&#8217; is the only supported driver: containers are connected to
the &#8216;docker0&#8217; bridge on each node and are assigned local IP address.
Refer to the <a class="reference internal" href="#networking">Networking</a> section for more details.</dd>
<dt>Volume driver (volume-driver)</dt>
<dd>Specified in the ClusterTemplate to select the volume driver to provide
persistent storage for containers.  The supported volume driver is
&#8216;rexray&#8217;.  The default is no volume driver.  When &#8216;rexray&#8217; or other
volume driver is deployed, you can use the Docker &#8216;volume&#8217; command to
create, mount, unmount, delete volumes in containers.  Cinder block
storage is used as the backend to support this feature.
Refer to the <a class="reference internal" href="#storage">Storage</a> section for more details.</dd>
<dt>Storage driver (docker-storage-driver)</dt>
<dd>This is currently not supported for Mesos.</dd>
</dl>
<p>Image (image)</p>
<blockquote>
<div>Specified in the ClusterTemplate to indicate the image to boot the servers
for the Mesos master and slave.  The image binary is loaded in
Glance with the attribute &#8216;os_distro = ubuntu&#8217;.  You can download
the <a class="reference external" href="https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2">ready-built image</a>,
or you can create the image as described below in the <a class="reference internal" href="#building-mesos-image">Building
Mesos image</a> section.</div></blockquote>
<dl class="docutils">
<dt>TLS (tls-disabled)</dt>
<dd>Transport Layer Security is currently not implemented yet for Mesos.</dd>
<dt>Log into the servers</dt>
<dd>You can log into the manager and node servers with the account
&#8216;ubuntu&#8217; and the keypair specified in the ClusterTemplate.</dd>
</dl>
<p>In addition to the common attributes in the baymodel, you can specify
the following attributes that are specific to Mesos by using the
labels attribute.</p>
<dl class="docutils">
<dt><span class="target" id="rexray-preempt">rexray_preempt</span></dt>
<dd>When the volume driver &#8216;rexray&#8217; is used, you can mount a data volume
backed by Cinder to a host to be accessed by a container.  In this
case, the label &#8216;rexray_preempt&#8217; can optionally be set to True or
False to enable any host to take control of the volume regardless of
whether other hosts are using the volume.  This will in effect
unmount the volume from the current host and remount it on the new
host.  If this label is set to false, then rexray will ensure data
safety for locking the volume before remounting.  The default value
is False.</dd>
<dt><span class="target" id="mesos-slave-isolation">mesos_slave_isolation</span></dt>
<dd><p class="first">This label corresponds to the Mesos parameter for slave
&#8216;&#8211;isolation&#8217;.  The isolators are needed to provide proper isolation
according to the runtime configurations specified in the container
image.  For more details, refer to the <a class="reference external" href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</a>
and the <a class="reference external" href="http://mesos.apache.org/documentation/latest/container-image/">Mesos container image support</a>.
Valid values for this label are:</p>
<ul class="last simple">
<li>filesystem/posix</li>
<li>filesystem/linux</li>
<li>filesystem/shared</li>
<li>posix/cpu</li>
<li>posix/mem</li>
<li>posix/disk</li>
<li>cgroups/cpu</li>
<li>cgroups/mem</li>
<li>docker/runtime</li>
<li>namespaces/pid</li>
</ul>
</dd>
<dt><span class="target" id="mesos-slave-image-providers">mesos_slave_image_providers</span></dt>
<dd><p class="first">This label corresponds to the Mesos parameter for agent
&#8216;&#8211;image_providers&#8217;, which tells Mesos containerizer what
types of container images are allowed.
For more details, refer to the <a class="reference external" href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</a> and
the <a class="reference external" href="http://mesos.apache.org/documentation/latest/container-image/">Mesos container image support</a>.
Valid values are:</p>
<ul class="last simple">
<li>appc</li>
<li>docker</li>
<li>appc,docker</li>
</ul>
</dd>
<dt><span class="target" id="mesos-slave-work-dir">mesos_slave_work_dir</span></dt>
<dd><p class="first">This label corresponds to the Mesos parameter &#8216;&#8211;work_dir&#8217; for slave.
For more details, refer to the <a class="reference external" href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</a>.
Valid value is a directory path to use as the work directory for
the framework, for example:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span>mesos_slave_work_dir=/tmp/mesos
</pre></div>
</div>
</dd>
<dt><span class="target" id="mesos-slave-executor-env-variables">mesos_slave_executor_env_variables</span></dt>
<dd><p class="first">This label corresponds to the Mesos parameter for slave
&#8216;&#8211;executor_environment_variables&#8217;, which passes additional
environment variables to the executor and subsequent tasks.
For more details, refer to the <a class="reference external" href="http://mesos.apache.org/documentation/latest/configuration/">Mesos configuration</a>.
Valid value is the name of a JSON file, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>mesos_slave_executor_env_variables=/home/ubuntu/test.json
</pre></div>
</div>
<p>The JSON file should contain environment variables, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="s2">&quot;PATH&quot;</span><span class="p">:</span> <span class="s2">&quot;/bin:/usr/bin&quot;</span><span class="p">,</span>
   <span class="s2">&quot;LD_LIBRARY_PATH&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/local/lib&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p class="last">By default the executor will inherit the slave&#8217;s environment
variables.</p>
</dd>
</dl>
<div class="section" id="building-mesos-image">
<h2>Building Mesos image<a class="headerlink" href="#building-mesos-image" title="Permalink to this headline">¶</a></h2>
<p>The boot image for Mesos cluster is an Ubuntu 14.04 base image with the
following middleware pre-installed:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">docker</span></code></li>
<li><code class="docutils literal"><span class="pre">zookeeper</span></code></li>
<li><code class="docutils literal"><span class="pre">mesos</span></code></li>
<li><code class="docutils literal"><span class="pre">marathon</span></code></li>
</ul>
<p>The cluster driver provides two ways to create this image, as follows.</p>
<div class="section" id="diskimage-builder">
<h3>Diskimage-builder<a class="headerlink" href="#diskimage-builder" title="Permalink to this headline">¶</a></h3>
<p>To run the <a class="reference external" href="http://docs.openstack.org/developer/diskimage-builder">diskimage-builder</a> tool
manually, use the provided <a class="reference external" href="http://git.openstack.org/cgit/openstack/magnum/tree/magnum/drivers/mesos_ubuntu_v1/image/mesos/">elements</a>.
Following are the typical steps to use the diskimage-builder tool on
an Ubuntu server:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ sudo apt-get update
$ sudo apt-get install git qemu-utils python-pip

$ git clone https://git.openstack.org/openstack/magnum
$ git clone https://git.openstack.org/openstack/diskimage-builder.git
$ git clone https://git.openstack.org/openstack/dib-utils.git
$ git clone https://git.openstack.org/openstack/tripleo-image-elements.git
$ git clone https://git.openstack.org/openstack/heat-templates.git
$ export PATH=&quot;${PWD}/dib-utils/bin:$PATH&quot;
$ export ELEMENTS_PATH=tripleo-image-elements/elements:heat-templates/hot/software-config/elements:magnum/magnum/drivers/mesos_ubuntu_v1/image/mesos
$ export DIB_RELEASE=trusty

$ diskimage-builder/bin/disk-image-create ubuntu vm docker mesos \
    os-collect-config os-refresh-config os-apply-config \
    heat-config heat-config-script \
    -o ubuntu-mesos.qcow2
</pre></div>
</div>
</div>
<div class="section" id="dockerfile">
<h3>Dockerfile<a class="headerlink" href="#dockerfile" title="Permalink to this headline">¶</a></h3>
<p>To build the image as above but within a Docker container, use the
provided <a class="reference external" href="http://git.openstack.org/cgit/openstack/magnum/tree/magnum/drivers/mesos_ubuntu_v1/image/Dockerfile">Dockerfile</a>.
The output image will be saved as &#8216;/tmp/ubuntu-mesos.qcow2&#8217;.
Following are the typical steps to run a Docker container to build the image:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ git clone https://git.openstack.org/openstack/magnum
$ cd magnum/magnum/drivers/mesos_ubuntu_v1/image
$ sudo docker build -t magnum/mesos-builder .
$ sudo docker run -v /tmp:/output --rm -ti --privileged magnum/mesos-builder
...
Image file /output/ubuntu-mesos.qcow2 created...
</pre></div>
</div>
</div>
</div>
<div class="section" id="using-marathon">
<h2>Using Marathon<a class="headerlink" href="#using-marathon" title="Permalink to this headline">¶</a></h2>
<p>Marathon is a Mesos framework for long running applications.  Docker
containers can be deployed via Marathon&#8217;s REST API.  To get the
endpoint for Marathon, run the cluster-show command and look for the
property &#8216;api_address&#8217;.  Marathon&#8217;s endpoint is port 8080 on this IP
address, so the web console can be accessed at:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>http://&lt;api_address&gt;:8080/
</pre></div>
</div>
<p>Refer to Marathon documentation for details on running applications.
For example, you can &#8216;post&#8217; a JSON app description to
<code class="docutils literal"><span class="pre">http://&lt;api_address&gt;:8080/apps</span></code> to deploy a Docker container:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ cat &gt; app.json &lt;&lt; END
{
  &quot;container&quot;: {
    &quot;type&quot;: &quot;DOCKER&quot;,
    &quot;docker&quot;: {
      &quot;image&quot;: &quot;libmesos/ubuntu&quot;
    }
  },
  &quot;id&quot;: &quot;ubuntu&quot;,
  &quot;instances&quot;: 1,
  &quot;cpus&quot;: 0.5,
  &quot;mem&quot;: 512,
  &quot;uris&quot;: [],
  &quot;cmd&quot;: &quot;while sleep 10; do date -u +%T; done&quot;
}
END
$ API_ADDRESS=$(magnum cluster-show mesos-cluster | awk &#39;/ api_address /{print $4}&#39;)
$ curl -X POST -H &quot;Content-Type: application/json&quot; \
    http://${API_ADDRESS}:8080/v2/apps -d@app.json
</pre></div>
</div>
</div>
</div>
<div class="section" id="transport-layer-security">
<h1>Transport Layer Security<a class="headerlink" href="#transport-layer-security" title="Permalink to this headline">¶</a></h1>
<p>Magnum uses TLS to secure communication between a cluster&#8217;s services and
the outside world.  TLS is a complex subject, and many guides on it
exist already.  This guide will not attempt to fully describe TLS, but
instead will only cover the necessary steps to get a client set up to
talk to a cluster with TLS. A more in-depth guide on TLS can be found in
the <a class="reference external" href="https://www.feistyduck.com/books/openssl-cookbook/">OpenSSL Cookbook</a> by Ivan Ristić.</p>
<p>TLS is employed at 3 points in a cluster:</p>
<ol class="arabic simple">
<li>By Magnum to communicate with the cluster API endpoint</li>
<li>By the cluster worker nodes to communicate with the master nodes</li>
<li>By the end-user when they use the native client libraries to
interact with the cluster.  This applies to both a CLI or a program
that uses a client for the particular cluster.  Each client needs a
valid certificate to authenticate and communicate with a cluster.</li>
</ol>
<p>The first two cases are implemented internally by Magnum and are not
exposed to the users, while the last case involves the users and is
described in more details below.</p>
<div class="section" id="deploying-a-secure-cluster">
<h2>Deploying a secure cluster<a class="headerlink" href="#deploying-a-secure-cluster" title="Permalink to this headline">¶</a></h2>
<p>Current TLS support is summarized below:</p>
<table border="1" class="docutils">
<colgroup>
<col width="48%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">COE</th>
<th class="head">TLS support</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Kubernetes</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Swarm</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Mesos</td>
<td>no</td>
</tr>
</tbody>
</table>
<p>For cluster type with TLS support, e.g. Kubernetes and Swarm, TLS is
enabled by default.  To disable TLS in Magnum, you can specify the
parameter &#8216;&#8211;tls-disabled&#8217; in the ClusterTemplate.  Please note it is not
recommended to disable TLS due to security reasons.</p>
<p>In the following example, Kubernetes is used to illustrate a secure
cluster, but the steps are similar for other cluster types that have TLS
support.</p>
<p>First, create a ClusterTemplate; by default TLS is enabled in
Magnum, therefore it does not need to be specified via a parameter:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-template-create --name secure-kubernetes \
                           --keypair default \
                           --external-network public \
                           --image fedora-atomic-latest \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 3 \
                           --coe kubernetes \
                           --network-driver flannel

+-----------------------+--------------------------------------+
| Property              | Value                                |
+-----------------------+--------------------------------------+
| insecure_registry     | None                                 |
| http_proxy            | None                                 |
| updated_at            | None                                 |
| master_flavor_id      | None                                 |
| uuid                  | 5519b24a-621c-413c-832f-c30424528b31 |
| no_proxy              | None                                 |
| https_proxy           | None                                 |
| tls_disabled          | False                                |
| keypair_id            | time4funkey                          |
| public                | False                                |
| labels                | {}                                   |
| docker_volume_size    | 5                                    |
| server_type           | vm                                   |
| external_network_id   | public                               |
| cluster_distro        | fedora-atomic                        |
| image_id              | fedora-atomic-latest                 |
| volume_driver         | None                                 |
| registry_enabled      | False                                |
| docker_storage_driver | devicemapper                         |
| apiserver_port        | None                                 |
| name                  | secure-kubernetes                    |
| created_at            | 2016-07-25T23:09:50+00:00            |
| network_driver        | flannel                              |
| fixed_network         | None                                 |
| coe                   | kubernetes                           |
| flavor_id             | m1.small                             |
| dns_nameserver        | 8.8.8.8                              |
+-----------------------+--------------------------------------+
</pre></div>
</div>
<p>Now create a cluster. Use the ClusterTemplate name as a template for cluster
creation:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-create --name secure-k8s-cluster \
                      --cluster-template secure-kubernetes \
                      --node-count 1

+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_IN_PROGRESS                                         |
| uuid               | 3968ffd5-678d-4555-9737-35f191340fda                       |
| stack_id           | c96b66dd-2109-4ae2-b510-b3428f1e8761                       |
| status_reason      | None                                                       |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | None                                                       |
| create_timeout     | 0                                                          |
| api_address        | None                                                       |
| coe_version        | -                                                          |
| cluster_template_id| 5519b24a-621c-413c-832f-c30424528b31                       |
| master_addresses   | None                                                       |
| node_count         | 1                                                          |
| node_addresses     | None                                                       |
| master_count       | 1                                                          |
| container_version  | -                                                          |
| discovery_url      | https://discovery.etcd.io/ba52a8178e7364d43a323ee4387cf28e |
| name               | secure-k8s-cluster                                          |
+--------------------+------------------------------------------------------------+
</pre></div>
</div>
<p>Now run cluster-show command to get the details of the cluster and verify that
the api_address is &#8216;https&#8217;:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-show secure-k8scluster
+--------------------+------------------------------------------------------------+
| Property           | Value                                                      |
+--------------------+------------------------------------------------------------+
| status             | CREATE_COMPLETE                                            |
| uuid               | 04952c60-a338-437f-a7e7-d016d1d00e65                       |
| stack_id           | b7bf72ce-b08e-4768-8201-e63a99346898                       |
| status_reason      | Stack CREATE completed successfully                        |
| created_at         | 2016-07-25T23:14:06+00:00                                  |
| updated_at         | 2016-07-25T23:14:10+00:00                                  |
| create_timeout     | 60                                                         |
| coe_version        | v1.2.0                                                     |
| api_address        | https://192.168.19.86:6443                                 |
| cluster_template_id| da2825a0-6d09-4208-b39e-b2db666f1118                       |
| master_addresses   | [&#39;192.168.19.87&#39;]                                          |
| node_count         | 1                                                          |
| node_addresses     | [&#39;192.168.19.88&#39;]                                          |
| master_count       | 1                                                          |
| container_version  | 1.9.1                                                      |
| discovery_url      | https://discovery.etcd.io/3b7fb09733429d16679484673ba3bfd5 |
| name               | secure-k8s-cluster                                          |
+--------------------+------------------------------------------------------------+
</pre></div>
</div>
<p>You can see the api_address contains https in the URL, showing that
the Kubernetes services are configured securely with SSL certificates
and now any communication to kube-apiserver will be over https.</p>
</div>
<div class="section" id="interfacing-with-a-secure-cluster">
<h2>Interfacing with a secure cluster<a class="headerlink" href="#interfacing-with-a-secure-cluster" title="Permalink to this headline">¶</a></h2>
<p>To communicate with the API endpoint of a secure cluster, you will need so
supply 3 SSL artifacts:</p>
<ol class="arabic simple">
<li>Your client key</li>
<li>A certificate for your client key that has been signed by a
Certificate Authority (CA)</li>
<li>The certificate of the CA</li>
</ol>
<p>There are two ways to obtain these 3 artifacts.</p>
<div class="section" id="automated">
<h3>Automated<a class="headerlink" href="#automated" title="Permalink to this headline">¶</a></h3>
<p>Magnum provides the command &#8216;cluster-config&#8217; to help the user in setting
up the environment and artifacts for TLS, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-config swarm-cluster --dir myclusterconfig
</pre></div>
</div>
<p>This will display the necessary environment variables, which you
can add to your environment:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>export DOCKER_HOST=tcp://172.24.4.5:2376
export DOCKER_CERT_PATH=myclusterconfig
export DOCKER_TLS_VERIFY=True
</pre></div>
</div>
<p>And the artifacts are placed in the directory specified:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ca</span><span class="o">.</span><span class="n">pem</span>
<span class="n">cert</span><span class="o">.</span><span class="n">pem</span>
<span class="n">key</span><span class="o">.</span><span class="n">pem</span>
</pre></div>
</div>
<p>You can now use the native client to interact with the COE.
The variables and artifacts are unique to the bay.</p>
<p>The parameters for &#8216;bay-config&#8217; are as follows:</p>
<dl class="docutils">
<dt>&#8211;dir &lt;dirname&gt;</dt>
<dd>Directory to save the certificate and config files.</dd>
</dl>
<table class="docutils option-list" frame="void" rules="none">
<col class="option" />
<col class="description" />
<tbody valign="top">
<tr><td class="option-group">
<kbd><span class="option">--force</span></kbd></td>
<td>Overwrite existing files in the directory specified.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="manual">
<h3>Manual<a class="headerlink" href="#manual" title="Permalink to this headline">¶</a></h3>
<p>You can create the key and certificates manually using the following steps.</p>
<dl class="docutils">
<dt>Client Key</dt>
<dd><p class="first">Your personal private key is essentially a cryptographically generated
string of bytes. It should be protected in the same manner as a
password. To generate an RSA key, you can use the &#8216;genrsa&#8217; command of
the &#8216;openssl&#8217; tool:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>openssl genrsa -out key.pem 4096
</pre></div>
</div>
<p class="last">This command generates a 4096 byte RSA key at key.pem.</p>
</dd>
<dt>Signed Certificate</dt>
<dd><p class="first">To authenticate your key, you need to have it signed by a CA.  First
generate the Certificate Signing Request (CSR).  The CSR will be
used by Magnum to generate a signed certificate that you will use to
communicate with the cluster.  To generate a CSR, openssl requires a
config file that specifies a few values.  Using the example template
below, you can fill in the &#8216;CN&#8217; value with your name and save it as
client.conf:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ cat &gt; client.conf &lt;&lt; END
[req]
distinguished_name = req_distinguished_name
req_extensions     = req_ext
prompt = no
[req_distinguished_name]
CN = Your Name
[req_ext]
extendedKeyUsage = clientAuth
END
</pre></div>
</div>
<p>Once you have client.conf, you can run the openssl &#8216;req&#8217; command to
generate the CSR:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>openssl req -new -days 365 \
    -config client.conf \
    -key key.pem \
    -out client.csr
</pre></div>
</div>
<p>Now that you have your client CSR, you can use the Magnum CLI to
send it off to Magnum to get it signed:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span>magnum ca-sign --cluster secure-k8s-cluster --csr client.csr &gt; cert.pem
</pre></div>
</div>
</dd>
<dt>Certificate Authority</dt>
<dd><p class="first">The final artifact you need to retrieve is the CA certificate for
the cluster. This is used by your native client to ensure you are only
communicating with hosts that Magnum set up:</p>
<div class="last highlight-python"><div class="highlight"><pre><span></span>magnum ca-show --cluster secure-k8s-cluster &gt; ca.pem
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="user-examples">
<h2>User Examples<a class="headerlink" href="#user-examples" title="Permalink to this headline">¶</a></h2>
<p>Here are some examples for using the CLI on a secure Kubernetes and
Swarm cluster.  You can perform all the TLS set up automatically by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>eval $(magnum cluster-config &lt;cluster-name&gt;)
</pre></div>
</div>
<p>Or you can perform the manual steps as described above and specify
the TLS options on the CLI.  The SSL artifacts are assumed to be
saved in local files as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>- key.pem: your SSL key
- cert.pem: signed certificate
- ca.pem: certificate for cluster CA
</pre></div>
</div>
<p>For Kubernetes, you need to get &#8216;kubectl&#8217;, a kubernetes CLI tool, to
communicate with the cluster:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>curl -O https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/kubectl
</pre></div>
</div>
<p>Now let&#8217;s run some &#8216;kubectl&#8217; commands to check the secure communication.
If you used &#8216;cluster-config&#8217;, then you can simply run the &#8216;kubectl&#8217; command
without having to specify the TLS options since they have been defined
in the environment:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl version
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;0&quot;, GitVersion:&quot;v1.2.0&quot;, GitCommit:&quot;cffae0523cfa80ddf917aba69f08508b91f603d5&quot;, GitTreeState:&quot;clean&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;0&quot;, GitVersion:&quot;v1.2.0&quot;, GitCommit:&quot;cffae0523cfa80ddf917aba69f08508b91f603d5&quot;, GitTreeState:&quot;clean&quot;}
</pre></div>
</div>
<p>You can specify the TLS options manually as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>KUBERNETES_URL=$(magnum cluster-show secure-k8s-cluster |
                 awk &#39;/ api_address /{print $4}&#39;)
kubectl version --certificate-authority=ca.pem \
                --client-key=key.pem \
                --client-certificate=cert.pem -s $KUBERNETES_URL

kubectl create -f redis-master.yaml --certificate-authority=ca.pem \
                                    --client-key=key.pem \
                                    --client-certificate=cert.pem -s $KUBERNETES_URL

pods/test2

kubectl get pods --certificate-authority=ca.pem \
                 --client-key=key.pem \
                 --client-certificate=cert.pem -s $KUBERNETES_URL
NAME           READY     STATUS    RESTARTS   AGE
redis-master   2/2       Running   0          1m
</pre></div>
</div>
<p>Beside using the environment variables, you can also configure &#8216;kubectl&#8217;
to remember the TLS options:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl config set-cluster secure-k8s-cluster --server=${KUBERNETES_URL} \
    --certificate-authority=${PWD}/ca.pem
kubectl config set-credentials client --certificate-authority=${PWD}/ca.pem \
    --client-key=${PWD}/key.pem --client-certificate=${PWD}/cert.pem
kubectl config set-context secure-k8scluster --cluster=secure-k8scluster --user=client
kubectl config use-context secure-k8scluster
</pre></div>
</div>
<p>Then you can use &#8216;kubectl&#8217; commands without the certificates:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl get pods
NAME           READY     STATUS    RESTARTS   AGE
redis-master   2/2       Running   0          1m
</pre></div>
</div>
<p>Access to Kubernetes User Interface:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>curl -L ${KUBERNETES_URL}/ui --cacert ca.pem --key key.pem \
    --cert cert.pem
</pre></div>
</div>
<p>You may also set up &#8216;kubectl&#8217; proxy which will use your client
certificates to allow you to browse to a local address to use the UI
without installing a certificate in your browser:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl proxy --api-prefix=/ --certificate-authority=ca.pem --client-key=key.pem \
              --client-certificate=cert.pem -s $KUBERNETES_URL
</pre></div>
</div>
<p>You can then open <a class="reference external" href="http://localhost:8001/ui">http://localhost:8001/ui</a> in your browser.</p>
<p>The examples for Docker are similar.  With &#8216;cluster-config&#8217; set up,
you can just run docker commands without TLS options.  To specify the
TLS options manually:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>docker -H tcp://192.168.19.86:2376 --tlsverify \
       --tlscacert ca.pem \
       --tlskey key.pem \
       --tlscert cert.pem \
       info
</pre></div>
</div>
</div>
<div class="section" id="storing-the-certificates">
<h2>Storing the certificates<a class="headerlink" href="#storing-the-certificates" title="Permalink to this headline">¶</a></h2>
<p>Magnum generates and maintains a certificate for each cluster so that it
can also communicate securely with the cluster.  As a result, it is
necessary to store the certificates in a secure manner.  Magnum
provides the following methods for storing the certificates and this
is configured in /etc/magnum/magnum.conf in the section [certificates]
with the parameter &#8216;cert_manager_type&#8217;.</p>
<ol class="arabic">
<li><p class="first">Barbican:
Barbican is a service in OpenStack for storing secrets.  It is used
by Magnum to store the certificates when cert_manager_type is
configured as:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cert_manager_type</span> <span class="o">=</span> <span class="n">barbican</span>
</pre></div>
</div>
<p>This is the recommended configuration for a production environment.
Magnum will interface with Barbican to store and retrieve
certificates, delegating the task of securing the certificates to
Barbican.</p>
</li>
<li><p class="first">Magnum database:
In some cases, a user may want an alternative to storing the
certificates that does not require Barbican.  This can be a
development environment, or a private cloud that has been secured
by other means.  Magnum can store the certificates in its own
database; this is done with the configuration:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cert_manager_type</span> <span class="o">=</span> <span class="n">x509keypair</span>
</pre></div>
</div>
<p>This storage mode is only as secure as the controller server that
hosts the database for the OpenStack services.</p>
</li>
<li><p class="first">Local store:
As another alternative that does not require Barbican, Magnum can
simply store the certificates on the local host filesystem where the
conductor is running, using the configuration:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cert_manager_type</span> <span class="o">=</span> <span class="n">local</span>
</pre></div>
</div>
<p>Note that this mode is only supported when there is a single Magnum
conductor running since the certificates are stored locally.  The
&#8216;local&#8217; mode is not recommended for a production environment.</p>
</li>
</ol>
<p>For the nodes, the certificates for communicating with the masters are
stored locally and the nodes are assumed to be secured.</p>
</div>
</div>
<div class="section" id="networking">
<h1>Networking<a class="headerlink" href="#networking" title="Permalink to this headline">¶</a></h1>
<p>There are two components that make up the networking in a cluster.</p>
<ol class="arabic simple">
<li>The Neutron infrastructure for the cluster: this includes the
private network, subnet, ports, routers, load balancers, etc.</li>
<li>The networking model presented to the containers: this is what the
containers see in communicating with each other and to the external
world. Typically this consists of a driver deployed on each node.</li>
</ol>
<p>The two components are deployed and managed separately.  The Neutron
infrastructure is the integration with OpenStack; therefore, it
is stable and more or less similar across different COE
types.  The networking model, on the other hand, is specific to the
COE type and is still under active development in the various
COE communities, for example,
<a class="reference external" href="https://github.com/docker/libnetwork">Docker libnetwork</a> and
<a class="reference external" href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/design/networking.md">Kubernetes Container Networking</a>.
As a result, the implementation for the networking models is evolving and
new models are likely to be introduced in the future.</p>
<p>For the Neutron infrastructure, the following configuration can
be set in the ClusterTemplate:</p>
<dl class="docutils">
<dt>external-network</dt>
<dd>The external Neutron network ID to connect to this cluster. This
is used to connect the cluster to the external internet, allowing
the nodes in the cluster to access external URL for discovery, image
download, etc.  If not specified, the default value is &#8220;public&#8221; and this
is valid for a typical devstack.</dd>
<dt>fixed-network</dt>
<dd>The Neutron network to use as the private network for the cluster nodes.
If not specified, a new Neutron private network will be created.</dd>
<dt>dns-nameserver</dt>
<dd>The DNS nameserver to use for this cluster.  This is an IP address for
the server and it is used to configure the Neutron subnet of the
cluster (dns_nameservers).  If not specified, the default DNS is
8.8.8.8, the publicly available DNS.</dd>
<dt>http-proxy, https-proxy, no-proxy</dt>
<dd>The proxy for the nodes in the cluster, to be used when the cluster is
behind a firewall and containers cannot access URL&#8217;s on the external
internet directly.  For the parameter http-proxy and https-proxy, the
value to provide is a URL and it will be set in the environment
variable HTTP_PROXY and HTTPS_PROXY respectively in the nodes.  For
the parameter no-proxy, the value to provide is an IP or list of IP&#8217;s
separated by comma.  Likewise, the value will be set in the
environment variable NO_PROXY in the nodes.</dd>
</dl>
<p>For the networking model to the container, the following configuration
can be set in the ClusterTemplate:</p>
<dl class="docutils">
<dt>network-driver</dt>
<dd><p class="first">The network driver name for instantiating container networks.
Currently, the following network drivers are supported:</p>
<table border="1" class="docutils">
<colgroup>
<col width="18%" />
<col width="29%" />
<col width="24%" />
<col width="29%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Driver</th>
<th class="head">Kubernetes</th>
<th class="head">Swarm</th>
<th class="head">Mesos</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Flannel</td>
<td>supported</td>
<td>supported</td>
<td>unsupported</td>
</tr>
<tr class="row-odd"><td>Docker</td>
<td>unsupported</td>
<td>supported</td>
<td>supported</td>
</tr>
</tbody>
</table>
<p class="last">If not specified, the default driver is Flannel for Kubernetes, and
Docker for Swarm and Mesos.</p>
</dd>
</dl>
<p>Particular network driver may require its own set of parameters for
configuration, and these parameters are specified through the labels
in the ClusterTemplate.  Labels are arbitrary key=value pairs.</p>
<p>When Flannel is specified as the network driver, the following
optional labels can be added:</p>
<dl class="docutils">
<dt><span class="target" id="flannel-network-cidr">flannel_network_cidr</span></dt>
<dd>IPv4 network in CIDR format to use for the entire Flannel network.
If not specified, the default is 10.100.0.0/16.</dd>
<dt><span class="target" id="flannel-network-subnetlen">flannel_network_subnetlen</span></dt>
<dd>The size of the subnet allocated to each host. If not specified, the
default is 24.</dd>
<dt><span class="target" id="flannel-backend">flannel_backend</span></dt>
<dd>The type of backend for Flannel.  Possible values are <em>udp, vxlan,
host-gw</em>.  If not specified, the default is <em>udp</em>.  Selecting the
best backend depends on your networking.  Generally, <em>udp</em> is
the most generally supported backend since there is little
requirement on the network, but it typically offers the lowest
performance.  The <em>vxlan</em> backend performs better, but requires
vxlan support in the kernel so the image used to provision the
nodes needs to include this support.  The <em>host-gw</em> backend offers
the best performance since it does not actually encapsulate
messages, but it requires all the nodes to be on the same L2
network.  The private Neutron network that Magnum creates does
meet this requirement;  therefore if the parameter <em>fixed_network</em>
is not specified in the ClusterTemplate, <em>host-gw</em> is the best choice for
the Flannel backend.</dd>
</dl>
</div>
<div class="section" id="high-availability">
<h1>High Availability<a class="headerlink" href="#high-availability" title="Permalink to this headline">¶</a></h1>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="scaling">
<h1>Scaling<a class="headerlink" href="#scaling" title="Permalink to this headline">¶</a></h1>
<div class="section" id="performance-tuning-for-periodic-task">
<h2>Performance tuning for periodic task<a class="headerlink" href="#performance-tuning-for-periodic-task" title="Permalink to this headline">¶</a></h2>
<p>Magnum&#8217;s periodic task performs a <cite>stack-get</cite> operation on the Heat stack
underlying each of its clusters. If you have a large amount of clusters this
can create considerable load on the Heat API. To reduce that load you can
configure Magnum to perform one global <cite>stack-list</cite> per periodic task instead
of one per cluster. This is disabled by default, both from the Heat and Magnum
side since it causes a security issue, though: any user in any tenant holding
the <cite>admin</cite> role can perform a global <cite>stack-list</cite> operation if Heat is
configured to allow it for Magnum. If you want to enable it nonetheless,
proceed as follows:</p>
<ol class="arabic">
<li><p class="first">Set <cite>periodic_global_stack_list</cite> in magnum.conf to <cite>True</cite>
(<cite>False</cite> by default).</p>
</li>
<li><p class="first">Update heat policy to allow magnum list stacks. To this end, edit your heat
policy file, usually etc/heat/policy.json``:</p>
<div class="highlight-ini"><div class="highlight"><pre><span></span><span class="na">...</span>
<span class="na">stacks:global_index: &quot;rule:context_is_admin&quot;,</span>
</pre></div>
</div>
<p>Now restart heat.</p>
</li>
</ol>
</div>
<div class="section" id="containers-and-nodes">
<h2>Containers and nodes<a class="headerlink" href="#containers-and-nodes" title="Permalink to this headline">¶</a></h2>
<p>Scaling containers and nodes refers to increasing or decreasing
allocated system resources.  Scaling is a broad topic and involves
many dimensions.  In the context of Magnum in this guide, we consider
the following issues:</p>
<ul class="simple">
<li>Scaling containers and scaling cluster nodes (infrastructure)</li>
<li>Manual and automatic scaling</li>
</ul>
<p>Since this is an active area of development, a complete solution
covering all issues does not exist yet, but partial solutions are
emerging.</p>
<p>Scaling containers involves managing the number of instances of the
container by replicating or deleting instances.  This can be used to
respond to change in the workload being supported by the application;
in this case, it is typically driven by certain metrics relevant to the
application such as response time, etc.  Other use cases include
rolling upgrade, where a new version of a service can gradually be
scaled up while the older version is gradually scaled down.  Scaling
containers is supported at the COE level and is specific to each COE
as well as the version of the COE.  You will need to refer to the
documentation for the proper COE version for full details, but
following are some pointers for reference.</p>
<p>For Kubernetes, pods are scaled manually by setting the count in the
replication controller.  Kubernetes version 1.3 and later also
supports <a class="reference external" href="http://blog.kubernetes.io/2016/07/autoscaling-in-kubernetes.html">autoscaling</a>.
For Docker, the tool &#8216;Docker Compose&#8217; provides the command
<a class="reference external" href="https://docs.docker.com/compose/reference/scale/">docker-compose scale</a> which lets you
manually set the number of instances of a container.  For Swarm
version 1.12 and later, services can also be scaled manually through
the command <a class="reference external" href="https://docs.docker.com/engine/swarm/swarm-tutorial/scale-service/">docker service scale</a>.
Automatic scaling for Swarm is not yet available.  Mesos manages the
resources and does not support scaling directly; instead, this is
provided by frameworks running within Mesos.  With the Marathon
framework currently supported in the Mesos cluster, you can use the
<a class="reference external" href="https://mesosphere.github.io/marathon/docs/application-basics.html">scale operation</a>
on the Marathon UI or through a REST API call to manually set the
attribute &#8216;instance&#8217; for a container.</p>
<p>Scaling the cluster nodes involves managing the number of nodes in the
cluster by adding more nodes or removing nodes.  There is no direct
correlation between the number of nodes and the number of containers
that can be hosted since the resources consumed (memory, CPU, etc)
depend on the containers.  However, if a certain resource is exhausted
in the cluster, adding more nodes would add more resources for hosting
more containers.  As part of the infrastructure management, Magnum
supports manual scaling through the attribute &#8216;node_count&#8217; in the
cluster, so you can scale the cluster simply by changing this
attribute:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-update mycluster replace node_count=2
</pre></div>
</div>
<p>Refer to the section <a class="reference internal" href="#scale">Scale</a> lifecycle operation for more details.</p>
<p>Adding nodes to a cluster is straightforward: Magnum deploys
additional VMs or baremetal servers through the heat templates and
invokes the COE-specific mechanism for registering the new nodes to
update the available resources in the cluster.  Afterward, it is up to
the COE or user to re-balance the workload by launching new container
instances or re-launching dead instances on the new nodes.</p>
<p>Removing nodes from a cluster requires some more care to ensure
continuous operation of the containers since the nodes being removed
may be actively hosting some containers.  Magnum performs a simple
heuristic that is specific to the COE to find the best node candidates
for removal, as follows:</p>
<dl class="docutils">
<dt>Kubernetes</dt>
<dd>Magnum scans the pods in the namespace &#8216;Default&#8217; to determine the
nodes that are <em>not</em> hosting any (empty nodes).  If the number of
nodes to be removed is equal or less than the number of these empty
nodes, these nodes will be removed from the cluster.  If the number
of nodes to be removed is larger than the number of empty nodes, a
warning message will be sent to the Magnum log and the empty nodes
along with additional nodes will be removed from the cluster.  The
additional nodes are selected randomly and the pods running on them
will be deleted without warning.  For this reason, a good practice
is to manage the pods through the replication controller so that the
deleted pods will be relaunched elsewhere in the cluster.  Note also
that even when only the empty nodes are removed, there is no
guarantee that no pod will be deleted because there is no locking to
ensure that Kubernetes will not launch new pods on these nodes after
Magnum has scanned the pods.</dd>
<dt>Swarm</dt>
<dd>No node selection heuristic is currently supported.  If you decrease
the node_count, a node will be chosen by magnum without
consideration of what containers are running on the selected node.</dd>
<dt>Mesos</dt>
<dd>Magnum scans the running tasks on Marathon server to determine the
nodes on which there is <em>no</em> task running (empty nodes). If the
number of nodes to be removed is equal or less than the number of
these empty nodes, these nodes will be removed from the cluster.
If the number of nodes to be removed is larger than the number of
empty nodes, a warning message will be sent to the Magnum log and
the empty nodes along with additional nodes will be removed from the
cluster. The additional nodes are selected randomly and the containers
running on them will be deleted without warning. Note that even when
only the empty nodes are removed, there is no guarantee that no
container will be deleted because there is no locking to ensure that
Mesos will not launch new containers on these nodes after Magnum
has scanned the tasks.</dd>
</dl>
<p>Currently, scaling containers and scaling cluster nodes are handled
separately, but in many use cases, there are interactions between the
two operations.  For instance, scaling up the containers may exhaust
the available resources in the cluster, thereby requiring scaling up
the cluster nodes as well.  Many complex issues are involved in
managing this interaction.  A presentation at the OpenStack Tokyo
Summit 2015 covered some of these issues along with some early
proposals, <a class="reference external" href="https://www.openstack.org/summit/tokyo-2015/videos/presentation/exploring-magnum-and-senlin-integration-for-autoscaling-containers">Exploring Magnum and Senlin integration for autoscaling
containers</a>.
This remains an active area of discussion and research.</p>
</div>
</div>
<div class="section" id="storage">
<h1>Storage<a class="headerlink" href="#storage" title="Permalink to this headline">¶</a></h1>
<p>Currently Cinder provides the block storage to the containers, and the
storage is made available in two ways: as ephemeral storage and as
persistent storage.</p>
<div class="section" id="ephemeral-storage">
<h2>Ephemeral storage<a class="headerlink" href="#ephemeral-storage" title="Permalink to this headline">¶</a></h2>
<p>The filesystem for the container consists of multiple layers from the
image and a top layer that holds the modification made by the
container.  This top layer requires storage space and the storage is
configured in the Docker daemon through a number of storage options.
When the container is removed, the storage allocated to the particular
container is also deleted.</p>
<p>Magnum can manage the containers&#8217; filesystem in two ways, storing them
on the local disk of the compute instances or in a separate Cinder block
volume for each node in the cluster, mounts it to the node and
configures it to be used as ephemeral storage.  Users can specify the
size of the Cinder volume with the ClusterTemplate attribute
&#8216;docker-volume-size&#8217;. Currently the block size is fixed at cluster
creation time, but future lifecycle operations may allow modifying the
block size during the life of the cluster.</p>
<p>Both local disk and the Cinder block storage can be used with a number
of Docker storage drivers available.</p>
<ul class="simple">
<li>&#8216;devicemapper&#8217;: When used with a dedicated Cinder volume it is
configured using direct-lvm and offers very good performance. If it&#8217;s
used with the compute instance&#8217;s local disk uses a loopback device
offering poor performance and it&#8217;s not recommended for production
environments. Using the &#8216;devicemapper&#8217; driver does allow the use of
SELinux.</li>
<li>&#8216;overlay&#8217; When used with a dedicated Cinder volume offers as good
or better performance than devicemapper. If used on the local disk of
the compute instance (especially with high IOPS drives) you can get
significant performance gains. However, for kernel versions less than
4.9, SELinux must be disabled inside the containers resulting in worse
container isolation, although it still runs in enforcing mode on the
cluster compute instances.</li>
</ul>
</div>
<div class="section" id="persistent-storage">
<h2>Persistent storage<a class="headerlink" href="#persistent-storage" title="Permalink to this headline">¶</a></h2>
<p>In some use cases, data read/written by a container needs to persist
so that it can be accessed later.  To persist the data, a Cinder
volume with a filesystem on it can be mounted on a host and be made
available to the container, then be unmounted when the container exits.</p>
<p>Docker provides the &#8216;volume&#8217; feature for this purpose: the user
invokes the &#8216;volume create&#8217; command, specifying a particular volume
driver to perform the actual work.  Then this volume can be mounted
when a container is created.  A number of third-party volume drivers
support OpenStack Cinder as the backend, for example Rexray and
Flocker.  Magnum currently supports Rexray as the volume driver for
Swarm and Mesos.  Other drivers are being considered.</p>
<p>Kubernetes allows a previously created Cinder block to be mounted to
a pod and this is done by specifying the block ID in the pod YAML file.
When the pod is scheduled on a node, Kubernetes will interface with
Cinder to request the volume to be mounted on this node, then
Kubernetes will launch the Docker container with the proper options to
make the filesystem on the Cinder volume accessible to the container
in the pod.  When the pod exits, Kubernetes will again send a request
to Cinder to unmount the volume&#8217;s filesystem, making it available to be
mounted on other nodes.</p>
<p>Magnum supports these features to use Cinder as persistent storage
using the ClusterTemplate attribute &#8216;volume-driver&#8217; and the support matrix
for the COE types is summarized as follows:</p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="28%" />
<col width="28%" />
<col width="28%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Driver</th>
<th class="head">Kubernetes</th>
<th class="head">Swarm</th>
<th class="head">Mesos</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>cinder</td>
<td>supported</td>
<td>unsupported</td>
<td>unsupported</td>
</tr>
<tr class="row-odd"><td>rexray</td>
<td>unsupported</td>
<td>supported</td>
<td>supported</td>
</tr>
</tbody>
</table>
<p>Following are some examples for using Cinder as persistent storage.</p>
<div class="section" id="using-cinder-in-kubernetes">
<h3>Using Cinder in Kubernetes<a class="headerlink" href="#using-cinder-in-kubernetes" title="Permalink to this headline">¶</a></h3>
<p><strong>NOTE:</strong> This feature requires Kubernetes version 1.1.1 or above and
Docker version 1.8.3 or above.  The public Fedora image from Atomic
currently meets this requirement.</p>
<p><strong>NOTE:</strong> The following steps are a temporary workaround, and Magnum&#8217;s
development team is working on a long term solution to automate these steps.</p>
<ol class="arabic">
<li><p class="first">Create the ClusterTemplate.</p>
<p>Specify &#8216;cinder&#8217; as the volume-driver for Kubernetes:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-template-create --name k8s-cluster-template \
                           --image fedora-23-atomic-7 \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --flavor m1.small \
                           --docker-volume-size 5 \
                           --network-driver flannel \
                           --coe kubernetes \
                           --volume-driver cinder
</pre></div>
</div>
</li>
<li><p class="first">Create the cluster:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-create --name k8s-cluster \
                      --cluster-template k8s-cluster-template \
                      --node-count 1
</pre></div>
</div>
</li>
<li><p class="first">Configure kubelet.</p>
<p>To allow Kubernetes to interface with Cinder, log into each minion
node of your cluster and perform step 4 through 6:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo vi /etc/kubernetes/kubelet
</pre></div>
</div>
<p>Comment out the line:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1">#KUBELET_ARGS=--config=/etc/kubernetes/manifests --cadvisor-port=4194</span>
</pre></div>
</div>
<p>Uncomment the line:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1">#KUBELET_ARGS=&quot;--config=/etc/kubernetes/manifests --cadvisor-port=4194 --cloud-provider=openstack --cloud-config=/etc/kubernetes/kube_openstack_config&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Enter OpenStack user credential:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo vi /etc/kubernetes/kube_openstack_config
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>The username, tenant-name and region entries have been filled in with the
Keystone values of the user who created the cluster.  Enter the password
of this user on the entry for password:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">password</span><span class="o">=</span><span class="n">ChangeMe</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic" start="5">
<li><p class="first">Restart Kubernetes services:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo systemctl restart kubelet
</pre></div>
</div>
<p>On restart, the new configuration enables the Kubernetes cloud provider
plugin for OpenStack, along with the necessary credential for kubelet
to authenticate with Keystone and to make request to OpenStack services.</p>
</li>
<li><p class="first">Install nsenter:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo docker run -v /usr/local/bin:/target jpetazzo/nsenter
</pre></div>
</div>
<p>The nsenter utility is used by Kubernetes to run new processes within
existing kernel namespaces. This allows the kubelet agent to manage storage
for pods.</p>
</li>
</ol>
<p>Kubernetes is now ready to use Cinder for persistent storage.
Following is an example illustrating how Cinder is used in a pod.</p>
<ol class="arabic">
<li><p class="first">Create the cinder volume:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cinder create --display-name=test-repo 1

ID=$(cinder create --display-name=test-repo 1 | awk -F&#39;|&#39; &#39;$2~/^[[:space:]]*id/ {print $3}&#39;)
</pre></div>
</div>
<p>The command will generate the volume with a ID. The volume ID will be
specified in Step 2.</p>
</li>
<li><p class="first">Create a pod in this cluster and mount this cinder volume to the pod.
Create a file (e.g nginx-cinder.yaml) describing the pod:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cat &gt; nginx-cinder.yaml &lt;&lt; END
apiVersion: v1
kind: Pod
metadata:
  name: aws-web
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          hostPort: 8081
          protocol: TCP
      volumeMounts:
        - name: html-volume
          mountPath: &quot;/usr/share/nginx/html&quot;
  volumes:
    - name: html-volume
      cinder:
        # Enter the volume ID below
        volumeID: $ID
        fsType: ext4
END
</pre></div>
</div>
</li>
</ol>
<p><strong>NOTE:</strong> The Cinder volume ID needs to be configured in the YAML file
so the existing Cinder volume can be mounted in a pod by specifying
the volume ID in the pod manifest as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>volumes:
- name: html-volume
  cinder:
    volumeID: $ID
    fsType: ext4
</pre></div>
</div>
<ol class="arabic" start="3">
<li><p class="first">Create the pod by the normal Kubernetes interface:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>kubectl create -f nginx-cinder.yaml
</pre></div>
</div>
</li>
</ol>
<p>You can start a shell in the container to check that the mountPath exists,
and on an OpenStack client you can run the command &#8216;cinder list&#8217; to verify
that the cinder volume status is &#8216;in-use&#8217;.</p>
</div>
<div class="section" id="using-cinder-in-swarm">
<h3>Using Cinder in Swarm<a class="headerlink" href="#using-cinder-in-swarm" title="Permalink to this headline">¶</a></h3>
<p><em>To be filled in</em></p>
</div>
<div class="section" id="using-cinder-in-mesos">
<h3>Using Cinder in Mesos<a class="headerlink" href="#using-cinder-in-mesos" title="Permalink to this headline">¶</a></h3>
<ol class="arabic">
<li><p class="first">Create the ClusterTemplate.</p>
<p>Specify &#8216;rexray&#8217; as the volume-driver for Mesos.  As an option, you
can specify in a label the attributes &#8216;rexray_preempt&#8217; to enable
any host to take control of a volume regardless if other
hosts are using the volume. If this is set to false, the driver
will ensure data safety by locking the volume:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-template-create --name mesos-cluster-template \
                           --image ubuntu-mesos \
                           --keypair testkey \
                           --external-network public \
                           --dns-nameserver 8.8.8.8 \
                           --master-flavor m1.magnum \
                           --docker-volume-size 4 \
                           --tls-disabled \
                           --flavor m1.magnum \
                           --coe mesos \
                           --volume-driver rexray \
                           --labels rexray-preempt=true
</pre></div>
</div>
</li>
<li><p class="first">Create the Mesos cluster:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>magnum cluster-create --name mesos-cluster \
                      --cluster-template mesos-cluster-template \
                      --node-count 1
</pre></div>
</div>
</li>
<li><p class="first">Create the cinder volume and configure this cluster:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cinder create --display-name=redisdata 1
</pre></div>
</div>
<p>Create the following file</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cat</span> <span class="o">&gt;</span> <span class="n">mesos</span><span class="o">.</span><span class="n">json</span> <span class="o">&lt;&lt;</span> <span class="n">END</span>
<span class="p">{</span>
  <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;redis&quot;</span><span class="p">,</span>
  <span class="s2">&quot;container&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;docker&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="s2">&quot;redis&quot;</span><span class="p">,</span>
    <span class="s2">&quot;network&quot;</span><span class="p">:</span> <span class="s2">&quot;BRIDGE&quot;</span><span class="p">,</span>
    <span class="s2">&quot;portMappings&quot;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span> <span class="s2">&quot;containerPort&quot;</span><span class="p">:</span> <span class="mi">80</span><span class="p">,</span> <span class="s2">&quot;hostPort&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;protocol&quot;</span><span class="p">:</span> <span class="s2">&quot;tcp&quot;</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">[</span>
       <span class="p">{</span> <span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;volume-driver&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;rexray&quot;</span> <span class="p">},</span>
       <span class="p">{</span> <span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;volume&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;redisdata:/data&quot;</span> <span class="p">}</span>
    <span class="p">]</span>
    <span class="p">}</span>
 <span class="p">},</span>
 <span class="s2">&quot;cpus&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
 <span class="s2">&quot;mem&quot;</span><span class="p">:</span> <span class="mf">32.0</span><span class="p">,</span>
 <span class="s2">&quot;instances&quot;</span><span class="p">:</span> <span class="mi">1</span>
<span class="p">}</span>
<span class="n">END</span>
</pre></div>
</div>
</li>
</ol>
<p><strong>NOTE:</strong> When the Mesos cluster is created using this ClusterTemplate, the
Mesos cluster will be configured so that a filesystem on an existing cinder
volume can be mounted in a container by configuring the parameters to mount
the cinder volume in the JSON file</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>&quot;parameters&quot;: [
   { &quot;key&quot;: &quot;volume-driver&quot;, &quot;value&quot;: &quot;rexray&quot; },
   { &quot;key&quot;: &quot;volume&quot;, &quot;value&quot;: &quot;redisdata:/data&quot; }
]
</pre></div>
</div>
<ol class="arabic" start="4">
<li><p class="first">Create the container using Marathon REST API</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>MASTER_IP=$(magnum cluster-show mesos-cluster | awk &#39;/ api_address /{print $4}&#39;)
curl -X POST -H &quot;Content-Type: application/json&quot; \
http://${MASTER_IP}:8080/v2/apps -d@mesos.json
</pre></div>
</div>
</li>
</ol>
<p>You can log into the container to check that the mountPath exists, and
you can run the command &#8216;cinder list&#8217; to verify that your cinder
volume status is &#8216;in-use&#8217;.</p>
</div>
</div>
</div>
<div class="section" id="image-management">
<h1>Image Management<a class="headerlink" href="#image-management" title="Permalink to this headline">¶</a></h1>
<p>When a COE is deployed, an image from Glance is used to boot the nodes
in the cluster and then the software will be configured and started on
the nodes to bring up the full cluster.  An image is based on a
particular distro such as Fedora, Ubuntu, etc, and is prebuilt with
the software specific to the COE such as Kubernetes, Swarm, Mesos.
The image is tightly coupled with the following in Magnum:</p>
<ol class="arabic simple">
<li>Heat templates to orchestrate the configuration.</li>
<li>Template definition to map ClusterTemplate parameters to Heat
template parameters.</li>
<li>Set of scripts to configure software.</li>
</ol>
<p>Collectively, they constitute the driver for a particular COE and a
particular distro; therefore, developing a new image needs to be done
in conjunction with developing these other components.  Image can be
built by various methods such as diskimagebuilder, or in some case, a
distro image can be used directly.  A number of drivers and the
associated images is supported in Magnum as reference implementation.
In this section, we focus mainly on the supported images.</p>
<p>All images must include support for cloud-init and the heat software
configuration utility:</p>
<ul class="simple">
<li>os-collect-config</li>
<li>os-refresh-config</li>
<li>os-apply-config</li>
<li>heat-config</li>
<li>heat-config-script</li>
</ul>
<p>Additional software are described as follows.</p>
<div class="section" id="kubernetes-on-fedora-atomic">
<h2>Kubernetes on Fedora Atomic<a class="headerlink" href="#kubernetes-on-fedora-atomic" title="Permalink to this headline">¶</a></h2>
<p>This image can be downloaded from the <a class="reference external" href="https://alt.fedoraproject.org/pub/alt/atomic/stable/Cloud-Images/x86_64/Images/">public Atomic site</a>
or can be built locally using diskimagebuilder.  Details can be found in the
<a class="reference external" href="https://github.com/openstack/magnum/tree/master/magnum/elements/fedora-atomic">fedora-atomic element</a>
The image currently has the following OS/software:</p>
<table border="1" class="docutils">
<colgroup>
<col width="54%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">OS/software</th>
<th class="head">version</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Fedora</td>
<td>23</td>
</tr>
<tr class="row-odd"><td>Docker</td>
<td>1.9.1</td>
</tr>
<tr class="row-even"><td>Kubernetes</td>
<td>1.2.0</td>
</tr>
<tr class="row-odd"><td>etcd</td>
<td>2.2.1</td>
</tr>
<tr class="row-even"><td>Flannel</td>
<td>0.5.4</td>
</tr>
</tbody>
</table>
<p>The following software are managed as systemd services:</p>
<ul class="simple">
<li>kube-apiserver</li>
<li>kubelet</li>
<li>etcd</li>
<li>flannel (if specified as network driver)</li>
<li>docker</li>
</ul>
<p>The following software are managed as Docker containers:</p>
<ul class="simple">
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>kube-proxy</li>
</ul>
<p>The login for this image is <em>fedora</em>.</p>
</div>
<div class="section" id="kubernetes-on-coreos">
<h2>Kubernetes on CoreOS<a class="headerlink" href="#kubernetes-on-coreos" title="Permalink to this headline">¶</a></h2>
<p>CoreOS publishes a <a class="reference external" href="http://beta.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2">stock image</a>
that is being used to deploy Kubernetes.
This image has the following OS/software:</p>
<table border="1" class="docutils">
<colgroup>
<col width="54%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">OS/software</th>
<th class="head">version</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>CoreOS</td>
<td>4.3.6</td>
</tr>
<tr class="row-odd"><td>Docker</td>
<td>1.9.1</td>
</tr>
<tr class="row-even"><td>Kubernetes</td>
<td>1.0.6</td>
</tr>
<tr class="row-odd"><td>etcd</td>
<td>2.2.3</td>
</tr>
<tr class="row-even"><td>Flannel</td>
<td>0.5.5</td>
</tr>
</tbody>
</table>
<p>The following software are managed as systemd services:</p>
<ul class="simple">
<li>kubelet</li>
<li>flannel (if specified as network driver)</li>
<li>docker</li>
<li>etcd</li>
</ul>
<p>The following software are managed as Docker containers:</p>
<ul class="simple">
<li>kube-apiserver</li>
<li>kube-controller-manager</li>
<li>kube-scheduler</li>
<li>kube-proxy</li>
</ul>
<p>The login for this image is <em>core</em>.</p>
</div>
<div class="section" id="kubernetes-on-ironic">
<h2>Kubernetes on Ironic<a class="headerlink" href="#kubernetes-on-ironic" title="Permalink to this headline">¶</a></h2>
<p>This image is built manually using diskimagebuilder.  The scripts and
instructions are included in <a class="reference external" href="https://github.com/openstack/magnum/tree/master/magnum/templates/kubernetes/elements">Magnum code repo</a>.
Currently Ironic is not fully supported yet, therefore more details will be
provided when this driver has been fully tested.</p>
</div>
<div class="section" id="swarm-on-fedora-atomic">
<h2>Swarm on Fedora Atomic<a class="headerlink" href="#swarm-on-fedora-atomic" title="Permalink to this headline">¶</a></h2>
<p>This image is the same as the image for <a class="reference internal" href="#kubernetes-on-fedora-atomic">Kubernetes on Fedora Atomic</a>
described above.  The login for this image is <em>fedora</em>.</p>
</div>
<div class="section" id="mesos-on-ubuntu">
<h2>Mesos on Ubuntu<a class="headerlink" href="#mesos-on-ubuntu" title="Permalink to this headline">¶</a></h2>
<p>This image is built manually using diskimagebuilder.  The instructions are
provided in the section <a class="reference internal" href="#diskimage-builder">Diskimage-builder</a>.
The Fedora site hosts the current image <a class="reference external" href="https://fedorapeople.org/groups/magnum/ubuntu-mesos-latest.qcow2">ubuntu-mesos-latest.qcow2</a>.</p>
<table border="1" class="docutils">
<colgroup>
<col width="54%" />
<col width="46%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">OS/software</th>
<th class="head">version</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Ubuntu</td>
<td>14.04</td>
</tr>
<tr class="row-odd"><td>Docker</td>
<td>1.8.1</td>
</tr>
<tr class="row-even"><td>Mesos</td>
<td>0.25.0</td>
</tr>
<tr class="row-odd"><td>Marathon</td>
<td>0.11.1</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="notification">
<h1>Notification<a class="headerlink" href="#notification" title="Permalink to this headline">¶</a></h1>
<p>Magnum provides notifications about usage data so that 3rd party applications
can use the data for auditing, billing, monitoring, or quota purposes. This
document describes the current inclusions and exclusions for Magnum
notifications.</p>
<p>Magnum uses Cloud Auditing Data Federation (<a class="reference external" href="http://www.dmtf.org/standards/cadf">CADF</a>) Notification as its
notification format for better support of auditing, details about CADF are
documented below.</p>
<div class="section" id="auditing-with-cadf">
<h2>Auditing with CADF<a class="headerlink" href="#auditing-with-cadf" title="Permalink to this headline">¶</a></h2>
<p>Magnum uses the <a class="reference external" href="http://docs.openstack.org/developer/pycadf">PyCADF</a> library to emit CADF notifications, these events
adhere to the DMTF <a class="reference external" href="http://www.dmtf.org/standards/cadf">CADF</a> specification. This standard provides auditing
capabilities for compliance with security, operational, and business processes
and supports normalized and categorized event data for federation and
aggregation.</p>
<p>Below table describes the event model components and semantics for
each component:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%" />
<col width="77%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">model component</th>
<th class="head">CADF Definition</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>OBSERVER</td>
<td>The RESOURCE that generates the CADF Event Record based
on its observation (directly or indirectly) of the
Actual Event.</td>
</tr>
<tr class="row-odd"><td>INITIATOR</td>
<td>The RESOURCE that initiated, originated, or instigated
the event&#8217;s ACTION, according to the OBSERVER.</td>
</tr>
<tr class="row-even"><td>ACTION</td>
<td>The operation or activity the INITIATOR has performed,
has attempted to perform or has pending against the
event&#8217;s TARGET, according to the OBSERVER.</td>
</tr>
<tr class="row-odd"><td>TARGET</td>
<td>The RESOURCE against which the ACTION of a CADF Event
Record was performed, attempted, or is pending,
according to the OBSERVER.</td>
</tr>
<tr class="row-even"><td>OUTCOME</td>
<td>The result or status of the ACTION against the TARGET,
according to the OBSERVER.</td>
</tr>
</tbody>
</table>
<p>The <code class="docutils literal"><span class="pre">payload</span></code> portion of a CADF Notification is a CADF <code class="docutils literal"><span class="pre">event</span></code>, which
is represented as a JSON dictionary. For example:</p>
<div class="highlight-javascript"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;http://schemas.dmtf.org/cloud/audit/1.0/event&quot;</span><span class="p">,</span>
    <span class="s2">&quot;initiator&quot;</span><span class="o">:</span> <span class="p">{</span>
        <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;service/security/account/user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;host&quot;</span><span class="o">:</span> <span class="p">{</span>
            <span class="s2">&quot;agent&quot;</span><span class="o">:</span> <span class="s2">&quot;curl/7.22.0(x86_64-pc-linux-gnu)&quot;</span><span class="p">,</span>
            <span class="s2">&quot;address&quot;</span><span class="o">:</span> <span class="s2">&quot;127.0.0.1&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;&lt;initiator_id&gt;&quot;</span>
    <span class="p">},</span>
    <span class="s2">&quot;target&quot;</span><span class="o">:</span> <span class="p">{</span>
        <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;&lt;target_uri&gt;&quot;</span><span class="p">,</span>
        <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;openstack:1c2fc591-facb-4479-a327-520dade1ea15&quot;</span>
    <span class="p">},</span>
    <span class="s2">&quot;observer&quot;</span><span class="o">:</span> <span class="p">{</span>
        <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;service/security&quot;</span><span class="p">,</span>
        <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;openstack:3d4a50a9-2b59-438b-bf19-c231f9c7625a&quot;</span>
    <span class="p">},</span>
    <span class="s2">&quot;eventType&quot;</span><span class="o">:</span> <span class="s2">&quot;activity&quot;</span><span class="p">,</span>
    <span class="s2">&quot;eventTime&quot;</span><span class="o">:</span> <span class="s2">&quot;2014-02-14T01:20:47.932842+00:00&quot;</span><span class="p">,</span>
    <span class="s2">&quot;action&quot;</span><span class="o">:</span> <span class="s2">&quot;&lt;action&gt;&quot;</span><span class="p">,</span>
    <span class="s2">&quot;outcome&quot;</span><span class="o">:</span> <span class="s2">&quot;success&quot;</span><span class="p">,</span>
    <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;openstack:f5352d7b-bee6-4c22-8213-450e7b646e9f&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Where the following are defined:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">&lt;initiator_id&gt;</span></code>: ID of the user that performed the operation</li>
<li><code class="docutils literal"><span class="pre">&lt;target_uri&gt;</span></code>: CADF specific target URI, (i.e.:  data/security/project)</li>
<li><code class="docutils literal"><span class="pre">&lt;action&gt;</span></code>: The action being performed, typically:
<code class="docutils literal"><span class="pre">&lt;operation&gt;</span></code>. <code class="docutils literal"><span class="pre">&lt;resource_type&gt;</span></code></li>
</ul>
<p>Additionally there may be extra keys present depending on the operation being
performed, these will be discussed below.</p>
<p>Note, the <code class="docutils literal"><span class="pre">eventType</span></code> property of the CADF payload is different from the
<code class="docutils literal"><span class="pre">event_type</span></code> property of a notifications. The former (<code class="docutils literal"><span class="pre">eventType</span></code>) is a
CADF keyword which designates the type of event that is being measured, this
can be: <cite>activity</cite>, <cite>monitor</cite> or <cite>control</cite>. Whereas the latter
(<code class="docutils literal"><span class="pre">event_type</span></code>) is described in previous sections as:
<cite>magnum.&lt;resource_type&gt;.&lt;operation&gt;</cite></p>
</div>
<div class="section" id="supported-events">
<h2>Supported Events<a class="headerlink" href="#supported-events" title="Permalink to this headline">¶</a></h2>
<p>The following table displays the corresponding relationship between resource
types and operations. The bay type is deprecated and will be removed in a
future version. Cluster is the new equivalent term.</p>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="41%" />
<col width="37%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">resource type</th>
<th class="head">supported operations</th>
<th class="head">typeURI</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>bay</td>
<td>create, update, delete</td>
<td>service/magnum/bay</td>
</tr>
<tr class="row-odd"><td>cluster</td>
<td>create, update, delete</td>
<td>service/magnum/cluster</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="example-notification-cluster-create">
<h2>Example Notification - Cluster Create<a class="headerlink" href="#example-notification-cluster-create" title="Permalink to this headline">¶</a></h2>
<p>The following is an example of a notification that is sent when a cluster is
created. This example can be applied for any <code class="docutils literal"><span class="pre">create</span></code>, <code class="docutils literal"><span class="pre">update</span></code> or
<code class="docutils literal"><span class="pre">delete</span></code> event that is seen in the table above. The <code class="docutils literal"><span class="pre">&lt;action&gt;</span></code> and
<code class="docutils literal"><span class="pre">typeURI</span></code> fields will be change.</p>
<div class="highlight-javascript"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;event_type&quot;</span><span class="o">:</span> <span class="s2">&quot;magnum.cluster.created&quot;</span><span class="p">,</span>
    <span class="s2">&quot;message_id&quot;</span><span class="o">:</span> <span class="s2">&quot;0156ee79-b35f-4cef-ac37-d4a85f231c69&quot;</span><span class="p">,</span>
    <span class="s2">&quot;payload&quot;</span><span class="o">:</span> <span class="p">{</span>
        <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;http://schemas.dmtf.org/cloud/audit/1.0/event&quot;</span><span class="p">,</span>
        <span class="s2">&quot;initiator&quot;</span><span class="o">:</span> <span class="p">{</span>
            <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;service/security/account/user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;c9f76d3c31e142af9291de2935bde98a&quot;</span><span class="p">,</span>
            <span class="s2">&quot;user_id&quot;</span><span class="o">:</span> <span class="s2">&quot;0156ee79-b35f-4cef-ac37-d4a85f231c69&quot;</span><span class="p">,</span>
            <span class="s2">&quot;project_id&quot;</span><span class="o">:</span> <span class="s2">&quot;3d4a50a9-2b59-438b-bf19-c231f9c7625a&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;target&quot;</span><span class="o">:</span> <span class="p">{</span>
            <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;service/magnum/cluster&quot;</span><span class="p">,</span>
            <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;openstack:1c2fc591-facb-4479-a327-520dade1ea15&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;observer&quot;</span><span class="o">:</span> <span class="p">{</span>
            <span class="s2">&quot;typeURI&quot;</span><span class="o">:</span> <span class="s2">&quot;service/magnum/cluster&quot;</span><span class="p">,</span>
            <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;openstack:3d4a50a9-2b59-438b-bf19-c231f9c7625a&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;eventType&quot;</span><span class="o">:</span> <span class="s2">&quot;activity&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eventTime&quot;</span><span class="o">:</span> <span class="s2">&quot;2015-05-20T01:20:47.932842+00:00&quot;</span><span class="p">,</span>
        <span class="s2">&quot;action&quot;</span><span class="o">:</span> <span class="s2">&quot;create&quot;</span><span class="p">,</span>
        <span class="s2">&quot;outcome&quot;</span><span class="o">:</span> <span class="s2">&quot;success&quot;</span><span class="p">,</span>
        <span class="s2">&quot;id&quot;</span><span class="o">:</span> <span class="s2">&quot;openstack:f5352d7b-bee6-4c22-8213-450e7b646e9f&quot;</span><span class="p">,</span>
        <span class="s2">&quot;resource_info&quot;</span><span class="o">:</span> <span class="s2">&quot;671da331c47d4e29bb6ea1d270154ec3&quot;</span>
    <span class="p">}</span>
    <span class="s2">&quot;priority&quot;</span><span class="o">:</span> <span class="s2">&quot;INFO&quot;</span><span class="p">,</span>
    <span class="s2">&quot;publisher_id&quot;</span><span class="o">:</span> <span class="s2">&quot;magnum.host1234&quot;</span><span class="p">,</span>
    <span class="s2">&quot;timestamp&quot;</span><span class="o">:</span> <span class="s2">&quot;2016-05-20 15:03:45.960280&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Magnum User Guide</a></li>
<li><a class="reference internal" href="#contents">Contents</a></li>
<li><a class="reference internal" href="#terminology">Terminology</a></li>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#clustertemplate">ClusterTemplate</a><ul>
<li><a class="reference internal" href="#labels">Labels</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cluster">Cluster</a><ul>
<li><a class="reference internal" href="#infrastructure">Infrastructure</a></li>
<li><a class="reference internal" href="#life-cycle">Life cycle</a><ul>
<li><a class="reference internal" href="#create">Create</a></li>
<li><a class="reference internal" href="#list">List</a></li>
<li><a class="reference internal" href="#show">Show</a></li>
<li><a class="reference internal" href="#update">Update</a></li>
<li><a class="reference internal" href="#scale">Scale</a></li>
<li><a class="reference internal" href="#delete">Delete</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#python-client">Python Client</a><ul>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#verifying-installation">Verifying installation</a></li>
<li><a class="reference internal" href="#using-the-command-line-client">Using the command-line client</a></li>
</ul>
</li>
<li><a class="reference internal" href="#horizon-interface">Horizon Interface</a></li>
<li><a class="reference internal" href="#cluster-drivers">Cluster Drivers</a><ul>
<li><a class="reference internal" href="#directory-structure">Directory structure</a></li>
<li><a class="reference internal" href="#sample-cluster-driver">Sample cluster driver</a></li>
<li><a class="reference internal" href="#installing-a-cluster-driver">Installing a cluster driver</a></li>
</ul>
</li>
<li><a class="reference internal" href="#choosing-a-coe">Choosing a COE</a></li>
<li><a class="reference internal" href="#native-clients">Native Clients</a></li>
<li><a class="reference internal" href="#kubernetes">Kubernetes</a><ul>
<li><a class="reference internal" href="#external-load-balancer-for-services">External load balancer for services</a></li>
</ul>
</li>
<li><a class="reference internal" href="#swarm">Swarm</a></li>
<li><a class="reference internal" href="#mesos">Mesos</a><ul>
<li><a class="reference internal" href="#building-mesos-image">Building Mesos image</a><ul>
<li><a class="reference internal" href="#diskimage-builder">Diskimage-builder</a></li>
<li><a class="reference internal" href="#dockerfile">Dockerfile</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-marathon">Using Marathon</a></li>
</ul>
</li>
<li><a class="reference internal" href="#transport-layer-security">Transport Layer Security</a><ul>
<li><a class="reference internal" href="#deploying-a-secure-cluster">Deploying a secure cluster</a></li>
<li><a class="reference internal" href="#interfacing-with-a-secure-cluster">Interfacing with a secure cluster</a><ul>
<li><a class="reference internal" href="#automated">Automated</a></li>
<li><a class="reference internal" href="#manual">Manual</a></li>
</ul>
</li>
<li><a class="reference internal" href="#user-examples">User Examples</a></li>
<li><a class="reference internal" href="#storing-the-certificates">Storing the certificates</a></li>
</ul>
</li>
<li><a class="reference internal" href="#networking">Networking</a></li>
<li><a class="reference internal" href="#high-availability">High Availability</a></li>
<li><a class="reference internal" href="#scaling">Scaling</a><ul>
<li><a class="reference internal" href="#performance-tuning-for-periodic-task">Performance tuning for periodic task</a></li>
<li><a class="reference internal" href="#containers-and-nodes">Containers and nodes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#storage">Storage</a><ul>
<li><a class="reference internal" href="#ephemeral-storage">Ephemeral storage</a></li>
<li><a class="reference internal" href="#persistent-storage">Persistent storage</a><ul>
<li><a class="reference internal" href="#using-cinder-in-kubernetes">Using Cinder in Kubernetes</a></li>
<li><a class="reference internal" href="#using-cinder-in-swarm">Using Cinder in Swarm</a></li>
<li><a class="reference internal" href="#using-cinder-in-mesos">Using Cinder in Mesos</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#image-management">Image Management</a><ul>
<li><a class="reference internal" href="#kubernetes-on-fedora-atomic">Kubernetes on Fedora Atomic</a></li>
<li><a class="reference internal" href="#kubernetes-on-coreos">Kubernetes on CoreOS</a></li>
<li><a class="reference internal" href="#kubernetes-on-ironic">Kubernetes on Ironic</a></li>
<li><a class="reference internal" href="#swarm-on-fedora-atomic">Swarm on Fedora Atomic</a></li>
<li><a class="reference internal" href="#mesos-on-ubuntu">Mesos on Ubuntu</a></li>
</ul>
</li>
<li><a class="reference internal" href="#notification">Notification</a><ul>
<li><a class="reference internal" href="#auditing-with-cadf">Auditing with CADF</a></li>
<li><a class="reference internal" href="#supported-events">Supported Events</a></li>
<li><a class="reference internal" href="#example-notification-cluster-create">Example Notification - Cluster Create</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="troubleshooting-guide.html"
                                  title="previous chapter">Magnum Troubleshooting Guide</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="configuring.html"
                                  title="next chapter">Configuration</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack/magnum
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/userguide.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="configuring.html" title="Configuration"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="troubleshooting-guide.html" title="Magnum Troubleshooting Guide"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">magnum 4.0.1.dev40 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer" role="contentinfo">
        &copy; Copyright 2013, OpenStack Foundation.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/magnum");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>