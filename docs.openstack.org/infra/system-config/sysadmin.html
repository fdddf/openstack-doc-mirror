
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>System Administration &mdash; OpenStack Project Infrastructure 0.0.1.dev12076 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.0.1.dev12076',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="OpenStack Project Infrastructure 0.0.1.dev12076 documentation" href="index.html" />
    <link rel="next" title="Major Systems" href="systems.html" />
    <link rel="prev" title="Test infrastructure Requirements" href="test-infra-requirements.html" /> 
  </head>
  <body>
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
<li><a href="index.html" title="Go to the Home page" class="link">Home</a></li>
<li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
<li><a href="http://review.openstack.org/" title="Go to the OpenStack Gerrit server">Gerrit</a></li>
<li><a href="http://jenkins.openstack.org/" title="Go to the OpenStack Jenkins server">Jenkins</a></li>
<li><a href="http://logstash.openstack.org/" title="Go to the OpenStack Logstash server">Logstash</a></li>
<li><a href="http://etherpad.openstack.org/" title="Go to the OpenStack Etherpad server">Etherpad</a></li>
<li><a href="http://paste.openstack.org/" title="Go to the OpenStack Paste server">Paste</a></li>
<li><a href="http://planet.openstack.org/" title="Go to the OpenStack Planet server">Planet</a></li>
<li><a href="http://lists.openstack.org/" title="Go to the OpenStack Mailman server">Mailman</a></li>

    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="system-administration">
<span id="sysadmin"></span><h1>System Administration<a class="headerlink" href="#system-administration" title="Permalink to this headline">¶</a></h1>
<p>Our infrastructure is code and contributions to it are handled just
like the rest of OpenStack.  This means that anyone can contribute to
the installation and long-running maintenance of systems without shell
access, and anyone who is interested can provide feedback and
collaborate on code reviews.</p>
<p>The configuration of every system operated by the infrastructure team
is managed by Puppet in a single Git repository:</p>
<blockquote>
<div><a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config">https://git.openstack.org/cgit/openstack-infra/system-config</a></div></blockquote>
<p>All system configuration should be encoded in that repository so that
anyone may propose a change in the running configuration to Gerrit.</p>
<div class="section" id="making-a-change-in-puppet">
<h2>Making a Change in Puppet<a class="headerlink" href="#making-a-change-in-puppet" title="Permalink to this headline">¶</a></h2>
<p>Many changes to the Puppet configuration can safely be made while only
performing syntax checks.  Some more complicated changes merit local
testing and an interactive development cycle.  The system-config repo
is structured to facilitate local testing before proposing a change
for review.  This is accomplished by separating the puppet
configuration into several layers with increasing specificity about
site configuration higher in the stack.</p>
<p>The <cite>modules/</cite> directory holds puppet modules that abstractly describe
the configuration of a service.  Ideally, these should have no
OpenStack-specific information in them, and eventually they should all
become modules that are directly consumed from PuppetForge, only
existing in the system-config repo during an initial incubation period.
This is not yet the case, so you may find OpenStack-specific
configuration in these modules, though we are working to reduce it.</p>
<p>The <cite>modules/openstack_project/manifests/</cite> directory holds
configuration for each of the servers that the OpenStack project runs.
Think of these manifests as describing how OpenStack runs a particular
service.  However, no site-specific configuration such as hostnames or
credentials should be included in these files.  This is what lets you
easily test an OpenStack project manifest on your own server.</p>
<p>Finally, the <cite>manifests/site.pp</cite> file contains the information that is
specific to the actual servers that OpenStack runs.  These should be
very simple node definitions that largely exist simply to provide
private data from hiera to the more robust manifests in the
<cite>openstack_project</cite> modules.</p>
<p>This means that you can run the same configuration on your own server
simply by providing a different manifest file instead of site.pp.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The example below is for Debian / Ubuntu systems.  If you are using a
Red Hat based system be sure to setup sudo or simply run the commands as
the root user.</p>
</div>
<p>As an example, to run the etherpad configuration on your own server,
start by ensuring <cite>git</cite> is installed and then cloning the system-config
Git repo:</p>
<div class="highlight-python"><pre>sudo su -
apt-get install git
git clone https://git.openstack.org/openstack-infra/system-config
cd system-config</pre>
</div>
<p>Then copy the etherpad node definition from <cite>manifests/site.pp</cite> to a new
file (be sure to specify the FQDN of the host you are working with in
the node specifier).  It might look something like this:</p>
<div class="highlight-python"><pre># local.pp
class { 'openstack_project::etherpad':
  ssl_cert_file_contents  =&gt; hiera('etherpad_ssl_cert_file_contents'),
  ssl_key_file_contents   =&gt; hiera('etherpad_ssl_key_file_contents'),
  ssl_chain_file_contents =&gt; hiera('etherpad_ssl_chain_file_contents'),
  mysql_host              =&gt; hiera('etherpad_db_host', 'localhost'),
  mysql_user              =&gt; hiera('etherpad_db_user', 'username'),
  mysql_password          =&gt; hiera('etherpad_db_password'),
}</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Be sure not to use any of the hiera functionality from manifests/site.pp
since it is not installed yet. You should be able to comment out the logic
safely.</p>
</div>
<p>Then to apply that configuration, run the following from the root of the
system-config repository:</p>
<div class="highlight-python"><pre>./install_puppet.sh
./install_modules.sh
puppet apply -l /tmp/manifest.log --modulepath=modules:/etc/puppet/modules manifests/local.pp</pre>
</div>
<p>That should turn the system you are logged into into an etherpad
server with the same configuration as that used by the OpenStack
project. You can edit the contents of the system-config repo and
iterate <tt class="docutils literal"><span class="pre">puppet</span> <span class="pre">apply</span></tt> as needed. When you&#8217;re ready to propose the
change for review, you can propose the change with git-review. See the
<a class="reference external" href="http://docs.openstack.org/infra/manual/developers.html#development-workflow">Development workflow section in the Developer&#8217;s Guide</a>
for more information.</p>
</div>
<div class="section" id="accessing-clouds">
<h2>Accessing Clouds<a class="headerlink" href="#accessing-clouds" title="Permalink to this headline">¶</a></h2>
<p>As an unprivileged user who is a member of the <cite>admin</cite> group on
puppetmaster, you can access any of the clouds with:</p>
<div class="highlight-python"><pre>export OS_CLIENT_CONFIG_FILE=/etc/openstack/all-clouds.yaml
openstack --os-cloud &lt;cloud name&gt; --os-cloud-region &lt;region name&gt;</pre>
</div>
</div>
<div class="section" id="adding-a-new-server">
<h2>Adding a New Server<a class="headerlink" href="#adding-a-new-server" title="Permalink to this headline">¶</a></h2>
<p>To create a new server, do the following:</p>
<blockquote>
<div><ul>
<li><p class="first">Add a file in <a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config/tree/modules/openstack_project/manifests/">system-config: modules/openstack_project/manifests/</a> that defines a
class which specifies the configuration of the server.</p>
</li>
<li><p class="first">Add a node pattern entry in <a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config/tree/manifests/site.pp">system-config: manifests/site.pp</a> for the server
that uses that class. Make sure it supports an ordinal naming pattern
(e.g., fooserver01.openstack.org not just fooserver.openstack.org, even
if you&#8217;re replacing an existing server) and that another server with the
same does not already exist in the ansible inventory.</p>
</li>
<li><p class="first">If your server needs private information such as passwords, use
hiera calls in the site manifest, and ask an infra-core team member
to manually add the private information to hiera.</p>
</li>
<li><p class="first">You should be able to install and configure most software only with
puppet.  Nonetheless, if you need SSH access to the host, add your
public key to <a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config/tree/modules/openstack_project/manifests/users.pp">system-config: modules/openstack_project/manifests/users.pp</a> and
include a stanza like this in your server class:</p>
<div class="highlight-python"><pre>realize (
   User::Virtual::Localuser['USERNAME'],
)</pre>
</div>
</li>
<li><p class="first">Add an RST file with documentation about the server in <a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config/tree/doc/source">system-config: doc/source</a>
and add it to the index in that directory.</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="ssh-access">
<h2>SSH Access<a class="headerlink" href="#ssh-access" title="Permalink to this headline">¶</a></h2>
<p>For any of the systems managed by the OpenStack Infrastructure team, the
following practices must be observed for SSH access:</p>
<blockquote>
<div><ul class="simple">
<li>SSH access is only permitted with SSH public/private key
authentication.</li>
<li>Users must use a strong passphrase to protect their private key.  A
passphrase of several words, at least one of which is not in a
dictionary is advised, or a random string of at least 16
characters.</li>
<li>To mitigate the inconvenience of using a long passphrase, users may
want to use an SSH agent so that the passphrase is only requested
once per desktop session.</li>
<li>Users private keys must never be stored anywhere except their own
workstation(s).  In particular, they must never be stored on any
remote server.</li>
<li>If users need to &#8216;hop&#8217; from a server or bastion host to another
machine, they must not copy a private key to the intermediate
machine (see above).  Instead SSH agent forwarding may be used.
However due to the potential for a compromised intermediate machine
to ask the agent to sign requests without the users knowledge, in
this case only an SSH agent that interactively prompts the user
each time a signing request (ie, ssh-agent, but not gnome-keyring)
is received should be used, and the SSH keys should be added with
the confirmation constraint (&#8216;ssh-add -c&#8217;).</li>
<li>The number of SSH keys that are configured to permit access to
OpenStack machines should be kept to a minimum.</li>
<li>OpenStack Infrastructure machines must use puppet to centrally manage and
configure user accounts, and the SSH authorized_keys files from the
openstack-infra/system-config repository.</li>
<li>SSH keys should be periodically rotated (at least once per year).
During rotation, a new key can be added to puppet for a time, and
then the old one removed.  Be sure to run puppet on the backup
servers to make sure they are updated.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="github-access">
<h2>GitHub Access<a class="headerlink" href="#github-access" title="Permalink to this headline">¶</a></h2>
<p>To ensure that code review and testing are not bypassed in the public
Git repositories, only Gerrit will be permitted to commit code to
OpenStack repositories.  Because GitHub always allows project
administrators to commit code, accounts that have access to manage the
GitHub projects necessarily will have commit access to the
repositories.  Therefore, to avoid inadvertent commits to the public
repositories, unique administrative-only accounts must be used to
manage the OpenStack GitHub organization and projects.  These accounts
will not be used to check out or commit code for any project.</p>
</div>
</div>
<div class="section" id="root-only-information">
<h1>Root only information<a class="headerlink" href="#root-only-information" title="Permalink to this headline">¶</a></h1>
<p>Some information is only relevant if you have root access to the system - e.g.
you are an OpenStack CI root operator, or you are running a clone of the
OpenStack CI infrastructure for another project.</p>
<div class="section" id="backups">
<h2>Backups<a class="headerlink" href="#backups" title="Permalink to this headline">¶</a></h2>
<p>Off-site backups are made to two servers:</p>
<blockquote>
<div><ul class="simple">
<li>ci-backup-rs-ord.openstack.org</li>
<li>TBD</li>
</ul>
</div></blockquote>
<p>Puppet is used to perform the initial configuration of those machines,
but to protect them from unauthorized access in case access to the
puppet git repo is compromised, it is not run in agent or in cron mode
on them.  Instead, it should be manually run when changes are made
that should be applied to the backup servers.</p>
<p>To start backing up a server, some commands need to be run manually on
both the backup server, and the server to be backed up.  On the server
to be backed up:</p>
<div class="highlight-python"><pre>sudo su -
ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ""
bup init</pre>
</div>
<p>And then <tt class="docutils literal"><span class="pre">cat</span> <span class="pre">/root/.ssh/id_rsa.pub</span></tt> for use later.</p>
<p>On the backup servers:</p>
<div class="highlight-python"><pre>sudo su -
BUPUSER=bup-&lt;short-servername&gt;  # eg, bup-jenkins-dev
useradd -r $BUPUSER -s /bin/bash -d /opt/backups/$BUPUSER -m
cd /opt/backups/$BUPUSER
mkdir .ssh
cat &gt;.ssh/authorized_keys</pre>
</div>
<p>and add this to the authorized_keys file:</p>
<div class="highlight-python"><pre>command="BUP_DEBUG=0 BUP_FORCE_TTY=3 bup server",no-port-forwarding,no-agent-forwarding,no-X11-forwarding,no-pty &lt;ssh key from earlier&gt;</pre>
</div>
<p>Switching back to the server to be backed up, run:</p>
<div class="highlight-python"><pre>ssh $BUPUSER@ci-backup-rs-ord.openstack.org</pre>
</div>
<p>And verify the host key.  Note this will start the bup server on the
remote end, you will not be given a pty. Use ^D to close the connection
cleanly.  Add the &#8220;backup&#8221; class in puppet to the server
to be backed up.</p>
<div class="section" id="restore-from-backup">
<h3>Restore from Backup<a class="headerlink" href="#restore-from-backup" title="Permalink to this headline">¶</a></h3>
<p>On the server that needs items restored from backup become root, start a
screen session as restoring can take a while, and create a working
directory to restore the backups into. This allows us to be selective in
how we restore content from backups:</p>
<div class="highlight-python"><pre>sudo su -
screen
mkdir /root/backup-restore-$DATE
cd /root/backup-restore-$DATE</pre>
</div>
<p>At this point we can join the tar that was split by the backup cron:</p>
<div class="highlight-python"><pre>bup join -r bup-&lt;short-servername&gt;@ci-backup-rs-ord.openstack.org: root &gt; backup.tar</pre>
</div>
<p>At this point you may need to wait a while. These backups are stored on
servers geographically distant from our normal servers resulting in less
network throughput between servers than we are used to.</p>
<p>Once the <tt class="docutils literal"><span class="pre">bup</span> <span class="pre">join</span></tt> is complete you will have a tar archive of that
backup. It may be useful to list the files in the backup
<tt class="docutils literal"><span class="pre">tar</span> <span class="pre">-tf</span> <span class="pre">backup.tar</span></tt> to get an idea of what things are available. At
this point you will probably either want to extract the entire backup:</p>
<div class="highlight-python"><pre>tar -xvf backup.tar
ls -al</pre>
</div>
<p>Or selectively extract files:</p>
<div class="highlight-python"><pre># path/to/file needs to match the output given by tar -t
tar -xvf backup.tar path/to/file</pre>
</div>
<p>Note if you created your working directory in a path that is not
excluded by bup you will want to remove that directory when your work is
done. /root/backup-restore-* is excluded so the path above is safe.</p>
</div>
</div>
<div class="section" id="launching-new-servers">
<h2>Launching New Servers<a class="headerlink" href="#launching-new-servers" title="Permalink to this headline">¶</a></h2>
<p>New servers are launched using the <tt class="docutils literal"><span class="pre">launch/launch-node.py</span></tt> tool from the git
repository <tt class="docutils literal"><span class="pre">https://git.openstack.org/openstack-infra/system-config</span></tt>. This
tool is run from a checkout on the puppetmaster - please see <a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config/tree/launch/README">system-config: launch/README</a>
for detailed instructions.</p>
</div>
<div class="section" id="disable-enable-puppet">
<span id="id1"></span><h2>Disable/Enable Puppet<a class="headerlink" href="#disable-enable-puppet" title="Permalink to this headline">¶</a></h2>
<p>You should normally not make manual changes to servers, but instead,
make changes through puppet.  However, under some circumstances, you
may need to temporarily make a manual change to a puppet-managed
resource on a server.</p>
<p>OpenStack Infra uses a non-trivial combination of Dynamic and Static
Inventory in Ansible to control execution of puppet. A full understanding
of the concepts in
<a class="reference external" href="http://docs.ansible.com/ansible/intro_inventory.html">Ansible Inventory Introduction</a>
and
<a class="reference external" href="http://docs.ansible.com/ansible/intro_dynamic_inventory.html">Ansible Dynamic Inventory</a>
is essential for being able to make informed decisions about actions
to take.</p>
<p>In the case of needing to disable the running of puppet on a node, it&#8217;s a
simple matter of adding an entry to the ansible inventory &#8220;disabled&#8221; group
in <a class="reference external" href="https://git.openstack.org/cgit/openstack-infra/system-config/tree/modules/openstack_project/files/puppetmaster/groups.txt">system-config: modules/openstack_project/files/puppetmaster/groups.txt</a>. The
disabled entry is an input to <cite>ansible &#8211;list-hosts</cite> so you can check your
entry simply by running it with <cite>ansible $hostlist &#8211;list-hosts</cite> as root
on the puppetmaster host and ensuring that the list of hosts returned is as
expected. Globs, group names and server UUIDs should all be acceptable input.</p>
<p>If you need to disable a host immediately without waiting for a patch to land
to <cite>system-config</cite>, there is a file on the puppetmaster host,
<cite>/etc/ansible/hosts/emergency</cite> that can be edited directly.</p>
<p><cite>/etc/ansible/hosts/emergency</cite> is a file that should normally be empty, but
the contents are not managed by puppet. It&#8217;s purpose is to allow for disabling
puppet at times when landing a change to the puppet repo would be either
unreasonable or impossible.</p>
<p>There are two sections in the emergency file, <cite>disabled</cite> and
<cite>disabled:children</cite>. To disable a single host, put it in <cite>disabled</cite>. If you
want to disable a group of hosts, put it in <cite>disabled:children</cite>. Any hosts we
have that have more than one host with the same name (such as in the case of
being in the midst of a migration) will show up as a group with the name of
the hostname and the individual servers will be listed by UUID.</p>
<p>Because of the way static and dynamic inventories get merged by ansible, the
emergency file needs to stand alone. If you need to disable a group of servers
from OpenStack you need to not only add it to <cite>disabled:children</cite>, you need to
add an emtpy group into the emergency file too.</p>
<p>Disabling puppet via ansible inventory does not disable puppet from being
able to be run directly on the host, it merely prevents ansible from
attempting to run it. If you choose to run puppet manually on a host, take care
to ensure that it has not been disabled at the puppetmaster level first.</p>
<div class="section" id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>To disable an OpenStack instance called <cite>amazing.openstack.org</cite> temporarily
without landing a puppet change, ensure the following is in
<cite>/etc/ansible/hosts/emergency</cite></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">disabled</span><span class="p">]</span>
<span class="n">amazing</span><span class="o">.</span><span class="n">openstack</span><span class="o">.</span><span class="n">org</span>
</pre></div>
</div>
<p>To disable one of the OpenStack instances called <cite>git.openstack.org</cite>
temporarily without landing a puppet change but leaving the other running,
find its UUID via OpenStack tools and ensure it&#8217;s in the emergency file.</p>
<div class="highlight-python"><pre>[disabled]
811c5197-dba7-4d3a-a3f6-68ca5328b9a7</pre>
</div>
<p>To disable a group of hosts in the emergency file, such as all of the pypi
hosts.</p>
<div class="highlight-python"><pre>[disabled:children]
pypi</pre>
</div>
<p>To disable a staticly defined host that is not an OpenStack host, such as
the Infra cloud controller hosts, put the following in groups.txt.</p>
<div class="highlight-python"><pre>disabled controller.useast.openstack.org</pre>
</div>
</div>
</div>
<div class="section" id="cinder-volume-management">
<span id="cinder"></span><h2>Cinder Volume Management<a class="headerlink" href="#cinder-volume-management" title="Permalink to this headline">¶</a></h2>
<div class="section" id="adding-a-new-device">
<h3>Adding a New Device<a class="headerlink" href="#adding-a-new-device" title="Permalink to this headline">¶</a></h3>
<p>If the main volume group doesn&#8217;t have enough space for what you want
to do, this is how you can add a new volume.</p>
<p>Log into puppetmaster.openstack.org and run:</p>
<div class="highlight-python"><pre>export OS_CLOUD=openstackci-rax
export OS_REGION_NAME=DFW

openstack server list
openstack volume list</pre>
</div>
<p>Change the variables to use a different environment. ORD for example:</p>
<div class="highlight-python"><pre>export OS_CLOUD=openstackci-rax
export OS_REGION_NAME=ORD</pre>
</div>
<ul>
<li><p class="first">Add a new 1024G cinder volume (substitute the hostname and the next number
in series for NN):</p>
<div class="highlight-python"><pre>openstack volume create --size 1024 "$HOSTNAME.ord.openstack.org/mainNN"
openstack server add volume "HOSTNAME.openstack.org" "HOSTNAME.openstack.org/mainNN"</pre>
</div>
</li>
<li><p class="first">or to add a 100G SSD volume:</p>
<div class="highlight-python"><pre>openstack volume create --type SSD --size 100 "HOSTNAME.openstack.org/mainNN"
openstack server add volume "HOSTNAME.openstack.org" "HOSTNAME.openstack.org/mainNN"</pre>
</div>
</li>
<li><p class="first">Then, on the host, create the partition table:</p>
<div class="highlight-python"><pre>DEVICE=/dev/xvdX
sudo parted $DEVICE mklabel msdos mkpart primary 0% 100% set 1 lvm on
sudo pvcreate ${DEVICE}1</pre>
</div>
</li>
<li><p class="first">It should show up in pvs:</p>
<div class="highlight-python"><pre>$ sudo pvs
  PV         VG   Fmt  Attr PSize    PFree
  /dev/xvdX1      lvm2 a-   1024.00g 1024.00g</pre>
</div>
</li>
<li><p class="first">Add it to the main volume group:</p>
<div class="highlight-python"><pre>sudo vgextend main ${DEVICE}1</pre>
</div>
</li>
<li><p class="first">However, if the volume group does not exist yet, you can create it:</p>
<div class="highlight-python"><pre>sudo vgcreate main ${DEVICE}1</pre>
</div>
</li>
</ul>
</div>
<div class="section" id="creating-a-new-logical-volume">
<h3>Creating a New Logical Volume<a class="headerlink" href="#creating-a-new-logical-volume" title="Permalink to this headline">¶</a></h3>
<p>Make sure there is enough space in the volume group:</p>
<div class="highlight-python"><pre>$ sudo vgs
  VG   #PV #LV #SN Attr   VSize VFree
  main   4   2   0 wz--n- 2.00t 347.98g</pre>
</div>
<p>If not, see <a class="reference internal" href="#adding-a-new-device">Adding a New Device</a>.</p>
<p>Create the new logical volume and initialize the filesystem:</p>
<div class="highlight-python"><pre>NAME=newvolumename
sudo lvcreate -L1500GB -n $NAME main

sudo mkfs.ext4 -m 0 -j -L $NAME /dev/main/$NAME
sudo tune2fs -i 0 -c 0 /dev/main/$NAME</pre>
</div>
<p>Be sure to add it to <tt class="docutils literal"><span class="pre">/etc/fstab</span></tt>.</p>
</div>
<div class="section" id="expanding-an-existing-logical-volume">
<h3>Expanding an Existing Logical Volume<a class="headerlink" href="#expanding-an-existing-logical-volume" title="Permalink to this headline">¶</a></h3>
<p>Make sure there is enough space in the volume group:</p>
<div class="highlight-python"><pre>$ sudo vgs
  VG   #PV #LV #SN Attr   VSize VFree
  main   4   2   0 wz--n- 2.00t 347.98g</pre>
</div>
<p>If not, see <a class="reference internal" href="#adding-a-new-device">Adding a New Device</a>.</p>
<p>The following example increases the size of a volume by 100G:</p>
<div class="highlight-python"><pre>NAME=volumename
sudo lvextend -L+100G /dev/main/$NAME
sudo resize2fs /dev/main/$NAME</pre>
</div>
<p>The following example increases the size of a volume to the maximum allowable:</p>
<div class="highlight-python"><pre>NAME=volumename
sudo lvextend -l +100%FREE /dev/main/$NAME
sudo resize2fs /dev/main/$NAME</pre>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">System Administration</a><ul>
<li><a class="reference internal" href="#making-a-change-in-puppet">Making a Change in Puppet</a></li>
<li><a class="reference internal" href="#accessing-clouds">Accessing Clouds</a></li>
<li><a class="reference internal" href="#adding-a-new-server">Adding a New Server</a></li>
<li><a class="reference internal" href="#ssh-access">SSH Access</a></li>
<li><a class="reference internal" href="#github-access">GitHub Access</a></li>
</ul>
</li>
<li><a class="reference internal" href="#root-only-information">Root only information</a><ul>
<li><a class="reference internal" href="#backups">Backups</a><ul>
<li><a class="reference internal" href="#restore-from-backup">Restore from Backup</a></li>
</ul>
</li>
<li><a class="reference internal" href="#launching-new-servers">Launching New Servers</a></li>
<li><a class="reference internal" href="#disable-enable-puppet">Disable/Enable Puppet</a><ul>
<li><a class="reference internal" href="#examples">Examples</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cinder-volume-management">Cinder Volume Management</a><ul>
<li><a class="reference internal" href="#adding-a-new-device">Adding a New Device</a></li>
<li><a class="reference internal" href="#creating-a-new-logical-volume">Creating a New Logical Volume</a></li>
<li><a class="reference internal" href="#expanding-an-existing-logical-volume">Expanding an Existing Logical Volume</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="test-infra-requirements.html"
                                  title="previous chapter">Test infrastructure Requirements</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="systems.html"
                                  title="next chapter">Major Systems</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack-infra/system-config
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/sysadmin.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="systems.html" title="Major Systems"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="test-infra-requirements.html" title="Test infrastructure Requirements"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">OpenStack Project Infrastructure 0.0.1.dev12076 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer">
        &copy; Copyright 2012-2014, OpenStack Infastructure Team - see the <a href="https://git.openstack.org/cgit/openstack-infra/system-config/">system-config git repo</a> for details.
      Last updated on Mon Feb 13 23:40:08 2017, commit 91bca8d.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/OpenStack Project Infrastructure");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>