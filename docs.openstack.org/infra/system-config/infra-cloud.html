
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Infra Cloud &mdash; OpenStack Project Infrastructure 0.0.1.dev12076 documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/tweaks.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.0.1.dev12076',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="OpenStack Project Infrastructure 0.0.1.dev12076 documentation" href="index.html" />
    <link rel="next" title="Running your own CI infrastructure" href="running-your-own.html" />
    <link rel="prev" title="Asterisk" href="asterisk.html" /> 
  </head>
  <body>
  <div id="header">
    <h1 id="logo"><a href="http://www.openstack.org/">OpenStack</a></h1>
    <ul id="navigation">
      
<li><a href="index.html" title="Go to the Home page" class="link">Home</a></li>
<li><a href="http://wiki.openstack.org/" title="Go to the OpenStack Wiki">Wiki</a></li>
<li><a href="http://review.openstack.org/" title="Go to the OpenStack Gerrit server">Gerrit</a></li>
<li><a href="http://jenkins.openstack.org/" title="Go to the OpenStack Jenkins server">Jenkins</a></li>
<li><a href="http://logstash.openstack.org/" title="Go to the OpenStack Logstash server">Logstash</a></li>
<li><a href="http://etherpad.openstack.org/" title="Go to the OpenStack Etherpad server">Etherpad</a></li>
<li><a href="http://paste.openstack.org/" title="Go to the OpenStack Paste server">Paste</a></li>
<li><a href="http://planet.openstack.org/" title="Go to the OpenStack Planet server">Planet</a></li>
<li><a href="http://lists.openstack.org/" title="Go to the OpenStack Mailman server">Mailman</a></li>

    </ul>
  </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="infra-cloud">
<span id="id1"></span><h1>Infra Cloud<a class="headerlink" href="#infra-cloud" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>With donated hardware and datacenter space, we can run an optimized
semi-private cloud for the purpose of adding testing capacity and also
with an eye for &#8220;dog fooding&#8221; OpenStack itself.</p>
</div>
<div class="section" id="current-status">
<h2>Current Status<a class="headerlink" href="#current-status" title="Permalink to this headline">¶</a></h2>
<p>Currently this cloud is in the planning and design phases. This section
will be updated or removed as that changes.</p>
</div>
<div class="section" id="mission">
<h2>Mission<a class="headerlink" href="#mission" title="Permalink to this headline">¶</a></h2>
<p>The infra-cloud&#8217;s mission is to turn donated raw hardware resources into
expanded capacity for the OpenStack infrastructure nodepool.</p>
</div>
<div class="section" id="methodology">
<h2>Methodology<a class="headerlink" href="#methodology" title="Permalink to this headline">¶</a></h2>
<p>Infra-cloud is run like any other infra managed service. Puppet modules
and Ansible do the bulk of configuring hosts, and Gerrit code review
drives 99% of activities, with logins used only for debugging and
repairing the service.</p>
</div>
<div class="section" id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><p class="first">Compute - The intended workload is mostly nodepool launched Jenkins
slaves. Thus flavors that are capable of running these tests in a
reasonable amount of time must be available. The flavor(s) must provide:</p>
<blockquote>
<div><ul class="simple">
<li>8GB RAM</li>
<li>8 * <cite>vcpu</cite></li>
<li>30GB root disk</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Images - Image upload must be allowed for nodepool.</p>
</li>
<li><p class="first">Uptime - Because there are other clouds that can keep some capacity
running, 99.9% uptime should be acceptable.</p>
</li>
<li><p class="first">Performance - The performance of compute and networking in infra-cloud
should be at least as good as, if not better than, the other nodepool
clouds that infra uses today.</p>
</li>
<li><p class="first">Infra-core - Infra-core is in charge of running the service.</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="multi-site">
<h3>Multi-Site<a class="headerlink" href="#multi-site" title="Permalink to this headline">¶</a></h3>
<p>Despite the servers being in the same physical location and network,
they are divided in at least two logical &#8220;sites&#8221;, vanilla and chocolate,
Each site will have its own cloud, and these clouds will share no data.</p>
<div class="section" id="vanilla">
<h4>Vanilla<a class="headerlink" href="#vanilla" title="Permalink to this headline">¶</a></h4>
<p>The vanilla cloud has 48 machines. Each machine has 96G of RAM, 1.8TiB of disk and
24 Cores of Intel Xeon X5650 &#64; 2.67GHz processors.</p>
</div>
<div class="section" id="chocolate">
<h4>Chocolate<a class="headerlink" href="#chocolate" title="Permalink to this headline">¶</a></h4>
<p>The chocolate cloud has 100 machines. Each machine has 96G of RAM, 1.8TiB of disk and
32 Cores of Intel Xeon E5-2670 0 &#64; 2.60GHz processors.</p>
</div>
</div>
<div class="section" id="software">
<h3>Software<a class="headerlink" href="#software" title="Permalink to this headline">¶</a></h3>
<p>Infra-cloud runs the most recent OpenStack stable release. During the
period following a release, plans must be made to upgrade as soon as
possible. In the future the cloud may be continuously deployed.</p>
</div>
<div class="section" id="management">
<h3>Management<a class="headerlink" href="#management" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li>Currently a single &#8220;Ironic Controller&#8221; is installed by hand and used by both
sites. That machine is enrolled into the puppet/ansible infrastructure and
can be reached at baremetal00.vanilla.ic.openstack.org.</li>
<li>The &#8220;Ironic Controller&#8221; will have bifrost installed on it. All of the
other machines in that site will be enrolled in the Ironic that bifrost
manages. bifrost will be responsible for booting base OS with IP address
and ssh key for each machine.</li>
<li>You can interact with the Bifrost Ironic installation by sourcing
<tt class="docutils literal"><span class="pre">/opt/stack/bifrost/env-vars</span></tt> then running the ironic cli client (for
example: <tt class="docutils literal"><span class="pre">ironic</span> <span class="pre">node-list</span></tt>).</li>
<li>The machines will all be added to a manual ansible inventory file adjacent
to the dynamic inventory that ansible currently uses to run puppet. Any
metadata that the ansible infrastructure for running puppet needs that
would have come from OpenStack infrastructure will simply be put into
static ansible group_vars.</li>
<li>The static inventory should be put into puppet so that it is public, with
the IPMI passwords in hiera.</li>
<li>An OpenStack Cloud with KVM as the hypervisor will be installed using
OpenStack puppet modules as per normal infra installation of services.</li>
<li>As with all OpenStack services, metrics will be collected in public
cacti and graphite services. The particular metrics are TBD.</li>
<li>As a cloud has a large amount of pertinent log data, a public ELK cluster
will be needed to capture and expose it.</li>
<li>All Infra services run on the public internet, and the same will be true
for the Infra Clouds and the Ironic Clouds. Insecure services that need
to be accessible across machine boundaries will employ per-IP iptables
rules rather then relying on a squishy middle.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="architecture">
<h3>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h3>
<p>The generally accepted &#8220;Controller&#8221; and &#8220;Compute&#8221; layout is used,
with controllers running all non-compute services and compute nodes
running only nova-compute and supporting services.</p>
<blockquote>
<div><ul class="simple">
<li>The cloud is deployed with two controllers in a DRBD storage pair
with ACTIVE/PASSIVE configured and a VIP shared between the two.
This is done to avoid complications with Galera and RabbitMQ at
the cost of making failovers more painful and under-utilizing the
passive stand-by controller.</li>
<li>The cloud will use KVM because it is the default free hypervisor and
has the widest user base in OpenStack.</li>
<li>The cloud will use Neutron configured for Provider VLAN because we
do not require tenant isolation and this simplifies our networking on
compute nodes.</li>
<li>The cloud will not use floating IPs because every node will need to be
reachable via routable IPs and thus there is no need for separation. Also
Nodepool is under our control, so we don&#8217;t have to worry about DNS TTLs
or anything else causing a need for a particular endpoint to remain at
a stable IP.</li>
<li>The cloud will not use security groups because these are single use VMs
and they will configure any firewall inside the VM.</li>
<li>The cloud will use MySQL because it is the default in OpenStack and has
the widest user base.</li>
<li>The cloud will use RabbitMQ because it is the default in OpenStack and
has the widest user base. We don&#8217;t have scaling demands that come close
to pushing the limits of RabbitMQ.</li>
<li>The cloud will run swift as a backend for glance so that we can scale
image storage out as need arises.</li>
<li>The cloud will run keystone v3 and glance v2 APIs because these are the
versions upstream recommends using.</li>
<li>The cloud will run keystone on port 443.</li>
<li>The cloud will not use the glance task API for image uploads, it will use
the PUT interface because the task API does not function and we are not
expecting a wide user base to be uploading many images simultaneously.</li>
<li>The cloud will provide DHCP directly to its nodes because we trust DHCP.</li>
<li>The cloud will have config drive enabled because we believe it to be more
robust than the EC2-style metadata service.</li>
<li>The cloud will not have the meta-data service enabled because we do not
believe it to be robust.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="networking">
<h3>Networking<a class="headerlink" href="#networking" title="Permalink to this headline">¶</a></h3>
<p>Neutron is used, with a single <a class="reference external" href="http://docs.openstack.org/networking-guide/scenario-provider-lb.html">provider VLAN</a> attached to VMs for the
simplest possible networking. DHCP is configured to hand the machine a
routable IP which can be reached directly from the internet to facilitate
nodepool/zuul communications.</p>
<p>Each site will need 2 VLANs. One for the public IPs which every NIC of every
host will be attached to. That VLAN will get a publicly routable /23. Also,
there should be a second VLAN that is connected only to the NIC of the
Ironic Cloud and is routed to the IPMI management network of all of the other
nodes. Whether we use LinuxBridge or Open vSwitch is still TBD.</p>
</div>
</div>
<div class="section" id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h2>
<div class="section" id="regenerating-images">
<h3>Regenerating images<a class="headerlink" href="#regenerating-images" title="Permalink to this headline">¶</a></h3>
<p>When redeploying servers with bifrost, we may have the need to refresh the image
that is deployed to them, because we may need to add some packages, update the
elements that we use, consume latest versions of projects...</p>
<p>To generate an image, you need to follow these steps:</p>
<div class="highlight-python"><pre>1. In the baremetal server, remove everything under /httpboot directory.
   This will clean the generated qcow2 image that is consumed by servers.

2. If there is a need to also update the CoreOS image, remove everything
   under /tftpboot directory. This will clean the ramdisk image that is
   used when PXE booting.

3. Run the install playbook again, so it generates the image. You need to
   be sure that you pass the skip_install flag, to avoid the update of all
   the bifrost related projects (ironic, dib, etc...):

   ansible-playbook -vvv -e @/etc/bifrost/bifrost_global_vars \
       -e skip_install=true \
       -i /opt/stack/bifrost/playbooks/inventory/bifrost_inventory.py \
       /opt/stack/bifrost/playbooks/install.yaml

4. After the install finishes, you can redeploy the servers again
   using ``run_bifrost.sh`` script.</pre>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
<div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Infra Cloud</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#current-status">Current Status</a></li>
<li><a class="reference internal" href="#mission">Mission</a></li>
<li><a class="reference internal" href="#methodology">Methodology</a></li>
<li><a class="reference internal" href="#requirements">Requirements</a></li>
<li><a class="reference internal" href="#implementation">Implementation</a><ul>
<li><a class="reference internal" href="#multi-site">Multi-Site</a><ul>
<li><a class="reference internal" href="#vanilla">Vanilla</a></li>
<li><a class="reference internal" href="#chocolate">Chocolate</a></li>
</ul>
</li>
<li><a class="reference internal" href="#software">Software</a></li>
<li><a class="reference internal" href="#management">Management</a></li>
<li><a class="reference internal" href="#architecture">Architecture</a></li>
<li><a class="reference internal" href="#networking">Networking</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#regenerating-images">Regenerating images</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="asterisk.html"
                                  title="previous chapter">Asterisk</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="running-your-own.html"
                                  title="next chapter">Running your own CI infrastructure</a></p>
            <h3>Project Source</h3>
            <ul class="this-page-menu">
              <li><a href="http://git.openstack.org/cgit/openstack-infra/system-config
"
                     rel="nofollow">Project Source</a></li>
            </ul>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/infra-cloud.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
    </div>
</div>

      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="running-your-own.html" title="Running your own CI infrastructure"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="asterisk.html" title="Asterisk"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">OpenStack Project Infrastructure 0.0.1.dev12076 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer">
        &copy; Copyright 2012-2014, OpenStack Infastructure Team - see the <a href="https://git.openstack.org/cgit/openstack-infra/system-config/">system-config git repo</a> for details.
      Last updated on Mon Feb 13 23:40:08 2017, commit 91bca8d.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
//Tracking docs.openstack.org/developer/<projectname> only
//The URL is built from the project variable in conf.py
var pageTracker = _gat._getTracker("UA-17511903-1");
pageTracker._setCookiePath("/developer/OpenStack Project Infrastructure");
pageTracker._trackPageview();
} catch(err) {}</script>

  </body>
</html>